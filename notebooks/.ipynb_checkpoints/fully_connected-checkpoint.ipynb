{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#im skipping leap days currently in multiple places\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ERCOT data\n",
    "lds = datetime.datetime(year=2016, month=2, day=29, hour=0, minute=0) #lines 1415 to 1439\n",
    "lde = datetime.datetime(year=2016, month=2, day=29, hour=23, minute=0)\n",
    "\n",
    "path = \"/home/chase/projects/peakload/data/ercot/\"\n",
    "ercot2017 = pd.read_excel(path + \"native_Load_2017.xlsx\")\n",
    "ercot2016 = pd.read_excel(path + \"native_Load_2016.xlsx\") #leap year\n",
    "ercot2016.drop(ercot2016.index[1415:1439], inplace=True)\n",
    "\n",
    "ercot2015 = pd.read_excel(path + \"native_Load_2015.xls\")\n",
    "ercot2014 = pd.read_excel(path + \"2014_ERCOT_Hourly_Load_Data.xls\")\n",
    "ercot2013 = pd.read_excel(path + \"2013_ERCOT_Hourly_Load_Data.xls\")\n",
    "ercot2012 = pd.read_excel(path + \"2012_ERCOT_Hourly_Load_Data.xls\") #leap year\n",
    "ercot2012.drop(ercot2012.index[1415:1439], inplace=True)\n",
    "\n",
    "ercot2011 = pd.read_excel(path + \"2011_ERCOT_Hourly_Load_Data.xls\")\n",
    "ercot2010 = pd.read_excel(path + \"2010_ERCOT_Hourly_Load_Data.xls\")\n",
    "\n",
    "ercotdata = list(ercot2010['ERCOT']) + list(ercot2011['ERCOT']) + list(ercot2012['ERCOT']) + list(ercot2013['ERCOT']) + list(ercot2014['ERCOT']) + list(ercot2015['ERCOT']) + list(ercot2016['ERCOT']) + list(ercot2017['ERCOT'])\n",
    "yearly_data = [ercot2010, ercot2011, ercot2012, ercot2013, ercot2014, ercot2015, ercot2016, ercot2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = datetime.datetime(year=2011, month=1, day=1, hour=0, minute=0, second=0)\n",
    "curr = startdate\n",
    "enddate = datetime.datetime(year=2017, month=12, day=31, hour=23, minute=0, second=0)\n",
    "\n",
    "allhours = []\n",
    "\n",
    "while curr <= enddate:\n",
    "    if curr.month == 2 and curr.day == 29:\n",
    "        pass\n",
    "    else:\n",
    "        allhours.append(curr)\n",
    "    curr += datetime.timedelta(hours=1)\n",
    "    \n",
    "trainhours = allhours[0:-8760]\n",
    "valhours = allhours[-8760:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yearly_loads = np.zeros((8,8760))\n",
    "\n",
    "for i in range(len(yearly_data)):\n",
    "    yearly_loads[i,:] = np.array(yearly_data[i]['ERCOT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = yearly_loads[0:6,:]\n",
    "train_data_vec = yearly_loads[0,:]\n",
    "for j in range(1,train_data.shape[0]):\n",
    "    train_data_vec = np.concatenate((train_data_vec, train_data[j,:]), axis= 0)\n",
    "\n",
    "val_data = yearly_loads[-1,:]\n",
    "\n",
    "train_data_labels = np.zeros((train_data.shape[0]*train_data.shape[1], 1))\n",
    "val_data_labels = np.zeros((val_data.shape[0], 1))\n",
    "\n",
    "for i in range(train_data_labels.shape[0]):\n",
    "    year = int(np.floor(float(i)/8760.0))\n",
    "    greater = np.sum([1 for k in yearly_loads[year,:] if k >= train_data_vec[i]])/float(yearly_loads.shape[1])\n",
    "    train_data_labels[i] = 1.0 - greater\n",
    "        \n",
    "for i in range(val_data_labels.shape[0]):\n",
    "    year = train_data.shape[0] - 1\n",
    "    greater = np.sum([1 for k in yearly_loads[year,:] if k >= val_data[i]])/float(yearly_loads.shape[1])\n",
    "    val_data_labels[i] = 1.0 - greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_arr = np.reshape(train_data_labels, (6,8760))\n",
    "model = np.mean(train_data_arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61320,)\n",
      "61320\n",
      "8760\n",
      "52560\n"
     ]
    }
   ],
   "source": [
    "all_loads = np.concatenate((train_data_vec, val_data))\n",
    "print(all_loads.shape)\n",
    "print(len(allhours))\n",
    "print(len(valhours))\n",
    "print(len(trainhours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_features = {}\n",
    "seasons = {1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 2, 8: 2, 9: 2, 10: 3, 11: 3, 12: 3}\n",
    "\n",
    "for i in range(len(allhours)):\n",
    "    load_features[allhours[i]] = {}\n",
    "    loads_dayhour = []\n",
    "    loads_hour = []\n",
    "    \n",
    "    currtime = allhours[i]\n",
    "    dt_back = currtime\n",
    "    stop_1 = currtime - datetime.timedelta(days=30*4)\n",
    "    stop_2 = currtime - datetime.timedelta(days=30*2)\n",
    "    \n",
    "    while dt_back > stop_1:\n",
    "        if dt_back < allhours[0]:\n",
    "            break\n",
    "        if dt_back.month == 2 and dt_back.day == 29:\n",
    "            pass\n",
    "        else:\n",
    "            #get loads over the last 2 months by matching day/hour\n",
    "            if dt_back.hour == currtime.hour and dt_back.weekday() == currtime.weekday():\n",
    "                loads_dayhour.append(all_loads[allhours.index(dt_back)])\n",
    "            #get loads over the last 2 months by matching hour\n",
    "            if dt_back > stop_2:\n",
    "                if dt_back.hour == currtime.hour:\n",
    "                    loads_hour.append(all_loads[allhours.index(dt_back)])\n",
    "        dt_back -= datetime.timedelta(hours=24)\n",
    "    \n",
    "    dhhist = np.histogram(loads_dayhour, bins=7)[0]\n",
    "    dayhourmax = np.max(loads_dayhour)\n",
    "    dayhourmin = np.min(loads_dayhour)\n",
    "    dayhourhist = (1.0/sum(dhhist)) * dhhist\n",
    "    dayhourmean = np.mean(loads_dayhour)\n",
    "    dayhourvar = np.var(loads_dayhour)\n",
    "    hourmax = np.max(loads_hour)\n",
    "    hourmin = np.min(loads_hour)\n",
    "    hhist = np.histogram(loads_hour, bins=7)[0]\n",
    "    hourhist = (1.0/sum(hhist)) * hhist\n",
    "    hourmean = np.mean(loads_hour)\n",
    "    hourvar = np.var(loads_hour)\n",
    "    \n",
    "    load_features[currtime][\"dayhourmax\"] = dayhourmax #1\n",
    "    load_features[currtime][\"dayhourmin\"] = dayhourmin #1\n",
    "    load_features[currtime][\"dayhourhist\"] = dayhourhist #7\n",
    "    load_features[currtime][\"dayhourmean\"] = dayhourmean #1\n",
    "    load_features[currtime][\"dayhourvar\"] = dayhourvar #1\n",
    "    load_features[currtime][\"hourmax\"] = hourmax #1\n",
    "    load_features[currtime][\"hourmin\"] = hourmin #1\n",
    "    load_features[currtime][\"hourhist\"] = hourhist #7\n",
    "    load_features[currtime][\"hourmean\"] = hourmean #1\n",
    "    load_features[currtime][\"hourvar\"] = hourvar #1\n",
    "    load_features[currtime][\"month\"] = currtime.month #1\n",
    "    load_features[currtime][\"week\"] = currtime.isocalendar()[1] #1\n",
    "    load_features[currtime][\"weekday\"] = currtime.weekday() #1\n",
    "    load_features[currtime][\"season\"] = seasons[currtime.month] #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "load_feautres_copy = copy.copy(load_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "out_features = {}\n",
    "for key in load_feautres_copy:\n",
    "    t = str(int(time.mktime(key.timetuple())))\n",
    "    out_features[t] = {}\n",
    "    for kk in load_feautres_copy[key]:\n",
    "        item = load_feautres_copy[key][kk]\n",
    "        if type(item) == np.ndarray:\n",
    "            out_features[t][kk] = item.tolist()\n",
    "        else:\n",
    "            out_features[t][kk] = item\n",
    "        \n",
    "\n",
    "with open(\"/home/chase/projects/peakload/data/ercot/load_features.json\", 'w') as d:\n",
    "    out = json.dumps(out_features)\n",
    "    d.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reload feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature construction\n",
    "\n",
    "lookback = 24*7\n",
    "\n",
    "#training data input arrays, output_arrays 24 hours ahead\n",
    "training_data_pairs = []\n",
    "\n",
    "for i in range(len(trainhours)-lookback-24): #must offset for last 24 hours in dataset\n",
    "    currtime = trainhours[i]\n",
    "    data_point = np.zeros((lookback, 28)) #current number of features\n",
    "    label = np.zeros((24,1))\n",
    "    for j in range(lookback):\n",
    "        features = load_features[currtime]\n",
    "        data_point[j,0] = train_data_vec[i+j]\n",
    "        data_point[j,1:8] = features[\"hourhist\"]\n",
    "        data_point[j,8:15] = features[\"dayhourhist\"]\n",
    "        data_point[j,16] = features[\"dayhourmax\"]\n",
    "        data_point[j,17] = features[\"dayhourmin\"]\n",
    "        data_point[j,18] = features[\"dayhourmean\"]\n",
    "        data_point[j,19] = features[\"dayhourvar\"]\n",
    "        data_point[j,20] = features[\"hourmax\"]\n",
    "        data_point[j,21] = features[\"hourmin\"]\n",
    "        data_point[j,22] = features[\"hourmean\"]\n",
    "        data_point[j,23] = features[\"hourvar\"]\n",
    "        data_point[j,24] = features[\"month\"]\n",
    "        data_point[j,25] = features[\"week\"]\n",
    "        data_point[j,26] = features[\"weekday\"]\n",
    "        data_point[j,27] = features[\"season\"]\n",
    "    for j in range(24):\n",
    "        label[j,0] = train_data_labels[(i+24)+j]\n",
    "    training_data_pairs.append((data_point, label))\n",
    "\n",
    "test_data_pairs = []\n",
    "for i in range(len(valhours)-lookback-24):\n",
    "    currtime = valhours[i]\n",
    "    data_point = np.zeros((lookback,28))\n",
    "    label = np.zeros((24,1))\n",
    "    for j in range(lookback):\n",
    "        features = load_features[currtime]\n",
    "        data_point[j,0] = val_data[i+j]\n",
    "        data_point[j,1:8] = features[\"hourhist\"]\n",
    "        data_point[j,8:15] = features[\"dayhourhist\"]\n",
    "        data_point[j,16] = features[\"dayhourmax\"]\n",
    "        data_point[j,17] = features[\"dayhourmin\"]\n",
    "        data_point[j,18] = features[\"dayhourmean\"]\n",
    "        data_point[j,19] = features[\"dayhourvar\"]\n",
    "        data_point[j,20] = features[\"hourmax\"]\n",
    "        data_point[j,21] = features[\"hourmin\"]\n",
    "        data_point[j,22] = features[\"hourmean\"]\n",
    "        data_point[j,23] = features[\"hourvar\"]\n",
    "        data_point[j,24] = features[\"month\"]\n",
    "        data_point[j,25] = features[\"week\"]\n",
    "        data_point[j,26] = features[\"weekday\"]\n",
    "        data_point[j,27] = features[\"season\"]\n",
    "    for j in range(24):\n",
    "        label[j,0] = val_data_labels[(i+24)+j]\n",
    "    test_data_pairs.append((data_point, label))\n",
    "    \n",
    "all_data_pairs = training_data_pairs + test_data_pairs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "normcolumns = [0, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
    "\n",
    "colmaxs = np.zeros((1,28))\n",
    "colmins = np.zeros((1,28))\n",
    "for pair in all_data_pairs:\n",
    "    currmax = np.max(pair[0],axis=0)\n",
    "    currmax = currmax[np.newaxis,:]\n",
    "    currmin = np.min(pair[0],axis=0)\n",
    "    currmin = currmin[np.newaxis,:]\n",
    "    \n",
    "    cmax = np.concatenate((colmaxs, currmax), axis=0)\n",
    "    cmin = np.concatenate((colmins, currmin), axis=0)\n",
    "    \n",
    "    colmaxs = np.max(cmax, axis=0)[np.newaxis,:]\n",
    "    colmins = np.min(cmin, axis=0)[np.newaxis,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.62225388,  0.63982049,  0.66447699,  0.66209164,  0.64650042,\n",
       "        0.62037223,  0.58126031,  0.54566653,  0.52258983,  0.51207992,\n",
       "        0.50926412,  0.51098238,  0.526481  ,  0.56037706,  0.60994559,\n",
       "        0.62627455,  0.62099246,  0.61905359,  0.61006128,  0.5994832 ,\n",
       "        0.58605415,  0.57489956,  0.56218268,  0.5545562 ,  0.55391924,\n",
       "        0.56608222,  0.59392455,  0.60310894,  0.59778975,  0.58581418,\n",
       "        0.56294497,  0.5404721 ,  0.5233133 ,  0.51545999,  0.51429912,\n",
       "        0.5165894 ,  0.52474984,  0.54258038,  0.56528528,  0.58511312,\n",
       "        0.59829371,  0.59996695,  0.58697286,  0.56131656,  0.53085642,\n",
       "        0.50073705,  0.47436781,  0.45615193,  0.45185108,  0.4635578 ,\n",
       "        0.50354332,  0.52213457,  0.52280614,  0.51631834,  0.50149007,\n",
       "        0.48327012,  0.46723709,  0.45931745,  0.45659349,  0.45803619,\n",
       "        0.46521167,  0.47881139,  0.49834902,  0.51189935,  0.51947899,\n",
       "        0.5125051 ,  0.49993991,  0.48709956,  0.4792548 ,  0.47509357,\n",
       "        0.47125965,  0.47396736,  0.48056142,  0.49758782,  0.53170077,\n",
       "        0.54760354,  0.54916536,  0.5428948 ,  0.52616996,  0.50581088,\n",
       "        0.49560233,  0.49611597,  0.50124859,  0.51064239,  0.5314766 ,\n",
       "        0.57177215,  0.62994327,  0.6517474 ,  0.63651568,  0.62366507,\n",
       "        0.60862634,  0.58777268,  0.56492398,  0.54412503,  0.5239092 ,\n",
       "        0.51057753,  0.50654079,  0.52168742,  0.56971812,  0.59753355,\n",
       "        0.60046167,  0.59164586,  0.56411322,  0.53477681,  0.52008326,\n",
       "        0.51739742,  0.52366507,  0.53351956,  0.55429718,  0.60044479,\n",
       "        0.66606271,  0.67530683,  0.63436979,  0.59521187,  0.56396415,\n",
       "        0.53550463,  0.51230159,  0.4948911 ,  0.47877259,  0.46969844,\n",
       "        0.4695624 ,  0.48445983,  0.5342305 ,  0.56452838,  0.57091998,\n",
       "        0.56296972,  0.53500903,  0.50867477,  0.49481322,  0.49250195,\n",
       "        0.49828509,  0.50894888,  0.53209342,  0.58045831,  0.65267369,\n",
       "        0.66155713,  0.61590454,  0.57667141,  0.54419073,  0.51580192,\n",
       "        0.49104103,  0.47470869,  0.45916325,  0.44967375,  0.44846826,\n",
       "        0.46091444,  0.50319275,  0.53001396,  0.53763281,  0.53124824,\n",
       "        0.50449585,  0.47605456,  0.46166921,  0.45852065,  0.4624726 ,\n",
       "        0.47180857,  0.49357675,  0.54041817,  0.61077004,  0.62133455,\n",
       "        0.58172621,  0.54631212,  0.51982558,  0.49524028,  0.47763333,\n",
       "        0.46459181,  0.45270534,  0.44558611])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.96204076e+04,   8.81355932e-01,   7.28813559e-01,\n",
       "          5.93220339e-01,   1.00000000e+00,   4.33333333e-01,\n",
       "          6.00000000e-01,   5.00000000e-01,   8.33333333e-01,\n",
       "          6.66666667e-01,   6.66666667e-01,   1.00000000e+00,\n",
       "          5.00000000e-01,   5.55555556e-01,   6.66666667e-01,\n",
       "          0.00000000e+00,   6.96204076e+04,   5.21139666e+04,\n",
       "          6.14793360e+04,   1.90730266e+08,   6.96204076e+04,\n",
       "          5.36184458e+04,   6.36651761e+04,   9.78771220e+07,\n",
       "          1.20000000e+01,   5.30000000e+01,   6.00000000e+00,\n",
       "          3.00000000e+00]])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colmaxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_data_pairs = []\n",
    "\n",
    "for pair in all_data_pairs:\n",
    "    normedarr = pair[0]\n",
    "    for column in normcolumns:\n",
    "        coldata = copy.copy(pair[0][:,column])\n",
    "        coldata = (1.0/(colmaxs[0,column] - colmins[0,column])) * (pair[0][:,column] - colmins[0,column])\n",
    "        normedarr[:,column] = coldata.T\n",
    "    \n",
    "    normed_data_pairs.append((normedarr, pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle training/test data pairs, split out validation\n",
    "import random\n",
    "\n",
    "random.shuffle(training_data_pairs)\n",
    "training_pairs = training_data_pairs[0:int(0.9*float(len(training_data_pairs)))]\n",
    "val_pairs = training_data_pairs[int(0.9*float(len(training_data_pairs))):]\n",
    "\n",
    "random.shuffle(test_data_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sample_list(datalist, batchsize):\n",
    "    remainder = len(datalist) % batchsize\n",
    "    diff = batchsize - remainder\n",
    "    tail = datalist[-diff:] + datalist[0:remainder]\n",
    "    out = [ datalist[i*batchsize:(i+1)*batchsize] for i in range(int(float(len(datalist))/float(batchsize)))]\n",
    "    out = out + [tail]\n",
    "    return(out)\n",
    "\n",
    "def torch_reshape_data(databatch):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for sample in databatch:\n",
    "        inputs.append(sample[0].flatten())\n",
    "        labels.append(sample[1].flatten())\n",
    "    return(torch.Tensor(np.asarray(inputs)), torch.Tensor(np.asarray(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainbatches = batch_sample_list(val_pairs, 10)\n",
    "inputs, labels = torch_reshape_data(trainbatches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.FloatTensor, torch.Size([10, 24]))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs), labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472\n",
      "47.2\n"
     ]
    }
   ],
   "source": [
    "train_batches = batch_sample_list(training_pairs, 100)\n",
    "num_batches = len(train_batches)\n",
    "tenth = num_batches/10\n",
    "\n",
    "print(num_batches)\n",
    "print(tenth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_loss(net_obj, datalist):\n",
    "    losses = []\n",
    "    for i, data, in enumerate(datalist):\n",
    "        inputs, labels = torch_reshape_data([data])\n",
    "        labels = Variable(labels.cuda())\n",
    "        outputs = net_obj(Variable(inputs.cuda()))\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.data[0])\n",
    "    return(np.mean(losses))\n",
    "    \n",
    "\n",
    "def train(net_obj, loss_fnc, opt_fnc, traindata, valdata, batchsize=100, epochs=100):  \n",
    "    train_batches = batch_sample_list(traindata, batchsize)\n",
    "    num_batches = len(train_batches)\n",
    "    tenth = int(num_batches/10)\n",
    "    \n",
    "    training_epoch_loss = []\n",
    "    train_epoch_set_loss = []\n",
    "    val_epoch_set_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainbatches):\n",
    "            inputs, labels = torch_reshape_data(data)\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda()) #they need to be .cuda() with each training epoch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net_obj(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "\n",
    "        #training loss\n",
    "        print(running_loss)\n",
    "        training_epoch_loss.append(running_loss)\n",
    "       \n",
    "    #epoch set loss\n",
    "    train_set_loss = set_loss(net_obj, traindata)\n",
    "    print(\"training set loss: \" + str(train_set_loss))\n",
    "    train_epoch_set_loss.append(train_set_loss)\n",
    "    val_set_loss = set_loss(net_obj, valdata)\n",
    "    print(\"val set loss:\" + str(val_set_loss))\n",
    "    val_epoch_set_loss.append(val_set_loss)\n",
    "                \n",
    "    return(training_epoch_loss, train_epoch_set_loss, val_epoch_set_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class linear_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(linear_net, self).__init__()\n",
    "        self.D_in = lookback * 28\n",
    "        self.H1 = 250\n",
    "        self.H2 = 150\n",
    "        self.H3 = 50\n",
    "        self.D_out = 24\n",
    "        self.l1 = nn.Linear(self.D_in, self.H1)\n",
    "        self.l2 = nn.Linear(self.H1, self.H2)\n",
    "        self.l3 = nn.Linear(self.H2, self.H3)\n",
    "        self.l4 = nn.Linear(self.H3, self.D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.l1(F.dropout(x)))\n",
    "        x = F.sigmoid(self.l2(x))\n",
    "        x = F.softmax(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class conv_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(linear_net, self).__init__()\n",
    "        self.D_in = lookback * 28\n",
    "        self.H = 250\n",
    "        self.window = 2\n",
    "        self.pool = 10\n",
    "        self.D_out = 24\n",
    "        self.l1 = nn.Linear(self.D_in, self.H)\n",
    "        self.conv1 = nn.Conv1d(self.H, self.H, self.window)\n",
    "        self.l2 = nn.Linear(self.H, self.D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.avg_pool1d(self.conv1(x),self.pool)\n",
    "        x = self.l2(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100 #batch size\n",
    "\n",
    "net = linear_net().cuda()\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229.7384566962719\n",
      "172.11543729901314\n",
      "148.25309224426746\n",
      "138.3695435076952\n",
      "134.2889770269394\n",
      "132.62611673772335\n",
      "131.93922066688538\n",
      "131.65856318175793\n",
      "131.54775670170784\n",
      "131.50199507176876\n",
      "131.48302605748177\n",
      "131.47487179934978\n",
      "131.47100611031055\n",
      "131.46922755241394\n",
      "131.4680959135294\n",
      "131.46749185025692\n",
      "131.46715453267097\n",
      "131.46688525378704\n",
      "131.4667161256075\n",
      "131.46649700403214\n",
      "131.46631629765034\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-405-b2c3635d7584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-403-abf203694258>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net_obj, loss_fnc, opt_fnc, traindata, valdata, batchsize, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_reshape_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# wrap them in Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-257-2345be6bf7c0>\u001b[0m in \u001b[0;36mtorch_reshape_data\u001b[0;34m(databatch)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, test_accs = train(net, criterion, optimizer, training_pairs, val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = torch_reshape_data(training_pairs[0:10])\n",
    "labels = Variable(labels.cuda())\n",
    "outputs = net(Variable(inputs.cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.5055  0.5075  0.5062  0.5013  0.5014  0.4986  0.5043  0.5084  0.5039  0.5016\n",
       " 0.5056  0.5068  0.5062  0.5006  0.5024  0.4982  0.5051  0.5087  0.5041  0.5011\n",
       " 0.5053  0.5073  0.5063  0.5013  0.5013  0.4986  0.5045  0.5082  0.5039  0.5020\n",
       " 0.5054  0.5073  0.5064  0.5012  0.5012  0.4986  0.5046  0.5081  0.5037  0.5018\n",
       " 0.5054  0.5069  0.5066  0.5011  0.5014  0.4988  0.5051  0.5084  0.5039  0.5018\n",
       " 0.5056  0.5068  0.5062  0.5007  0.5023  0.4984  0.5050  0.5086  0.5040  0.5014\n",
       " 0.5056  0.5068  0.5062  0.5007  0.5023  0.4984  0.5050  0.5086  0.5040  0.5014\n",
       " 0.5053  0.5072  0.5068  0.5012  0.5011  0.4986  0.5044  0.5080  0.5036  0.5017\n",
       " 0.5053  0.5067  0.5067  0.5005  0.5028  0.4981  0.5053  0.5086  0.5042  0.5009\n",
       " 0.5055  0.5074  0.5064  0.5013  0.5012  0.4986  0.5044  0.5082  0.5036  0.5018\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.5076  0.5027  0.5021  0.4996  0.5019  0.5042  0.5030  0.4997  0.5068  0.5069\n",
       " 0.5072  0.5021  0.5017  0.4997  0.5011  0.5040  0.5034  0.4999  0.5065  0.5070\n",
       " 0.5077  0.5028  0.5020  0.4997  0.5020  0.5042  0.5028  0.4998  0.5070  0.5070\n",
       " 0.5075  0.5029  0.5017  0.4996  0.5017  0.5042  0.5025  0.4997  0.5068  0.5073\n",
       " 0.5076  0.5029  0.5018  0.4996  0.5016  0.5040  0.5026  0.4996  0.5068  0.5071\n",
       " 0.5074  0.5023  0.5017  0.4997  0.5013  0.5039  0.5032  0.4998  0.5067  0.5070\n",
       " 0.5074  0.5023  0.5017  0.4997  0.5013  0.5039  0.5032  0.4998  0.5067  0.5070\n",
       " 0.5076  0.5028  0.5017  0.4995  0.5017  0.5042  0.5026  0.4998  0.5067  0.5072\n",
       " 0.5065  0.5022  0.5012  0.5000  0.5009  0.5038  0.5031  0.5002  0.5063  0.5071\n",
       " 0.5075  0.5029  0.5020  0.4997  0.5017  0.5041  0.5029  0.4998  0.5069  0.5069\n",
       "\n",
       "Columns 20 to 23 \n",
       " 0.5113  0.5081  0.5077  0.5058\n",
       " 0.5114  0.5085  0.5075  0.5056\n",
       " 0.5114  0.5081  0.5077  0.5060\n",
       " 0.5114  0.5083  0.5080  0.5061\n",
       " 0.5116  0.5082  0.5081  0.5062\n",
       " 0.5114  0.5084  0.5077  0.5057\n",
       " 0.5114  0.5084  0.5077  0.5057\n",
       " 0.5115  0.5083  0.5081  0.5062\n",
       " 0.5118  0.5083  0.5078  0.5061\n",
       " 0.5114  0.5082  0.5078  0.5060\n",
       "[torch.cuda.FloatTensor of size 10x24 (GPU 0)]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff6bc87e0b8>]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff70128c0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(outputs.data[0,:].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, lookback*28, 250, 24\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs torch.Size([3, 5, 7])\n",
      "hidden torch.Size([2, 5, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.c1 = nn.Conv1d(input_size, hidden_size, 2)\n",
    "        self.p1 = nn.AvgPool1d(2)\n",
    "        self.c2 = nn.Conv1d(hidden_size, hidden_size, 1)\n",
    "        self.p2 = nn.AvgPool1d(2)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=0.01)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        batch_size = inputs.size(1)\n",
    "        \n",
    "        # Turn (seq_len x batch_size x input_size) into (batch_size x input_size x seq_len) for CNN\n",
    "        inputs = inputs.transpose(0, 1).transpose(1, 2)\n",
    "\n",
    "        # Run through Conv1d and Pool1d layers\n",
    "        c = self.c1(inputs)\n",
    "        p = self.p1(c)\n",
    "        c = self.c2(p)\n",
    "        p = self.p2(c)\n",
    "\n",
    "        # Turn (batch_size x hidden_size x seq_len) back into (seq_len x batch_size x hidden_size) for RNN\n",
    "        p = p.transpose(1, 2).transpose(0, 1)\n",
    "        \n",
    "        p = F.tanh(p)\n",
    "        output, hidden = self.gru(p, hidden)\n",
    "        conv_seq_len = output.size(0)\n",
    "        output = output.view(conv_seq_len * batch_size, self.hidden_size) # Treating (conv_seq_len x batch_size) as batch_size for linear layer\n",
    "        output = F.tanh(self.out(output))\n",
    "        output = output.view(conv_seq_len, -1, self.output_size)\n",
    "        return output, hidden\n",
    "\n",
    "input_size = 20\n",
    "hidden_size = 50\n",
    "output_size = 7\n",
    "batch_size = 5\n",
    "n_layers = 2\n",
    "seq_len = 15\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, output_size, n_layers=n_layers)\n",
    "\n",
    "inputs = Variable(torch.rand(seq_len, batch_size, input_size)) # seq_len x batch_size x \n",
    "outputs, hidden = rnn(inputs, None)\n",
    "print('outputs', outputs.size()) # conv_seq_len x batch_size x output_size\n",
    "print('hidden', hidden.size()) # n_layers x batch_size x hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 5, 20])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
