{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import urllib3\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime\n",
    "from dateutil import tz\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pytz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#daylight savings time dates\n",
    "2010: March 14 2 AM  PST --> PDT Offset \n",
    "      Nov 7 2 AM    PDT --> PST\n",
    "2011: March 13\n",
    "      Nov 6\n",
    "2012: March 11\n",
    "      Nov 4\n",
    "2013: March 10\n",
    "      Nov 3\n",
    "2014: March 9\n",
    "      Nov 2\n",
    "2015: March 8\n",
    "      Nov 1\n",
    "2016: March 13\n",
    "      Nov 6\n",
    "2017: March 12\n",
    "      Nov 5\n",
    "2018: March 11\n",
    "      Nov 4\n",
    "        \n",
    "#what happens in ERCOT data before and after a time zone event\n",
    "#ERCOT data has missing hour during March daylight savings hour skip 23 rows long CDT UTC - 5\n",
    "#ERCOT data has extra hour during November daylight savings hour fallback 25 rows long CST UTC - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#request weather by cities\n",
    "#all cities are CST/CDT\n",
    "cities = {\"Witchita Falls\": (33.9137, -98.4934), \n",
    "          \"Mineral Wells\": (32.8085, -98.1128), \n",
    "          \"Paris\": (33.6609, -95.5555), \n",
    "          \"Dallas\": (32.7767, -96.7970), \n",
    "          \"Fort Worth\": (32.7555, -97.3308), \n",
    "          \"Tyler\": (32.3513, -95.3011),  \n",
    "          \"Waco\": (31.5493, -97.1467), \n",
    "          \"San Angelo\": (31.4638, -100.4370), \n",
    "          \"Abilene\": (32.4487, -99.7331), \n",
    "          \"Midland\": (31.9973, -102.0779), \n",
    "          \"Wink\": (31.7512, -103.1599), \n",
    "          \"Junction\": (30.4894, -99.7720), \n",
    "          \"Austin\": (30.2672, -97.7431), \n",
    "          \"Houston\": (29.7604, -95.3698), \n",
    "          \"Galveston\": (29.3013, -94.7977), \n",
    "          \"Victoria\": (28.8053, -97.0036), \n",
    "          \"Corpus Christi\": (27.8006, -97.39640), \n",
    "          \"Laredo\": (27.5306, -99.4803),\n",
    "          \"San Antonio\": (29.4241, -98.4936)}\n",
    "\n",
    "cdt_starts = [datetime.datetime(year=2010, month=3, day=14, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2011, month=3, day=13, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2012, month=3, day=11, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2013, month=3, day=10, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2014, month=3, day=9, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2015, month=3, day=8, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2016, month=3, day=13, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2017, month=3, day=12, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2018, month=3, day=11, hour=2, minute=0, second=0)]\n",
    "\n",
    "cst_starts = [datetime.datetime(year=2010, month=11, day=7, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2011, month=11, day=6, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2012, month=11, day=4, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2013, month=11, day=3, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2014, month=11, day=2, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2015, month=11, day=1, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2016, month=11, day=6, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2017, month=11, day=5, hour=2, minute=0, second=0),\n",
    "             datetime.datetime(year=2018, month=11, day=4, hour=2, minute=0, second=0)]\n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2010, 1, 1, 2, 0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.utcfromtimestamp(alldays[0]) #- datetime.timedelta(hours=5) #5 hours during CDT, 5 hours during CST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#startdate = datetime.datetime(year=2010, month=1, day=1, hour=0, minute=0, second=0) #naive local time PST\n",
    "startdate = 1262325600\n",
    "curr = startdate\n",
    "#enddate = datetime.datetime(year=2018, month=1, day=1, hour=0, minute=0, second=0)\n",
    "enddate = 1514786400\n",
    "\n",
    "alldays = []\n",
    "\n",
    "while curr <= enddate:\n",
    "    alldays.append(curr) #need to shift for daylight savings\n",
    "    curr += 24 * 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1514872800"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldays[-1] + 24*3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#startdate = datetime.datetime(year=2010, month=1, day=1, hour=0, minute=0, second=0) #naive local time PST\n",
    "startdate = 1262325600\n",
    "curr = startdate\n",
    "#enddate = datetime.datetime(year=2018, month=1, day=1, hour=0, minute=0, second=0)\n",
    "enddate = 1514786400\n",
    "\n",
    "allhours = []\n",
    "\n",
    "while curr <= enddate:\n",
    "    allhours.append(curr) #need to shift for daylight savings\n",
    "    curr += 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1446361200 in allhours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "#files names are grid_longitude,latitude.json\n",
    "tz = pytz.timezone('America/Chicago')\n",
    "\n",
    "time_keys_to_norm = ['sunriseTime', 'sunsetTime', 'temperatureHighTime', 'temperatureLowTime', 'temperatureMinTime', 'temperatureMaxTime', 'apparentTemperatureMinTime', 'apparentTemperatureMaxTime', 'precipIntensityMaxTime', 'time', 'apparentTemperatureLowTime', 'apparentTemperatureHighTime']\n",
    "values_to_norm = ['temperature', 'apparentTemperature', 'dewPoint', 'humidity', 'windSpeed', 'windBearing', 'visibility', 'temperatureHigh', 'temperatureLow', 'apparentTemperatureHigh', 'apparentTemperatureLow', 'pressure', 'temperatureMin', 'temperatureMax', 'apparentTemperatureMax', 'apparentTemperatureMin', 'precipIntensity', 'precipIntensityMax', 'precipAccumulation']\n",
    "str_to_norm = {'icon': ['clear-day', 'clear-night', 'cloudy', 'fog', 'partly-cloudy-day', 'partly-cloudy-night', 'rain', 'snow', 'wind'], 'precipType': ['snow', 'rain']}\n",
    "icons = ['clear-day', 'clear-night', 'cloudy', 'fog', 'partly-cloudy-day', 'partly-cloudy-night', 'rain', 'snow', 'wind']\n",
    "precips = ['snow', 'rain']\n",
    "values_no_norm = ['cloudCover', 'moonPhase', 'precipProbability']\n",
    "values_to_drop = ['latitude', 'longitude', 'summary']\n",
    "\n",
    "def parse_filename(fnamestr):\n",
    "    token = fnamestr.split(\"_\")[1]\n",
    "    token_ = token[:-5] #break off .json extension\n",
    "    tokens = token_.split(\",\")\n",
    "    #return (lat, long)\n",
    "    return((float(tokens[1]), float(tokens[0])))\n",
    "\n",
    "def norm_to_time_of_day(unixtime):\n",
    "    if np.isnan(unixtime):\n",
    "        val = np.nan\n",
    "    else:\n",
    "        dt = datetime.datetime.fromtimestamp(unixtime, tz)\n",
    "        h = dt.hour * 3600 + dt.minute * 60 + dt.second \n",
    "        val = h/86400\n",
    "    return(val)\n",
    "\n",
    "def str_to_norm_func(strin, key):\n",
    "    if type(strin) == str:\n",
    "        vals = sorted(list(str_to_norm[key]))\n",
    "        i = vals.index(strin)\n",
    "        denom = len(vals) - 1\n",
    "        out = i/denom\n",
    "    else:\n",
    "        out = np.nan\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global variables\n",
    "\n",
    "weatherdir = \"/home/chase/projects/peakload/data/weather/ercot/major_cities/\"\n",
    "outdir = weatherdir + \"array_formatted/\"\n",
    "years = range(2010,2018)\n",
    "tkeys = ['currently', 'daily', 'hourly', 'latitude', 'longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#want to scan ahead to get all keys and use map for missing data points\n",
    "#dont' need k + i + lk, just want lk's for each hour\n",
    "lkeys = {'daily': set(), 'hourly': set()}\n",
    "\n",
    "grid_locs = set()\n",
    "\n",
    "for year in years:\n",
    "    days = os.listdir(weatherdir + str(year))\n",
    "    for day in days:\n",
    "        wms = os.listdir(weatherdir + str(year) + \"/\" + day)\n",
    "        for fname in wms:\n",
    "            loc = parse_filename(fname)\n",
    "            sloc = str(loc[0]) + \",\" + str(loc[1])\n",
    "            grid_locs.add(sloc)\n",
    "            with open(weatherdir + str(year) + \"/\" + day + \"/\" + fname, 'r') as d:\n",
    "                data = json.load(d)\n",
    "            for k in tkeys:\n",
    "                if k == 'daily' or k == 'hourly':\n",
    "                    curr = data[k]['data'] #list of dicts\n",
    "                    for di in range(len(curr)):\n",
    "                        d = curr[di]\n",
    "                        for lk in d:\n",
    "                            lkeys[k].add(lk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_locs = sorted(list(grid_locs))\n",
    "with open(outdir + \"grid_locs.pck\", 'wb') as f:\n",
    "    pickle.dump(grid_locs, f)\n",
    "    \n",
    "with open(outdir + \"city_locs.pck\", 'wb') as f:\n",
    "    pickle.dump(cities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'daily': {'apparentTemperatureHigh': 0, 'temperatureHighTime': 1, 'sunriseTime': 2, 'apparentTemperatureMax': 3, 'icon': 4, 'windSpeed': 5, 'windBearing': 6, 'moonPhase': 7, 'pressure': 8, 'apparentTemperatureMinTime': 9, 'apparentTemperatureLowTime': 10, 'apparentTemperatureHighTime': 11, 'temperatureMin': 12, 'precipIntensity': 13, 'visibility': 14, 'temperatureLowTime': 15, 'temperatureMinTime': 16, 'apparentTemperatureMaxTime': 17, 'dewPoint': 18, 'precipIntensityMax': 19, 'humidity': 20, 'sunsetTime': 21, 'time': 22, 'precipType': 23, 'temperatureHigh': 24, 'precipIntensityMaxTime': 25, 'precipAccumulation': 26, 'precipProbability': 27, 'temperatureMaxTime': 28, 'apparentTemperatureLow': 29, 'temperatureMax': 30, 'cloudCover': 31, 'summary': 32, 'apparentTemperatureMin': 33, 'temperatureLow': 34}, 'hourly': {'temperature': 0, 'humidity': 1, 'icon': 2, 'time': 3, 'precipType': 4, 'windSpeed': 5, 'apparentTemperature': 6, 'summary': 7, 'cloudCover': 8, 'precipAccumulation': 9, 'windBearing': 10, 'precipIntensity': 11, 'visibility': 12, 'precipProbability': 13, 'dewPoint': 14, 'pressure': 15}}\n"
     ]
    }
   ],
   "source": [
    "#for each hour, create a vector with an element for each key in lkeys\n",
    "#assign an index for each element in lkeys\n",
    "\n",
    "lkeymap = {}\n",
    "daily = 0\n",
    "hourly = 0\n",
    "for tk in lkeys:\n",
    "    lkeymap[tk] = {}\n",
    "    for lk in lkeys[tk]:\n",
    "        if lk == 'latitude' or lk == 'longitude':\n",
    "            pass\n",
    "        elif tk == 'daily':\n",
    "            lkeymap[tk][lk] = daily\n",
    "            daily += 1\n",
    "        elif tk == 'hourly':\n",
    "            lkeymap[tk][lk] = hourly\n",
    "            hourly += 1\n",
    "\n",
    "print(lkeymap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save lkeymap to \n",
    "with open(outdir + \"data_keys.pck\", 'wb') as d:\n",
    "    pickle.dump(lkeymap, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hourly_data = {}\n",
    "daily_data = {}\n",
    "\n",
    "for h in allhours:\n",
    "    hourly_data[h] = {}\n",
    "    for l in grid_locs:\n",
    "        hourly_data[h][l] = {}\n",
    "\n",
    "for d in alldays:\n",
    "    daily_data[d] = {}\n",
    "    for l in grid_locs:\n",
    "        daily_data[d][l] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#daily[time] is coming back PST midnight, data is coming back correct to timezone\n",
    "\n",
    "for year in years:\n",
    "    days = os.listdir(weatherdir + str(year))\n",
    "    for day in days:\n",
    "        wms = os.listdir(weatherdir + str(year) + \"/\" + day)\n",
    "        for fname in wms:\n",
    "            loc = parse_filename(fname)\n",
    "            sloc = str(loc[0]) + \",\" + str(loc[1])\n",
    "            loc_i = grid_locs.index(sloc)\n",
    "            with open(weatherdir + str(year) + \"/\" + day + \"/\" + fname, 'r') as d:\n",
    "                data = json.load(d)\n",
    "\n",
    "            for k in tkeys:\n",
    "                if k == 'hourly':\n",
    "                    curr = data[k]['data'] #list of dicts\n",
    "                    \n",
    "                    #need to just pull out time stamps from data and start/stop accordingly\n",
    "                    for i in range(len(curr)):\n",
    "                        time_int = curr[i]['time']\n",
    "                        if time_int not in hourly_data.keys():\n",
    "                            print(\"hourly\", time_int)\n",
    "                            #print(curr)\n",
    "                            pass\n",
    "                        else:\n",
    "                            for key in curr[i]:\n",
    "                                hourly_data[time_int][sloc][key] = curr[i][key]\n",
    "                                \n",
    "                elif k == 'daily':\n",
    "                    d = data[k]['data'][0]\n",
    "                    time_int = int(day)\n",
    "                    if time_int not in daily_data.keys():\n",
    "                        #print('daily', time_int)\n",
    "                        daily_data[time_int] = {}\n",
    "                        for l in grid_locs:\n",
    "                            daily_data[time_int][l] = {}\n",
    "                    else:\n",
    "                        for key in d:\n",
    "                            daily_data[time_int][sloc][key] = d[key]\n",
    "        \n",
    "#all_days_hourly_arrays[day] = allhours_outvector_gridlocs\n",
    "with open(outdir + \"daily_weather_dict.pck\", 'wb') as f:\n",
    "    pickle.dump(daily_data, f)\n",
    "        \n",
    "#all_days_hourly_arrays[day] = allhours_outvector_gridlocs\n",
    "with open(outdir + \"hourly_weather_dict.pck\", 'wb') as f:\n",
    "    pickle.dump(hourly_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mins and maxes to normalize data\n",
    "minmaxs = {}\n",
    "for hour in hourly_data.keys():\n",
    "    for loc in hourly_data[hour].keys():\n",
    "        curr = hourly_data[hour][loc]\n",
    "        for k in values_to_norm:\n",
    "            if k not in minmaxs.keys() and k in lkeymap['hourly'].keys():\n",
    "                if k not in curr.keys():\n",
    "                    minmaxs[k] = {'min': 1000.0, 'max': -1000.0}\n",
    "                else:\n",
    "                    minmaxs[k] = {'min': curr[k], 'max': curr[k]}\n",
    "            elif k in lkeymap['hourly'].keys():\n",
    "                if k not in curr.keys():\n",
    "                    pass\n",
    "                else:\n",
    "                    if curr[k] < minmaxs[k]['min']:\n",
    "                        minmaxs[k]['min'] = curr[k]\n",
    "                    if curr[k] > minmaxs[k]['max']:\n",
    "                        minmaxs[k]['max'] = curr[k]\n",
    "                        \n",
    "for day in daily_data.keys():\n",
    "    for loc in daily_data[day].keys():\n",
    "        curr = daily_data[day][loc]\n",
    "        for k in values_to_norm:\n",
    "            if k not in minmaxs.keys() and k in lkeymap['daily'].keys():\n",
    "                if k not in curr.keys():\n",
    "                    minmaxs[k] = {'min': 1000.0, 'max': -1000.0}\n",
    "                else:\n",
    "                    minmaxs[k] = {'min': curr[k], 'max': curr[k]}\n",
    "            elif k in lkeymap['daily'].keys():\n",
    "                if k not in curr.keys():\n",
    "                    pass\n",
    "                else:\n",
    "                    if curr[k] < minmaxs[k]['min']:\n",
    "                        minmaxs[k]['min'] = curr[k]\n",
    "                    if curr[k] > minmaxs[k]['max']:\n",
    "                        minmaxs[k]['max'] = curr[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apparentTemperature': {'max': 117.45, 'min': -9.37},\n",
       " 'apparentTemperatureHigh': {'max': 117.45, 'min': 2.19},\n",
       " 'apparentTemperatureLow': {'max': 99.45, 'min': -9.37},\n",
       " 'apparentTemperatureMax': {'max': 117.45, 'min': 6.33},\n",
       " 'apparentTemperatureMin': {'max': 98.05, 'min': -9.37},\n",
       " 'dewPoint': {'max': 82.24, 'min': -13.06},\n",
       " 'humidity': {'max': 1, 'min': 0.03},\n",
       " 'precipAccumulation': {'max': 5.177, 'min': 0},\n",
       " 'precipIntensity': {'max': 4.0215, 'min': 0},\n",
       " 'precipIntensityMax': {'max': 4.0215, 'min': 0},\n",
       " 'pressure': {'max': 1048.73, 'min': 986.8},\n",
       " 'temperature': {'max': 111.34, 'min': 5.09},\n",
       " 'temperatureHigh': {'max': 111.34, 'min': 16.6},\n",
       " 'temperatureLow': {'max': 87.67, 'min': 5.09},\n",
       " 'temperatureMax': {'max': 111.34, 'min': 17.06},\n",
       " 'temperatureMin': {'max': 87.67, 'min': 5.09},\n",
       " 'visibility': {'max': 10, 'min': 0.03},\n",
       " 'windBearing': {'max': 359, 'min': 0},\n",
       " 'windSpeed': {'max': 43.7, 'min': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmaxs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "time_keys_to_norm = ['sunriseTime', 'sunsetTime', 'temperatureHighTime', 'temperatureLowTime', 'temperatureMinTime', 'temperatureMaxTime', 'apparentTemperatureMinTime', 'apparentTemperatureMaxTime', 'precipIntensityMaxTime', 'time']\n",
    "values_to_norm = ['temperature', 'apparentTemperature', 'dewPoint', 'humidity', 'windSpeed', 'windBearing', 'visibility', 'temperatureHigh', 'temperatureLow', 'apparentTemperatureHigh', 'apparentTemperatureLow', 'pressure', 'temperatureMin', 'temperatureMax', 'apparentTemperatureMax', 'apparentTemperatureMin', 'precipIntensity', 'precipIntensityMax', 'precipAccumulation']\n",
    "str_to_norm = ['icon', 'precipType']\n",
    "values_no_norm = ['cloudCover', 'moonPhase', 'precipProbability']\n",
    "values_to_drop = ['latitude', 'longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want a dict that has a local timestamp as the start time, and an array of locations x features, only work with hourly weather data\n",
    "A = np.empty((len(grid_locs), len(lkeymap['hourly'])), dtype=np.float32)\n",
    "A.fill(np.nan)\n",
    "\n",
    "hourly_array_data = {}\n",
    "\n",
    "for hour in allhours:\n",
    "    hourly_array_data[hour] = np.copy(A)\n",
    "    for i in range(len(grid_locs)):\n",
    "        sloc = grid_locs[i]\n",
    "        tokens = sloc.split(\",\")\n",
    "        lat = float(tokens[0])\n",
    "        lon = float(tokens[1])\n",
    "        data = hourly_data[hour][grid_locs[i]]\n",
    "        for metric in lkeymap['hourly'].keys():\n",
    "            j = lkeymap['hourly'][metric]\n",
    "            if metric in values_to_drop:\n",
    "                pass\n",
    "            elif metric not in data.keys():\n",
    "                hourly_array_data[hour][i,j] = np.nan\n",
    "            elif metric in values_to_norm:\n",
    "                hourly_array_data[hour][i,j] = (data[metric] - minmaxs[metric]['min'])/(minmaxs[metric]['max'] - minmaxs[metric]['min'])\n",
    "            elif metric in str_to_norm:\n",
    "                hourly_array_data[hour][i,j] = str_to_norm_func(data[metric], metric)\n",
    "            elif metric in time_keys_to_norm:\n",
    "                hourly_array_data[hour][i,j] = norm_to_time_of_day(data[metric])\n",
    "            elif metric in values_no_norm:\n",
    "                hourly_array_data[hour][i,j] = data[metric]\n",
    "            else:\n",
    "                print(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nans = np.zeros((hourly_array_data[allhours[0]].shape[1],))\n",
    "total = 0\n",
    "for t in allhours:\n",
    "    for l in range(daily_array_data[t].shape[0]):\n",
    "        total +=1\n",
    "        for j in range(daily_array_data[t].shape[1]):\n",
    "            if np.isnan(daily_arra_data[t][l,j]):\n",
    "                nans[j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want a dict that has a local timestamp as the start time, and an array of locations x features, only work with hourly weather data\n",
    "A = np.empty((len(grid_locs), len(lkeymap['daily'])), dtype=np.float32)\n",
    "A.fill(np.nan)\n",
    "\n",
    "daily_array_data = {}\n",
    "\n",
    "for hour in daily_data.keys():\n",
    "    #get day\n",
    "    dt = datetime.datetime.utcfromtimestamp(hour)\n",
    "    ndt = datetime.datetime(year=dt.year, month=dt.month, day=dt.day, hour=0, minute=0, second=0)\n",
    "    daily_array_data[ndt] = np.copy(A)\n",
    "    for i in range(len(grid_locs)):\n",
    "        sloc = grid_locs[i]\n",
    "        tokens = sloc.split(\",\")\n",
    "        lat = float(tokens[0])\n",
    "        lon = float(tokens[1])\n",
    "        data = daily_data[hour][grid_locs[i]]\n",
    "        for metric in lkeymap['daily'].keys():\n",
    "            j = lkeymap['daily'][metric]\n",
    "            if metric in values_to_drop:\n",
    "                pass\n",
    "            elif metric not in data.keys():\n",
    "                daily_array_data[ndt][i,j] = np.nan\n",
    "            elif metric in values_to_norm:\n",
    "                daily_array_data[ndt][i,j] = (data[metric] - minmaxs[metric]['min'])/(minmaxs[metric]['max'] - minmaxs[metric]['min'])\n",
    "            elif metric in str_to_norm:\n",
    "                daily_array_data[ndt][i,j] = str_to_norm_func(data[metric], metric)\n",
    "            elif metric in time_keys_to_norm:\n",
    "                daily_array_data[ndt][i,j] = norm_to_time_of_day(data[metric])\n",
    "            elif metric in values_no_norm:\n",
    "                daily_array_data[ndt][i,j] = data[metric]\n",
    "            else:\n",
    "                print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate where all the nan's are coming from\n",
    "daykeys = list(daily_array_data.keys())\n",
    "nans = np.zeros((daily_array_data[daykeys[0]].shape[1],))\n",
    "total = 0\n",
    "for t in daykeys:\n",
    "    for l in range(daily_array_data[t].shape[0]):\n",
    "        total+=1\n",
    "        for j in range(daily_array_data[t].shape[1]):\n",
    "            if np.isnan(daily_array_data[t][l,j]):\n",
    "                nans[j] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05295569, 0.05295569, 0.05295569, 0.05295569, 0.05295569,\n",
       "       0.05295569, 0.05295569, 0.05295569, 0.05295569, 0.05295569,\n",
       "       0.05295569, 0.05295569, 0.05295569, 0.05295569, 0.05295569,\n",
       "       0.05295569, 0.05295569, 0.05295569, 0.05295569, 0.05295569,\n",
       "       0.05295569, 0.05295569, 0.05295569, 0.65759764, 0.05295569,\n",
       "       0.5883285 , 0.99259953, 0.05295569, 0.05295569, 0.05295569,\n",
       "       0.05295569, 0.06979131, 1.        , 0.05295569, 0.05295569])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nans/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55537"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(daykeys) * len(grid_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-03-25 09:00:00 CDT-0500\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
