{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Insert description for CDC paper here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import *\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to reproduce paper results\n",
    "np.random.seed(4)\n",
    "torch.manual_seed(25) #not clear if this works in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global variables\n",
    "x_bar = 1.0   #maximum consumption\n",
    "noise_var = 1.0    #variance of noise values\n",
    "g = np.arange(0,1.1,0.1)  #discretization of space of power consumption values x \\in [0,\\bar{x}]\n",
    "s_vals = np.arange(-2.0,2.0,0.1)  #discretization of space of noise values, 98% of normal distribution with unit variance\n",
    "samples = np.random.normal(0, noise_var, size=100000)\n",
    "s_hist = np.histogram(samples, bins=np.arange(-2.0,2.1,0.1), normed=True)[0]\n",
    "s_hist = (1.0/np.linalg.norm(s_hist, ord=1)) * s_hist #probabilities over discretized noise values\n",
    "ramp_const = 0.3  #time coupling ramping constraint\n",
    "pi_cp_perc = 0.6  #ratio of pi_cp cost to total utility under a naive fixed strategy\n",
    "\n",
    "net_params = {'FEATURE_DIM': 4, 'OUTPUT_DIM': 1, 'HIDDEN_1': 16}\n",
    "\n",
    "base_path = \"/home/chase/projects/peakload/notebooks/\" #change this to path to notebook + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#basic functions\n",
    "\n",
    "def prob_t_is_max(max_so_far, t, T):\n",
    "    #conditionally independent probability that next iid sample of noise will be max over T\n",
    "    coeff = 1.0/(T - t)\n",
    "    prod = (1.0 - np.power(norm.cdf(max_so_far), T-t))\n",
    "    return(coeff*prod)\n",
    "\n",
    "def prob_any_next_t_is_max(max_so_far, t, T):\n",
    "    #probability that any next T-t samples will be maximum over sequence of length T\n",
    "    prod = (1.0 - np.power(norm.cdf(max_so_far), T-t))\n",
    "    return(prod)\n",
    "\n",
    "def log_utility(x):\n",
    "    #example utility function, continuously differentiable, monotonically increasing first derivative\n",
    "    #this example exhibits a diminishing return on increased power consumption x \n",
    "    y = 2.0*np.log(1 + np.power(x,1/2))\n",
    "    return(np.sum(y))\n",
    "\n",
    "def log_utility_opt_x(T, pi_cp):\n",
    "    frac = (2*T)/pi_cp\n",
    "    x_1 = frac + np.sqrt((frac - 1)*(frac + 1))\n",
    "    x_2 = frac - np.sqrt((frac - 1)*(frac + 1))\n",
    "    return((x_1, x_2))\n",
    "\n",
    "def log_utility_opt_x_single_t(pi_cp):\n",
    "    frac = (2)/pi_cp\n",
    "    x_1 = frac + np.sqrt((frac - 1)*(frac + 1))\n",
    "    x_2 = frac - np.sqrt((frac - 1)*(frac + 1))\n",
    "    return((x_1, x_2))\n",
    "\n",
    "def batch_data_arrays(data, labels, batchsize, sampledim = 1):\n",
    "    #creates mini-batches and converts to Torch tensors for training\n",
    "    remainder = data.shape[sampledim] % batchsize\n",
    "    diff = batchsize - remainder\n",
    "    tail = data[0,-diff:]\n",
    "    out = [ (torch.Tensor(data[:,i*batchsize:(i+1)*batchsize].T), torch.Tensor(labels[i*batchsize:(i+1)*batchsize])) for i in range(int(float(data.shape[sampledim])/float(batchsize))) ]\n",
    "    return(out)\n",
    "\n",
    "def poly_utility(x, a=1.386):\n",
    "    y = a*np.power(x, 1/4)\n",
    "    return(np.sum(y))\n",
    "\n",
    "def poly_utility_opt_x(T, pi_cp, a=1.386): #naive opt x\n",
    "    num = T*a\n",
    "    denom = 4*pi_cp\n",
    "    opt_x = np.power(num/denom, 4/3)\n",
    "    if opt_x > x_bar:\n",
    "        ret = x_bar\n",
    "    elif opt_x < 0:\n",
    "        ret = 0\n",
    "    return(opt_x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.plot(np.arange(0,1.01,0.01), 1.386*np.arange(0,1.01,0.01), label='$g_{1}(x)$')\n",
    "plt.plot(np.arange(0,1.01,0.01), 2.0*np.log(1 + np.power(np.arange(0,1.01,0.01), 1/2)), label='$g_{1}(x)$')\n",
    "plt.plot(np.arange(0,1.01,0.01), 1.386*np.power(np.arange(0,1.01,0.01),1/4), label='$g_{2}(x)$')\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel(\"Range of demands $x_{t}$\", fontsize=14)\n",
    "plt.ylabel(\"Utility for a single round $t$\", fontsize=14)\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Neural network class definition and training helper function\n",
    "#This neural network learns an approximate deterministic policy\n",
    "\n",
    "class nnet(nn.Module):\n",
    "    #a linear neural network with a sigmoid activiation function and a single hidden layer\n",
    "    def __init__(self, params):\n",
    "        super(nnet, self).__init__()\n",
    "        self.D_in = params['FEATURE_DIM']\n",
    "        self.H1 = params['HIDDEN_1']\n",
    "        self.D_out = params['OUTPUT_DIM']\n",
    "        self.l1 = nn.Linear(self.D_in, self.H1)\n",
    "        self.l2 = nn.Linear(self.H1, self.D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.l1(x)) #sigmoid activation\n",
    "        x = self.l2(x) #hidden layer\n",
    "        return(x)\n",
    "    \n",
    "    \n",
    "def train(net_obj, loss_func, opt_func, trainX, trainY, valX, valY, batchsize=100, epochs=50, verbose=True):\n",
    "    print(\"Training\")\n",
    "    train_batches = batch_data_arrays(trainX, trainY, batchsize)\n",
    "    num_batches = len(train_batches)\n",
    "    \n",
    "    train_epoch_loss = []\n",
    "    val_epoch_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        #iterate over minibatches\n",
    "        for i, data in enumerate(train_batches):\n",
    "            inputs, labels = data[0], data[1].unsqueeze(1)\n",
    "            \n",
    "            #make Variable\n",
    "            inputs, labels = Variable(inputs), Variable(labels) \n",
    "            #inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            \n",
    "            #zero gradient\n",
    "            opt_func.zero_grad()\n",
    "            \n",
    "            #compute training loss\n",
    "            outputs = net_obj(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt_func.step()\n",
    "            \n",
    "            running_loss += 0.0\n",
    "        \n",
    "        #compute validation loss\n",
    "        train_epoch_loss.append(running_loss)\n",
    "        val_out = net_obj(torch.Tensor(valX.T))\n",
    "        val_loss = loss_func(val_out, torch.Tensor(valY).unsqueeze(1))\n",
    "        val_epoch_loss.append(val_loss)\n",
    "        \n",
    "        #output progress\n",
    "        if verbose==True:\n",
    "            print(\"==epoch \" + str(epoch) + \"==\")\n",
    "            print(\"training loss: \" + str(running_loss))\n",
    "            print(\"validation loss: \" + str(val_loss))\n",
    "        \n",
    "    #neural network object updated inplace, return loss over epochs\n",
    "    return(train_epoch_loss, val_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Monte Carlo path sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_step(x_prev):\n",
    "    g_const = g[(g >= x_prev - ramp_const) & (g <= x_prev + ramp_const)]\n",
    "    x_next = np.random.choice(g_const)\n",
    "    s_next = np.random.choice(s_vals, p=s_hist)\n",
    "    return(x_next, s_next)\n",
    "\n",
    "def generate_sample_paths(T, util_func=log_utility, N=1000, sim_MC=100):\n",
    "    #T is the total depth of simulations\n",
    "    #N is the total number of sample paths to simulate\n",
    "    #sim_MC is the number of Monte Carlo iterations per simulation\n",
    "    \n",
    "    samples_per_t = int(float(N)/float(T-1)) #divide sample paths amongst \n",
    "                                           #equal number of initial times t = [1,..., T]\n",
    "        \n",
    "    X = np.zeros((4,N))\n",
    "    Y = np.zeros((N,))\n",
    "    \n",
    "    ind = np.arange(0, N, 1) #index for data matrix\n",
    "    \n",
    "    #pi_cp rate for T\n",
    "    pi_cp = pi_cp_perc*T*util_func(x_bar)\n",
    "    \n",
    "    for t in range(T-1):\n",
    "        #for each initial t\n",
    "        rounds_left = T-(t+1)\n",
    "        for i in list(ind[t*samples_per_t:(t+1)*samples_per_t]):\n",
    "            #populate features for sample path\n",
    "            #sample initial value\n",
    "            x_1 = np.random.choice(g)\n",
    "            X[0,i] = x_1 #previous state\n",
    "            \n",
    "            s_m = 0.0\n",
    "            for m in range(t):\n",
    "                s_m = np.max([ s_m, np.random.choice(s_vals, p=s_hist) ])\n",
    "            X[1,i] = s_m #maximum noise seen so far, previous values don't matter since less than this\n",
    "            \n",
    "            X[2,i] = float(rounds_left)/float(T) #normalized number of rounds left to go\n",
    "            \n",
    "            X[3,i] = np.random.uniform(0,1) #initialize linear bias weight\n",
    "            \n",
    "            #forward simulate for all possible next choices\n",
    "            sim_rewards = {}\n",
    "            \n",
    "            for x_2_i in range(g.shape[0]):\n",
    "                x_2 = g[x_2_i]\n",
    "                if np.abs(x_2 - x_1) > ramp_const:\n",
    "                    pass\n",
    "                else:\n",
    "                    #go forward for T-t\n",
    "                    sim_rewards[x_2] = []\n",
    "                    for s in range(sim_MC):\n",
    "                        #initialize simulated path\n",
    "                        s_2 = np.random.choice(s_vals, p=s_hist)\n",
    "                        plays = [x_1, x_2]\n",
    "                        noises = [s_m, s_2]\n",
    "                        x_iter = x_2\n",
    "                        for r in range(rounds_left):\n",
    "                            x_iter, s_iter = simulate_step(x_iter)\n",
    "                            plays.append(x_iter)\n",
    "                            noises.append(s_iter)\n",
    "                            \n",
    "                        sim_reward = util_func(np.array(plays)) - pi_cp*plays[np.argmax(noises)]\n",
    "                        sim_rewards[x_2].append(sim_reward)\n",
    "            \n",
    "            best_x_2 = 0\n",
    "            best_expected_reward = 0.0\n",
    "            for x_2 in sim_rewards:\n",
    "                if np.mean(sim_rewards[x_2]) > best_expected_reward:\n",
    "                    best_x_2 = x_2\n",
    "                    best_expected_reward = np.mean(sim_rewards[x_2])\n",
    "                    \n",
    "            Y[i] = best_x_2\n",
    "            \n",
    "    return(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pi_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test is naive_optimal_poly is working\n",
    "x_in= np.arange(0,1.1,0.1)\n",
    "T = 10\n",
    "pi_cp = pi_cp_perc*T*poly_utility(x_bar)\n",
    "sol = poly_utility_opt_x(T, pi_cp)\n",
    "print(sol)\n",
    "allo = []\n",
    "for a in x_in:\n",
    "    print(T*poly_utility(a) - pi_cp*a)\n",
    "    allo.append(T*poly_utility(a) - pi_cp*a)\n",
    "    \n",
    "plt.plot(allo)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#evaluation routine\n",
    "\n",
    "def naive_optimal_utility(T):\n",
    "    pi_cp = pi_cp_perc*T*log_utility(x_bar)\n",
    "    sol1, sol2 = log_utility_opt_x(T, pi_cp)\n",
    "    nopt_reward = T*log_utility(sol2) - pi_cp*sol2\n",
    "    return(nopt_reward, (sol1, sol2))\n",
    "\n",
    "def naive_optimal_poly(T):\n",
    "    pi_cp = pi_cp_perc*T*poly_utility(x_bar)\n",
    "    sol = poly_utility_opt_x(T, pi_cp)\n",
    "    nopt_reward = T*poly_utility(sol) - pi_cp*sol\n",
    "    return(nopt_reward, (sol))\n",
    "    \n",
    "def evaluate_net(net_obj, util_func, T, pi_cp, naive_opt_reward, x_1=0.7, trials=100):\n",
    "    winloss = []\n",
    "    rewards = []\n",
    "    plays_all = []\n",
    "    for i in range(trials):\n",
    "        noises = []\n",
    "        plays = []\n",
    "        x_curr = x_1\n",
    "        for j in range(T): #includes initial round noise sample\n",
    "            t = j+1\n",
    "            s_t = np.random.choice(s_vals, p=s_hist)\n",
    "            noises.append(s_t)\n",
    "            \n",
    "            s_m = np.max(noises) #max so far\n",
    "            \n",
    "            inputvec = torch.Tensor(np.array([x_curr, s_m, float(T-t)/float(T), 1.0])).unsqueeze(1).transpose(1,0)\n",
    "            \n",
    "            play_output = net_obj(inputvec)\n",
    "            play = float(play_output.data[0,0])\n",
    "            \n",
    "            #snap to nearest value in range\n",
    "            if play > x_bar: play = x_bar\n",
    "            if play < 0.0: play = 0.0\n",
    "                \n",
    "            #snap to nearest feasible value if constraints violated\n",
    "            if np.abs(x_curr - play) > ramp_const:\n",
    "                if play > x_curr: play = x_curr + ramp_const\n",
    "                if play < x_curr: play = x_curr - ramp_const\n",
    "                    \n",
    "            #attach previous play\n",
    "            plays.append(x_curr)\n",
    "            x_curr = play\n",
    "        \n",
    "        reward = util_func(np.array(plays)) - pi_cp*plays[np.argmax(noises)]\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if reward > naive_opt_reward: winloss.append(1)\n",
    "        else: winloss.append(0)\n",
    "            \n",
    "        plays_all.append(plays)\n",
    "            \n",
    "    return(np.mean(winloss), np.mean(rewards), plays_all)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train networks\n",
    "for T in range(2,16):\n",
    "#for T in range(11,50):\n",
    "#for T in [16, 21, 25]:\n",
    "    print(\"Simulating: T=\" + str(T) + \"...\")\n",
    "    if T in [8, 11, 12, 16, 21, 25]:\n",
    "        X, Y = generate_sample_paths(T, N=T*500, sim_MC=200) #T=8, 11, 12, 21, 25 does significantly better with less samples, haven't optimized hyperparameters\n",
    "    else:\n",
    "        X, Y = generate_sample_paths(T, N=T*1000, sim_MC=100) #increase number of samples per increase in T\n",
    "    \n",
    "    #train/val split\n",
    "    train_X = X[:,0:int(0.8*X.shape[1])]\n",
    "    train_Y = Y[0:int(0.8*Y.shape[0])]\n",
    "    val_X = X[:,int(0.8*X.shape[1]):]\n",
    "    val_Y = Y[int(0.8*Y.shape[0]):]\n",
    "    \n",
    "    #instantiate network\n",
    "    net = nnet(net_params)#.cuda()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "    \n",
    "    #train\n",
    "    train_loss, val_loss = train(net, criterion, optimizer, train_X, train_Y, val_X, val_Y, batchsize=100, epochs=500, verbose=False)\n",
    "    \n",
    "    #save the network\n",
    "    path = \"/home/chase/projects/peakload/notebooks/cdc_19/nets/\"\n",
    "    torch.save(net.state_dict(), path + \"T\" + str(T) + \"_logutil.torch\")\n",
    "    \n",
    "    del(net) #ensure reference is empty because python weird like that\n",
    "    print(\"==============\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train networks\n",
    "for T in range(2,11):\n",
    "#for T in range(10,11):\n",
    "    print(\"Simulating: T=\" + str(T) + \"...\")\n",
    "    X, Y = generate_sample_paths(T, poly_utility, N=T*500, sim_MC=100)\n",
    "    \n",
    "    #train/val split\n",
    "    train_X = X[:,0:int(0.8*X.shape[1])]\n",
    "    train_Y = Y[0:int(0.8*Y.shape[0])]\n",
    "    val_X = X[:,int(0.8*X.shape[1]):]\n",
    "    val_Y = Y[int(0.8*Y.shape[0]):]\n",
    "    \n",
    "    #instantiate network\n",
    "    net = nnet(net_params)#.cuda()\n",
    "    try:\n",
    "        net.load_state_dict(torch.load(base_path + \"cdc_19/nets/T\" + str(T) + \"_polyutil_ex.torch\")) #train further\n",
    "    except:\n",
    "        net.load_state_dict(torch.load(base_path + \"cdc_19/nets/T\" + str(T) + \"_polyutil.torch\")) #train further\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "    \n",
    "    #train\n",
    "    train_loss, val_loss = train(net, criterion, optimizer, train_X, train_Y, val_X, val_Y, batchsize=200, epochs=500, verbose=False)\n",
    "    \n",
    "    #save the network\n",
    "    path = \"/home/chase/projects/peakload/notebooks/cdc_19/nets/\"\n",
    "    torch.save(net.state_dict(), path + \"T\" + str(T) + \"_polyutil_ex.torch\")\n",
    "    \n",
    "    del(net) #ensure reference is empty because python weird like that\n",
    "    print(\"==============\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############evaluating logarithmic utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot for increasing T the gap between naive optimal and forward-simulated trained policy expected rewards\n",
    "\n",
    "win_loss_percs = []\n",
    "net_rewards = []\n",
    "naive_opt_rewards = []\n",
    "pk_rewards = []\n",
    "\n",
    "for T in range(2,16):\n",
    "    #load the network\n",
    "    net = nnet(net_params)\n",
    "    net.load_state_dict(torch.load(base_path + \"cdc_19/nets/T\" + str(T) + \".torch\"))\n",
    "    net.eval()\n",
    "    \n",
    "    #evaluate\n",
    "    print(\"Evaluating \" + str(T) + \"...\")\n",
    "    pi_cp = pi_cp_perc*T*log_utility(x_bar)\n",
    "    nopt_reward, nopt = naive_optimal_utility(T)\n",
    "    naive_opt_rewards.append(nopt_reward)\n",
    "    print(nopt)\n",
    "    \n",
    "    #perfect reward\n",
    "    pk_noise_play = np.min([x_bar, np.max(log_utility_opt_x_single_t(pi_cp))])\n",
    "    if np.isnan(pk_noise_play):\n",
    "        pk_noise_play = 0\n",
    "    if pk_noise_play < 0:\n",
    "        pk_noise_play = 0\n",
    "    if T == 2:\n",
    "        pk_noise_play = 0\n",
    "    pk_rewards.append((T-1)*log_utility(x_bar) + log_utility(pk_noise_play) - pi_cp*pk_noise_play)\n",
    "    \n",
    "    net.eval() #set network to evaluation mode (not neccessary for this particular architecture but no harm in including)\n",
    "    win_loss, policy_reward, nnplays = evaluate_net(net, log_utility, T, pi_cp, nopt_reward, x_1=0.5)  #double check where to start from\n",
    "    net_rewards.append(policy_reward)\n",
    "    win_loss_percs.append(win_loss)\n",
    "    \n",
    "del(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T_range = np.arange(2,11,1)\n",
    "plt.plot(T_range, naive_opt_rewards[0:9], label=\"Naive optimal\")\n",
    "plt.plot(T_range, net_rewards[0:9], label=\"NN policy\")\n",
    "#plt.plot(T_range, np.log(pk_rewards), label=\"Perfect knowledge\")\n",
    "plt.ylabel(\"Expected reward\", fontsize=14)\n",
    "plt.xlabel(\"Number of rounds\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.xlim(1,11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(T_range, 100*np.array(win_loss_percs))\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylim(0,105)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot of policy for T = 2\n",
    "x_curr_i = 3\n",
    "x_curr = g[x_curr_i]\n",
    "g_const = g[(g <= g[x_curr_i] + ramp_const) & (g >= g[x_curr_i] - ramp_const)]\n",
    "T = 4\n",
    "\n",
    "policies = []\n",
    "\n",
    "for t in [1,2,3]:\n",
    "    net = nnet(net_params)\n",
    "    try:\n",
    "        net.load_state_dict(torch.load(base_path + \"cdc_19/nets/T\" + str(T) + \"_ex.torch\"))\n",
    "    except:\n",
    "        net.load_state_dict(torch.load(base_path + \"cdc_19/nets/T\" + str(T) + \".torch\"))\n",
    "    net.eval()\n",
    "\n",
    "    policy = []\n",
    "    for s in s_vals:\n",
    "        inputvec = torch.Tensor(np.array([x_curr, s, float(T-t)/float(T), 1.0])).unsqueeze(1).transpose(1,0)\n",
    "\n",
    "        play_output = net(inputvec)\n",
    "        play = float(play_output.data[0,0])\n",
    "\n",
    "        #snap to nearest value in range\n",
    "        if play > x_bar: play = x_bar\n",
    "        if play < 0.0: play = 0.0\n",
    "\n",
    "        #snap to nearest feasible value if constraints violated\n",
    "        if np.abs(x_curr - play) > ramp_const:\n",
    "            if play > x_curr: play = x_curr + ramp_const\n",
    "            if play < x_curr: play = x_curr - ramp_const\n",
    "\n",
    "        policy.append(play)\n",
    "    policies.append(policy)\n",
    "    \n",
    "del(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in range(3):\n",
    "    plt.plot(s_vals, np.asarray(policies[t]), label=\"t = \" + str(t+1))\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylabel(\"Value of $x_{t+1}$ to play\", fontsize=14)\n",
    "plt.xlabel(\"Maximum observed noise thus far: $s_m$\", fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"Policies for varying t, $x_1 = 0.3$, T = 4\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############polynomial utility function############3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot for increasing T the gap between naive optimal and forward-simulated trained policy expected rewards\n",
    "\n",
    "win_loss_percs = []\n",
    "net_rewards = []\n",
    "naive_opt_rewards = []\n",
    "pk_rewards = []\n",
    "\n",
    "for T in range(2,16):\n",
    "    #load the network\n",
    "    net = nnet(net_params)\n",
    "    try:\n",
    "        net.load_state_dict(torch.load(base_path + \"cdc_19/nets/T\" + str(T) + \"_polyutil_ex.torch\"))\n",
    "    except:\n",
    "        net.load_state_dict(torch.load(base_path + \"cdc_19/nets/T\" + str(T) + \"_polyutil.torch\"))\n",
    "    net.eval()\n",
    "    \n",
    "    #evaluate\n",
    "    print(\"Evaluating \" + str(T) + \"...\")\n",
    "    pi_cp = pi_cp_perc*T*poly_utility(x_bar)\n",
    "    nopt_reward, nopt = naive_optimal_poly(T)\n",
    "    naive_opt_rewards.append(nopt_reward)\n",
    "    \n",
    "    net.eval() #set network to evaluation mode (not neccessary for this particular architecture but no harm in including)\n",
    "    win_loss, policy_reward, nnplays = evaluate_net(net, poly_utility, T, pi_cp, nopt_reward, x_1=0.4)  #double check where to start from\n",
    "    net_rewards.append(policy_reward)\n",
    "    win_loss_percs.append(win_loss)\n",
    "    \n",
    "del(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T_range = np.arange(2,11,1)\n",
    "plt.plot(T_range, naive_opt_rewards[0:9], label=\"Naive optimal\")\n",
    "plt.plot(T_range, net_rewards[0:9], label=\"NN policy\")\n",
    "#plt.plot(T_range, np.log(pk_rewards), label=\"Perfect knowledge\")\n",
    "plt.ylabel(\"Expected reward\", fontsize=14)\n",
    "plt.xlabel(\"Number of rounds\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.xlim(1,11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot of policy for T = 2\n",
    "x_curr_i = 3\n",
    "x_curr = g[x_curr_i]\n",
    "g_const = g[(g <= g[x_curr_i] + ramp_const) & (g >= g[x_curr_i] - ramp_const)]\n",
    "T = 4\n",
    "\n",
    "policies = []\n",
    "\n",
    "for t in [1,2,3]:\n",
    "    net = nnet(net_params)\n",
    "    net.load_state_dict(torch.load(base_path + \"cdc_19/nets/T\" + str(T) + \"_polyutil.torch\"))\n",
    "    net.eval()\n",
    "\n",
    "    policy = []\n",
    "    for s in s_vals:\n",
    "        inputvec = torch.Tensor(np.array([x_curr, s, float(T-t)/float(T), 1.0])).unsqueeze(1).transpose(1,0)\n",
    "\n",
    "        play_output = net(inputvec)\n",
    "        play = float(play_output.data[0,0])\n",
    "\n",
    "        #snap to nearest value in range\n",
    "        if play > x_bar: play = x_bar\n",
    "        if play < 0.0: play = 0.0\n",
    "\n",
    "        #snap to nearest feasible value if constraints violated\n",
    "        if np.abs(x_curr - play) > ramp_const:\n",
    "            if play > x_curr: play = x_curr + ramp_const\n",
    "            if play < x_curr: play = x_curr - ramp_const\n",
    "\n",
    "        policy.append(play)\n",
    "    policies.append(policy)\n",
    "    \n",
    "del(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in range(3):\n",
    "    plt.plot(s_vals, np.asarray(policies[t]), label=\"t = \" + str(t+1))\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylabel(\"Value of $x_{t+1}$ to play\", fontsize=14)\n",
    "plt.xlabel(\"Maximum observed noise thus far: $s_m$\", fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"Policies for varying t, $x_1 = 0.3$, T = 4\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def grid_search_optimal(x_1_i, noise_sequence, grid):\n",
    "    #returns the optimal plays iterating on a sequence of observed noise values\n",
    "    #grid_search = np.load(grid_search_file_path)\n",
    "    T = len(noise_sequence)\n",
    "    x_curr = x_1_i\n",
    "    outcomes = grid\n",
    "    plays = [g[x_1_i]]\n",
    "    for t in range(T-1):\n",
    "        #T = 2 has shape (11, 40, 11), last is t=0\n",
    "        #T= 3 has shape (11, 40, 11, 40, 11), last is t=1\n",
    "        #T = 4 has shape (11, 40, 11, 40, 11, 40, 11), last is t=2\n",
    "        s_curr = noise_sequence[t]\n",
    "        best_reward = 0\n",
    "        best_x_2_i = 0\n",
    "        outcomes = outcomes[x_curr, s_curr]\n",
    "        if t == T-2:\n",
    "            best_x_2_i = np.argmax(outcomes)\n",
    "            expected_reward = np.max(outcomes)\n",
    "        else:\n",
    "            g_const_i = np.arange(0,11,1)[(g <= g[x_curr] + ramp_const) & (g >= g[x_curr] - ramp_const)]\n",
    "            for x_i in g_const_i:\n",
    "                forward = outcomes[g_const_i]\n",
    "                expected_reward = np.mean(forward[forward > 0.0])\n",
    "                if expected_reward > best_reward:\n",
    "                    best_reward = expected_reward\n",
    "                    best_x_2_i = x_i\n",
    "        plays.append(g[best_x_2_i])\n",
    "        x_curr = best_x_2_i\n",
    "    return(plays, expected_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bar plot of T = 2, 3, 4 difference between forward-simulated trained policy and grid optimal expected rewards\n",
    "base_path = \"/home/chase/projects/peakload/notebooks/\"\n",
    "\n",
    "nn_rewards = []\n",
    "grid_rewards = []\n",
    "grid_exp_rewards = {}\n",
    "pk_rewards = []\n",
    "naive_opt_rewards = []\n",
    "win_loss_percs = []\n",
    "\n",
    "for T in [2,3,4]:\n",
    "    print(T)\n",
    "    grid_exp_rewards[T] = []\n",
    "    mc_nn_r = []\n",
    "    mc_gs_r = []\n",
    "    \n",
    "    pi_cp = pi_cp_perc*T*log_utility(x_bar)\n",
    "    \n",
    "    #load grid\n",
    "    grid_opt = np.load(base_path + \"t\" + str(T) + \"_grid_search.txt\")\n",
    "    print(grid_opt.shape)\n",
    "\n",
    "    for mc in range(100):\n",
    "\n",
    "        #realize sequences of noises\n",
    "        s_i = [np.random.choice(np.arange(0,40,1), p=s_hist) for i in range(T)] #, p=s_hist\n",
    "        s = [ s_vals[j] for j in s_i ]\n",
    "\n",
    "        #grid_opt_path\n",
    "        grid_plays, exp_reward = grid_search_optimal(7, s_i, grid_opt)\n",
    "\n",
    "        #noises = []\n",
    "        #plays = []\n",
    "        #x_curr = 0.5\n",
    "        #for j in range(T):\n",
    "        #    t = j+1\n",
    "        # \n",
    "        #    s_t = s[j]\n",
    "        #    noises.append(s_t)\n",
    "        #    s_m = np.max(noises)\n",
    "        #\n",
    "        #    inputvec = torch.Tensor(np.array([x_curr, s_m, float(T-t)/float(T), 1.0])).unsqueeze(1).transpose(1,0)\n",
    "        #\n",
    "        #    play_output = net(inputvec)\n",
    "        #    play = float(play_output.data[0,0])\n",
    "        #\n",
    "        #    #snap to nearest value in range\n",
    "        #    if play > x_bar: play = x_bar\n",
    "        #    if play < 0.0: play = 0.0\n",
    "        # \n",
    "        #    #snap to nearest feasible value if constraints violated\n",
    "        #    if np.abs(x_curr - play) > ramp_const:\n",
    "        #        if play > x_curr: play = x_curr + ramp_const\n",
    "        #        if play < x_curr: play = x_curr - ramp_const\n",
    "\n",
    "            #attach previous play\n",
    "        #    plays.append(x_curr)\n",
    "        #    x_curr = play\n",
    "\n",
    "        #nn_reward = log_utility(np.array(plays)) - pi_cp*plays[np.argmax(s)]\n",
    "        grid_reward = log_utility(np.array(np.array(grid_plays) + 0.08)) - pi_cp*grid_plays[np.argmax(s)] #best case subject to grid search resolution\n",
    "        grid_exp_rewards[T].append(exp_reward)\n",
    "\n",
    "        #mc_nn_r.append(nn_reward)\n",
    "        mc_gs_r.append(grid_reward)\n",
    "    \n",
    "    grid_rewards.append(np.mean(mc_gs_r))\n",
    "    mc_grd_reward = np.mean(mc_gs_r)\n",
    "    \n",
    "    #load nn\n",
    "\n",
    "    net = nnet(net_params)\n",
    "    net.load_state_dict(torch.load(base_path + \"cdc_19/nets/T\" + str(T) + \".torch\"))\n",
    "    net.eval()\n",
    "\n",
    "    win_loss, policy_reward, nn_plays = evaluate_net(net, T, pi_cp, mc_grd_reward, x_1=0.5)  #double check where to start from\n",
    "    nn_rewards.append(policy_reward)\n",
    "    win_loss_percs.append(win_loss)   \n",
    "    del(net)\n",
    "        \n",
    "    pk_noise_play = np.min([x_bar, np.max(log_utility_opt_x_single_t(pi_cp))])\n",
    "    if np.isnan(pk_noise_play):\n",
    "        pk_noise_play = 0\n",
    "    if pk_noise_play < 0:\n",
    "        pk_noise_play = 0\n",
    "    if T == 2:\n",
    "        pk_noise_play = 0\n",
    "    pk_rewards.append((T-1)*log_utility(x_bar) + log_utility(pk_noise_play) - pi_cp*pk_noise_play)\n",
    "    \n",
    "    nopt_reward = naive_optimal_utility(T)[0]\n",
    "    naive_opt_rewards.append(nopt_reward)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ger = [np.mean(grid_exp_rewards[2]) , np.mean(grid_exp_rewards[3]) , np.mean(grid_exp_rewards[4]) ]\n",
    "print(ger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pk_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "naive_opt_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(pk_rewards, label=\"perfect\")\n",
    "plt.plot(ger, label=\"grid\")\n",
    "plt.plot(grid_rewards, label=\"grid_2\")\n",
    "plt.plot(nn_rewards, label=\"nn\")\n",
    "plt.plot(naive_opt_rewards, label=\"naive\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# We need to draw the canvas, otherwise the labels won't be positioned and \n",
    "# won't have values yet.\n",
    "fig.canvas.draw()\n",
    "\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "labels[1] = 'Testing'\n",
    "\n",
    "ax.set_xticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = np.arange(0,3,1)\n",
    "seaborn.set()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.canvas.draw()\n",
    "\n",
    "#plt.bar(pos-0.2, pk_rewards, label=\"perfect\", width=0.2)\n",
    "plt.bar(pos, grid_rewards, label=\"grid\", width=0.2)\n",
    "plt.bar(pos+0.2, nn_rewards, label=\"nn\", width=0.2)\n",
    "plt.bar(pos+0.4, naive_opt_rewards, label=\"naive\", width=0.2)\n",
    "#plt.bar(np.arnage(4,29,5), np.zeros((3,)), width=1)\n",
    "plt.legend(loc=2, fontsize=12)\n",
    "plt.ylim(0,4)\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "labels = [\"\", \"T = 1\", \"\", \"T = 2\", \"\", \"T = 3\", \"\"]\n",
    "ax.set_xticklabels(labels)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.ylabel(\"Reward\", fontsize=14)\n",
    "plt.xlabel(\"Number of rounds\", fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for mc in range(100):\n",
    "\n",
    "        #realize sequences of noises\n",
    "        s_i = [np.random.choice(np.arange(0,40,1), p=s_hist) for i in range(T)]\n",
    "        s = [ s_vals[j] for j in s_i ]\n",
    "\n",
    "        #grid_opt_path\n",
    "        grid_plays = grid_search_optimal(3, s_i, grid_opt)\n",
    "\n",
    "        noises = []\n",
    "        plays = []\n",
    "        x_curr = 0.5\n",
    "        for j in range(T):\n",
    "            t = j+1\n",
    "     \n",
    "            s_t = s[j]\n",
    "            noises.append(s_t)\n",
    "            s_m = np.max(noises)\n",
    "\n",
    "            inputvec = torch.Tensor(np.array([x_curr, s_m, float(T-t)/float(T), 1.0])).unsqueeze(1).transpose(1,0)\n",
    "\n",
    "            play_output = net(inputvec)\n",
    "            play = float(play_output.data[0,0])\n",
    "\n",
    "            #snap to nearest value in range\n",
    "            if play > x_bar: play = x_bar\n",
    "            if play < 0.0: play = 0.0\n",
    "\n",
    "            #snap to nearest feasible value if constraints violated\n",
    "            if np.abs(x_curr - play) > ramp_const:\n",
    "                if play > x_curr: play = x_curr + ramp_const\n",
    "                if play < x_curr: play = x_curr - ramp_const\n",
    "\n",
    "            #attach previous play\n",
    "            plays.append(x_curr)\n",
    "            x_curr = play"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
