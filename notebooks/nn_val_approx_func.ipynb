{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import *\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob_t_is_max(max_so_far, t, T):\n",
    "    coeff = 1.0/(T - t)\n",
    "    prod = (1.0 - np.power(norm.cdf(max_so_far), T-t))\n",
    "    return(coeff*prod)\n",
    "\n",
    "def prob_any_next_t_is_max(max_so_far, t, T):\n",
    "    prod = (1.0 - np.power(norm.cdf(max_so_far), T-t))\n",
    "    return(prod)\n",
    "\n",
    "def log_utility(x):\n",
    "    y = 2.0*np.log(1 + np.power(x,1/2))\n",
    "    return(np.sum(y))\n",
    "\n",
    "def log_utility_opt_x(T, pi_cp):\n",
    "    frac = (2*T)/pi_cp\n",
    "    x_1 = frac + np.sqrt((frac - 1)*(frac + 1))\n",
    "    x_2 = frac - np.sqrt((frac - 1)*(frac + 1))\n",
    "    return((x_1, x_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_sample_list(datalist, batchsize):\n",
    "    #list of training, target pair tuples\n",
    "    remainder = len(datalist) % batchsize\n",
    "    diff = batchsize - remainder\n",
    "    tail = datalist[-diff:] + datalist[0:remainder]\n",
    "    out = [ datalist[i*batchsize:(i+1)*batchsize] for i in range(int(float(len(datalist))/float(batchsize)))]\n",
    "    out = out + [tail]\n",
    "    return(out)\n",
    "\n",
    "def batch_data_arrays(data, labels, batchsize, sampledim = 1):\n",
    "    remainder = data.shape[sampledim] % batchsize\n",
    "    diff = batchsize - remainder\n",
    "    tail = data[0,-diff:]\n",
    "    out = [ (torch.Tensor(data[:,i*batchsize:(i+1)*batchsize].T), torch.Tensor(labels[i*batchsize:(i+1)*batchsize])) for i in range(int(float(data.shape[sampledim])/float(batchsize))) ]\n",
    "    return(out)\n",
    "    \n",
    "\n",
    "def torch_reshape_data(databatch):\n",
    "    #flattens array inputs for a single list of training, target pairs\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for sample in databatch:\n",
    "        inputs.append(sample[0].flatten())\n",
    "        labels.append(sample[1].flatten())\n",
    "    return(torch.Tensor(np.asarray(inputs)), torch.Tensor(np.asarray(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#just try a multilayer NN\n",
    "class nnet(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(nnet, self).__init__()\n",
    "        self.D_in = params['FEATURE_DIM']\n",
    "        self.H1 = params['HIDDEN_1']\n",
    "        self.D_out = params['OUTPUT_DIM']\n",
    "        self.l1 = nn.Linear(self.D_in, self.H1)\n",
    "        self.l2 = nn.Linear(self.H1, self.D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return(x)\n",
    "    \n",
    "    \n",
    "def train(net_obj, loss_func, opt_func, trainX, trainY, valX, valY, batchsize=100, epochs=50, verbose=True):\n",
    "    print(\"Training\")\n",
    "    train_batches = batch_data_arrays(trainX, trainY, batchsize)\n",
    "    num_batches = len(train_batches)\n",
    "    \n",
    "    train_epoch_loss = []\n",
    "    val_epoch_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_batches):\n",
    "            inputs, labels = data[0], data[1].unsqueeze(1)\n",
    "            \n",
    "            #make Variable\n",
    "            inputs, labels = Variable(inputs), Variable(labels) \n",
    "            #inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            \n",
    "            opt_func.zero_grad()\n",
    "            \n",
    "            outputs = net_obj(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt_func.step()\n",
    "            \n",
    "            running_loss += 0.0\n",
    "            \n",
    "        \n",
    "        \n",
    "        train_epoch_loss.append(running_loss)\n",
    "        val_out = net_obj(torch.Tensor(valX.T))\n",
    "        val_loss = loss_func(val_out, torch.Tensor(valY).unsqueeze(1))\n",
    "        val_epoch_loss.append(val_loss)\n",
    "        \n",
    "        if verbose==True:\n",
    "            print(\"==epoch \" + str(epoch) + \"==\")\n",
    "            print(\"training loss: \" + str(running_loss))\n",
    "            print(\"validation loss: \" + str(val_loss))\n",
    "        \n",
    "    return(train_epoch_loss, val_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = 2\n",
    "grid_file_name = \"/home/chase/projects/peakload/notebooks/t\" + str(T) + \"_grid_search.txt\"\n",
    "\n",
    "with open(grid_file_name, 'rb') as d:\n",
    "    grid_MC = np.load(grid_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4833222224851716"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(grid_MC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample some paths\n",
    "\n",
    "g = np.arange(0,1.1,0.1)\n",
    "s_vals = np.arange(-2.0,2.0,0.1)\n",
    "ramp_const = 0.3\n",
    "\n",
    "N = 1000\n",
    "\n",
    "path_x = []\n",
    "path_s = []\n",
    "best_responses = []\n",
    "\n",
    "X = np.zeros((4, N))\n",
    "\n",
    "Y = np.zeros((N,))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "    x_1_i = np.random.randint(0,11) #we do better if we ignore states we'd never start in e.g. np.random.randint(4,11)\n",
    "    X[0,i] = g[x_1_i]  #previous state\n",
    "\n",
    "    s_1_i = np.random.randint(s_vals.shape[0])\n",
    "    X[1,i] = s_vals[s_1_i] #maximum noise seen so far\n",
    "\n",
    "    X[2,i] = 1 #rounds left to go\n",
    "    \n",
    "    X[3,i] = np.random.uniform(0,1) #linear bias term\n",
    "    \n",
    "    best_response_i = np.argmax(grid_MC[x_1_i, s_1_i, :])#pick best x_2 that maximizes expected reward given x_1, s_1\n",
    "    Y[i] = g[best_response_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = X[:,0:int(0.8*X.shape[1])]\n",
    "train_Y = Y[0:int(0.8*Y.shape[0])]\n",
    "val_X = X[:,int(0.8*X.shape[1]):]\n",
    "val_Y = Y[int(0.8*Y.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 800), (800,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'FEATURE_DIM': 4, 'OUTPUT_DIM': 1, 'HIDDEN_1': 4}\n",
    "\n",
    "net = nnet(params)#.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "==epoch 0==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.1657, grad_fn=<MseLossBackward>)\n",
      "==epoch 1==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.2166, grad_fn=<MseLossBackward>)\n",
      "==epoch 2==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0504, grad_fn=<MseLossBackward>)\n",
      "==epoch 3==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0651, grad_fn=<MseLossBackward>)\n",
      "==epoch 4==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0578, grad_fn=<MseLossBackward>)\n",
      "==epoch 5==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0407, grad_fn=<MseLossBackward>)\n",
      "==epoch 6==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0371, grad_fn=<MseLossBackward>)\n",
      "==epoch 7==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0348, grad_fn=<MseLossBackward>)\n",
      "==epoch 8==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0288, grad_fn=<MseLossBackward>)\n",
      "==epoch 9==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0238, grad_fn=<MseLossBackward>)\n",
      "==epoch 10==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0199, grad_fn=<MseLossBackward>)\n",
      "==epoch 11==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0163, grad_fn=<MseLossBackward>)\n",
      "==epoch 12==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0135, grad_fn=<MseLossBackward>)\n",
      "==epoch 13==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0120, grad_fn=<MseLossBackward>)\n",
      "==epoch 14==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0114, grad_fn=<MseLossBackward>)\n",
      "==epoch 15==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0112, grad_fn=<MseLossBackward>)\n",
      "==epoch 16==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0110, grad_fn=<MseLossBackward>)\n",
      "==epoch 17==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0109, grad_fn=<MseLossBackward>)\n",
      "==epoch 18==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0108, grad_fn=<MseLossBackward>)\n",
      "==epoch 19==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0107, grad_fn=<MseLossBackward>)\n",
      "==epoch 20==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0106, grad_fn=<MseLossBackward>)\n",
      "==epoch 21==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0105, grad_fn=<MseLossBackward>)\n",
      "==epoch 22==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "==epoch 23==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0103, grad_fn=<MseLossBackward>)\n",
      "==epoch 24==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0103, grad_fn=<MseLossBackward>)\n",
      "==epoch 25==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0102, grad_fn=<MseLossBackward>)\n",
      "==epoch 26==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0102, grad_fn=<MseLossBackward>)\n",
      "==epoch 27==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0102, grad_fn=<MseLossBackward>)\n",
      "==epoch 28==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0101, grad_fn=<MseLossBackward>)\n",
      "==epoch 29==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0101, grad_fn=<MseLossBackward>)\n",
      "==epoch 30==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0101, grad_fn=<MseLossBackward>)\n",
      "==epoch 31==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "==epoch 32==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "==epoch 33==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "==epoch 34==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "==epoch 35==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "==epoch 36==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "==epoch 37==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "==epoch 38==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "==epoch 39==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "==epoch 40==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "==epoch 41==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "==epoch 42==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "==epoch 43==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "==epoch 44==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "==epoch 45==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "==epoch 46==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "==epoch 47==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "==epoch 48==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "==epoch 49==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0098, grad_fn=<MseLossBackward>)\n",
      "==epoch 50==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "==epoch 51==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "==epoch 52==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "==epoch 53==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "==epoch 54==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "==epoch 55==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "==epoch 56==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "==epoch 57==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "==epoch 58==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "==epoch 59==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "==epoch 60==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "==epoch 61==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "==epoch 62==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "==epoch 63==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "==epoch 64==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "==epoch 65==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "==epoch 66==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "==epoch 67==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "==epoch 68==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "==epoch 69==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "==epoch 70==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "==epoch 71==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "==epoch 72==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "==epoch 73==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "==epoch 74==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "==epoch 75==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "==epoch 76==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "==epoch 77==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "==epoch 78==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "==epoch 79==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "==epoch 80==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "==epoch 81==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "==epoch 82==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "==epoch 83==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "==epoch 84==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "==epoch 85==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "==epoch 86==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "==epoch 87==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "==epoch 88==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "==epoch 89==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "==epoch 90==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "==epoch 91==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "==epoch 92==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "==epoch 93==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0094, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==epoch 94==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "==epoch 95==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "==epoch 96==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "==epoch 97==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0094, grad_fn=<MseLossBackward>)\n",
      "==epoch 98==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 99==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 100==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 101==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 102==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 103==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 104==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 105==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 106==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 107==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 108==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 109==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 110==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 111==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "==epoch 112==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 113==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 114==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 115==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 116==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 117==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 118==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 119==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 120==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 121==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 122==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 123==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 124==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 125==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "==epoch 126==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 127==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 128==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 129==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 130==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 131==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 132==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 133==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 134==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 135==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 136==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 137==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 138==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 139==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 140==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 141==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0091, grad_fn=<MseLossBackward>)\n",
      "==epoch 142==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 143==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 144==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 145==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 146==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 147==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 148==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 149==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 150==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 151==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 152==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 153==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 154==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 155==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 156==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 157==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "==epoch 158==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 159==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 160==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 161==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 162==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 163==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 164==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 165==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 166==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 167==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 168==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 169==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 170==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 171==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 172==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 173==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 174==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "==epoch 175==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 176==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 177==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 178==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 179==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 180==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 181==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 182==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 183==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 184==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 185==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 186==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 187==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 188==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 189==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 190==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 191==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 192==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "==epoch 193==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 194==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 195==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 196==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 197==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 198==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 199==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 200==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 201==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 202==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 203==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 204==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 205==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 206==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 207==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 208==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 209==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "==epoch 210==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 211==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 212==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 213==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==epoch 214==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 215==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 216==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 217==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 218==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 219==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 220==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 221==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 222==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 223==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 224==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 225==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 226==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "==epoch 227==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 228==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 229==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 230==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 231==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 232==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 233==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 234==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 235==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 236==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 237==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 238==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 239==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 240==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 241==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0085, grad_fn=<MseLossBackward>)\n",
      "==epoch 242==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "==epoch 243==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "==epoch 244==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "==epoch 245==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "==epoch 246==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "==epoch 247==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "==epoch 248==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "==epoch 249==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "==epoch 250==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "==epoch 251==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "==epoch 252==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "==epoch 253==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "==epoch 254==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "==epoch 255==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "==epoch 256==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "==epoch 257==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "==epoch 258==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "==epoch 259==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "==epoch 260==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "==epoch 261==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "==epoch 262==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "==epoch 263==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "==epoch 264==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "==epoch 265==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "==epoch 266==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "==epoch 267==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "==epoch 268==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "==epoch 269==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "==epoch 270==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "==epoch 271==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "==epoch 272==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "==epoch 273==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "==epoch 274==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "==epoch 275==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "==epoch 276==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "==epoch 277==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "==epoch 278==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "==epoch 279==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "==epoch 280==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "==epoch 281==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "==epoch 282==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "==epoch 283==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "==epoch 284==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "==epoch 285==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "==epoch 286==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "==epoch 287==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "==epoch 288==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0081, grad_fn=<MseLossBackward>)\n",
      "==epoch 289==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "==epoch 290==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "==epoch 291==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "==epoch 292==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "==epoch 293==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "==epoch 294==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "==epoch 295==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "==epoch 296==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "==epoch 297==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "==epoch 298==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "==epoch 299==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "==epoch 300==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "==epoch 301==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "==epoch 302==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "==epoch 303==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "==epoch 304==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "==epoch 305==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "==epoch 306==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "==epoch 307==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "==epoch 308==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "==epoch 309==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0079, grad_fn=<MseLossBackward>)\n",
      "==epoch 310==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "==epoch 311==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "==epoch 312==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "==epoch 313==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "==epoch 314==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "==epoch 315==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "==epoch 316==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "==epoch 317==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "==epoch 318==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "==epoch 319==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0078, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==epoch 320==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "==epoch 321==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "==epoch 322==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "==epoch 323==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "==epoch 324==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "==epoch 325==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "==epoch 326==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "==epoch 327==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "==epoch 328==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "==epoch 329==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "==epoch 330==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "==epoch 331==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "==epoch 332==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "==epoch 333==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "==epoch 334==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "==epoch 335==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "==epoch 336==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "==epoch 337==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "==epoch 338==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "==epoch 339==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "==epoch 340==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "==epoch 341==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "==epoch 342==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "==epoch 343==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0076, grad_fn=<MseLossBackward>)\n",
      "==epoch 344==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "==epoch 345==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "==epoch 346==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "==epoch 347==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "==epoch 348==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "==epoch 349==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "==epoch 350==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "==epoch 351==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "==epoch 352==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "==epoch 353==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "==epoch 354==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "==epoch 355==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0075, grad_fn=<MseLossBackward>)\n",
      "==epoch 356==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "==epoch 357==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "==epoch 358==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "==epoch 359==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "==epoch 360==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "==epoch 361==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "==epoch 362==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "==epoch 363==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "==epoch 364==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "==epoch 365==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "==epoch 366==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "==epoch 367==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "==epoch 368==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "==epoch 369==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 370==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 371==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 372==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 373==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 374==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 375==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 376==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 377==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 378==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 379==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 380==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 381==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 382==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 383==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "==epoch 384==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 385==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 386==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 387==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 388==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 389==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 390==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 391==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 392==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 393==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 394==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 395==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 396==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 397==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 398==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "==epoch 399==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 400==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 401==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 402==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 403==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 404==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 405==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 406==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 407==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 408==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 409==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 410==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 411==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 412==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 413==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 414==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 415==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "==epoch 416==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 417==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 418==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 419==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 420==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 421==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 422==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 423==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 424==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 425==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 426==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==epoch 427==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 428==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 429==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 430==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 431==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 432==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 433==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 434==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "==epoch 435==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 436==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 437==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 438==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 439==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 440==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 441==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 442==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 443==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 444==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 445==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 446==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 447==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 448==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 449==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 450==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 451==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 452==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 453==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 454==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 455==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "==epoch 456==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 457==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 458==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 459==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 460==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 461==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 462==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 463==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 464==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 465==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 466==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 467==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 468==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 469==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 470==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 471==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 472==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 473==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 474==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 475==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 476==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 477==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 478==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 479==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0068, grad_fn=<MseLossBackward>)\n",
      "==epoch 480==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 481==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 482==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 483==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 484==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 485==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 486==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 487==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 488==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 489==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 490==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 491==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 492==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 493==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 494==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 495==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 496==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 497==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 498==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "==epoch 499==\n",
      "training loss: 0.0\n",
      "validation loss: tensor(0.0067, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train(net, criterion, optimizer, train_X, train_Y, val_X, val_Y, batchsize=100, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "check = torch.Tensor(np.array([0.9, 2.0, 1.0, 1.0])).unsqueeze(1).transpose(1,0)\n",
    "print(check.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0781599283218384\n"
     ]
    }
   ],
   "source": [
    "check_out = net(check)\n",
    "print(float(check_out.data[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.591174268424552, 0.2178092012053261)\n",
      "1.1090354888959124\n",
      "1.1697236254452936\n"
     ]
    }
   ],
   "source": [
    "#how does the approx value function perform against naive?\n",
    "x_bar = 1\n",
    "pi_cp_perc = 0.6\n",
    "pi_cp = 0.6*T*log_utility(x_bar)\n",
    "\n",
    "naive_reward = T*log_utility(x_bar) - pi_cp*x_bar\n",
    "\n",
    "naive_opts = log_utility_opt_x(T, pi_cp)\n",
    "print(naive_opts)\n",
    "\n",
    "nopt_reward = T*log_utility(naive_opts[1]) - pi_cp *naive_opts[1]\n",
    "\n",
    "print(naive_reward)\n",
    "print(nopt_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win/loss perc: 0.86\n",
      "expected utility gap: 0.17289745530875766\n",
      "constraint violation perc: 0.0\n"
     ]
    }
   ],
   "source": [
    "#how does the approx value function perform against naive?\n",
    "MC = 100\n",
    "\n",
    "winloss = []\n",
    "rewards = []\n",
    "best_responses = []\n",
    "gaps = []\n",
    "cvs = 0\n",
    "\n",
    "x_1 = 0.7\n",
    "for i in range(MC):\n",
    "    smi = np.random.randint(0,s_vals.shape[0])\n",
    "    #try with  noise we trained with\n",
    "    sm = s_vals[smi]\n",
    "    \n",
    "    inputvec = torch.Tensor(np.array([x_1, sm, 1.0, 1.0])).unsqueeze(1).transpose(1,0)\n",
    "    #inputvec = torch.Tensor(np.array([x_1, sm, 1.0])).unsqueeze(1).transpose(1,0) #without linear bias\n",
    "    \n",
    "    br = net(inputvec)\n",
    "    best_response = float(br.data[0,0])\n",
    "    \n",
    "    if best_response > x_bar:\n",
    "        best_response = x_bar\n",
    "    if best_response < 0.0:\n",
    "        best_response = 0.0\n",
    "        \n",
    "    if np.abs(x_1 - best_response) > ramp_const:\n",
    "        cvs += 1\n",
    "    \n",
    "    best_responses.append(best_response)\n",
    "    plays = np.array([x_1, best_response])\n",
    "    noises = np.array([sm, np.random.choice(s_vals)])\n",
    "    \n",
    "    reward = log_utility(plays) - pi_cp*plays[np.argmax(noises)]\n",
    "    rewards.append(reward)\n",
    "    if reward > nopt_reward:\n",
    "        winloss.append(1)\n",
    "    else:\n",
    "        winloss.append(0)\n",
    "    gaps.append(reward - nopt_reward)\n",
    "        \n",
    "print(\"win/loss perc: \" + str(np.mean(winloss)))\n",
    "print(\"expected utility gap: \" + str(np.mean(gaps)))\n",
    "print(\"constraint violation perc: \" + str(cvs/MC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0299,  0.5195, -0.7092, -0.7588],\n",
       "        [ 0.3733,  1.8390, -0.5340, -0.5034],\n",
       "        [-2.2650,  1.0337, -0.9802, -0.2564],\n",
       "        [ 1.8102,  0.1315, -0.6048,  0.0329]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.l1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0003,  0.5817, -1.4422,  1.0523]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.l2.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#repeat process for T = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = 3\n",
    "grid_file_name = \"/home/chase/projects/peakload/notebooks/t\" + str(T) + \"_grid_search.txt\"\n",
    "\n",
    "with open(grid_file_name, 'rb') as d:\n",
    "    grid_MC = np.load(grid_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 40, 11, 40, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_MC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((2,2))\n",
    "a[1,1] = 1.0\n",
    "a[0,0] = 2.0\n",
    "\n",
    "\n",
    "\n",
    "a[np.nonzero(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#maximum mean slice\n",
    "#max_xi = 0\n",
    "#m_run = 0#\n",
    "#\n",
    "#for i in range(grid_MC.shape[0]):\n",
    "#    for k in range(grid_MC.shape[2]):\n",
    "#        noise_outcomes = grid_MC[i,:,k,:,:] #all choices of x_3 w.r.t. s_1, s_2\n",
    "#        mean_en = np.mean(grid_MC[i,:,k,:,:])\n",
    "#        if mean_en > m_run:\n",
    "#            m_run = mean_en\n",
    "#            max_xij = (i,k)\n",
    "#        \n",
    "#print(g[i],g[k])\n",
    "#print(np.mean(grid_MC[i,:,k,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chase/applications/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chase/applications/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#sample some paths even split between x_1 and x_2\n",
    "\n",
    "g = np.arange(0,1.1,0.1)\n",
    "s_vals = np.arange(-2.0,2.0,0.1)\n",
    "ramp_const = 0.3\n",
    "\n",
    "N = 5000\n",
    "half = int(N/2)\n",
    "\n",
    "best_responses = []\n",
    "\n",
    "X = np.zeros((4, N))\n",
    "\n",
    "Y = np.zeros((N,))\n",
    "\n",
    "ind = np.arange(0,N,1)\n",
    "\n",
    "for i in list(ind[0:half]):\n",
    "    x_1_i = np.random.randint(0,11)\n",
    "    x_1 = g[x_1_i]\n",
    "    X[0,i] = x_1  #previous state\n",
    "\n",
    "    s_1_i = np.random.randint(s_vals.shape[0])\n",
    "    X[1,i] = s_vals[s_1_i] #maximum noise seen so far\n",
    "\n",
    "    X[2,i] = 2 #rounds left to go\n",
    "    \n",
    "    X[3,i] = np.random.uniform(0,1) #linear bias term\n",
    "    \n",
    "    best_response_i = np.argmax(grid_MC[x_1_i, s_1_i, :])#pick best x_2 that maximizes expected reward given x_1, s_1\n",
    "    \n",
    "    #max expected reward independent of noise\n",
    "    #get all choices of x_2, x_3 with respect to noise s_1\n",
    "    best_reward = 0.0\n",
    "    best_x_2_i = 0\n",
    "    outcomes = grid_MC[x_1_i,s_1_i,:,:,:]\n",
    "    for x_2_i in range(g.shape[0]):\n",
    "        x_2 = g[x_2_i]\n",
    "        rewards_over_s_2 = []\n",
    "        if np.abs(x_2 - x_1) > ramp_const:\n",
    "            pass\n",
    "        else:\n",
    "            for s_2_i in range(s_vals.shape[0]):\n",
    "                for x_3_i in range(g.shape[0]):\n",
    "                    x_3 = g[x_3_i]\n",
    "                    if np.abs(x_3 - x_2) > ramp_const:\n",
    "                        pass\n",
    "                    else:\n",
    "                        rewards_over_s_2.append(grid_MC[x_1_i, s_1_i, x_2_i, s_2_i, x_3_i])\n",
    "        expected_reward = np.mean(rewards_over_s_2)\n",
    "        if expected_reward > best_reward:\n",
    "            best_reward = expected_reward\n",
    "            best_x_2_i = x_2_i\n",
    "                \n",
    "    Y[i] = g[best_x_2_i]\n",
    "    \n",
    "for i in list(ind[half:]):\n",
    "    x_2_i = np.random.randint(0,11) #ignore values you'd never start from\n",
    "    X[0,i] = g[x_2_i]  #previous state\n",
    "\n",
    "    s_1_i = np.random.randint(s_vals.shape[0])\n",
    "    s_2_i = np.random.randint(s_vals.shape[0])\n",
    "    X[1,i] = np.max([s_vals[s_1_i], s_vals[s_2_i]]) #maximum noise seen so far, want to reflect true s_m distribution shape so include iid samples from previous rounds\n",
    "\n",
    "    X[2,i] = 1 #rounds left to go\n",
    "    \n",
    "    X[3,i] = np.random.uniform(0,1) #linear bias term\n",
    "    \n",
    "    best_response_i = np.argmax(grid_MC[x_1_i, s_1_i, x_2_i, s_2_i, :])#pick best x_3 that maximizes expected reward given x_1, s_1, x_2, s_2\n",
    "    Y[i] = g[best_response_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = X[:,0:int(0.8*X.shape[1])]\n",
    "train_Y = Y[0:int(0.8*Y.shape[0])]\n",
    "val_X = X[:,int(0.8*X.shape[1]):]\n",
    "val_Y = Y[int(0.8*Y.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'FEATURE_DIM': 4, 'OUTPUT_DIM': 1, 'HIDDEN_1': 4}\n",
    "\n",
    "net3 = nnet(params)#.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net3.parameters(),lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train(net3, criterion, optimizer, train_X, train_Y, val_X, val_Y, batchsize=100, epochs=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.5911742684245525, 0.2178092012053261)\n",
      "1.6635532333438685\n",
      "1.7545854381679407\n"
     ]
    }
   ],
   "source": [
    "#how does the approx value function perform against naive?\n",
    "T = 3\n",
    "x_bar = 1\n",
    "pi_cp_perc = 0.6\n",
    "pi_cp = 0.6*T*log_utility(x_bar)\n",
    "\n",
    "naive_reward = T*log_utility(x_bar) - pi_cp*x_bar\n",
    "\n",
    "naive_opts = log_utility_opt_x(T, pi_cp)\n",
    "print(naive_opts)\n",
    "\n",
    "nopt_reward = T*log_utility(naive_opts[1]) - pi_cp *naive_opts[1]\n",
    "\n",
    "print(naive_reward)\n",
    "print(nopt_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win/loss perc: 0.67\n",
      "expected utility gap: 0.17411611076800748\n",
      "constraint violation perc: 1.62\n"
     ]
    }
   ],
   "source": [
    "#how does the approx value function perform against naive?\n",
    "MC = 100\n",
    "\n",
    "winloss = []\n",
    "rewards = []\n",
    "best_responses = []\n",
    "gaps = []\n",
    "cvs = 0\n",
    "\n",
    "\n",
    "######need to recurse on net()######################################\n",
    "x_1 = 0.7\n",
    "for i in range(MC):\n",
    "    noises = []\n",
    "    plays = []\n",
    "    x_curr = x_1\n",
    "    for i in range(T):\n",
    "\n",
    "        t = i+1 #we've already set x_1\n",
    "        \n",
    "        si = np.random.randint(0,s_vals.shape[0])\n",
    "        s = s_vals[si]\n",
    "        noises.append(s)\n",
    "        \n",
    "        sm = np.max(noises) #max so far\n",
    "    \n",
    "        inputvec = torch.Tensor(np.array([x_curr, sm, T-t, 1.0])).unsqueeze(1).transpose(1,0)\n",
    "    \n",
    "        br = net3(inputvec)\n",
    "        best_response = float(br.data[0,0])\n",
    "        \n",
    "        if best_response > x_bar:\n",
    "            best_response = x_bar\n",
    "        if best_response < 0.0:\n",
    "            best_response = 0.0\n",
    "        \n",
    "        if np.abs(x_1 - best_response) > ramp_const:\n",
    "            cvs += 1\n",
    "\n",
    "        #attach previous play\n",
    "        plays.append(x_curr)\n",
    "        x_curr = best_response #update play\n",
    "    \n",
    "    reward = log_utility(np.array(plays)) - pi_cp*plays[np.argmax(noises)]\n",
    "    rewards.append(reward)\n",
    "    if reward > nopt_reward:\n",
    "        winloss.append(1)\n",
    "    else:\n",
    "        winloss.append(0)\n",
    "    gaps.append(reward - nopt_reward)\n",
    "        \n",
    "print(\"win/loss perc: \" + str(np.mean(winloss)))\n",
    "print(\"expected utility gap: \" + str(np.mean(gaps)))\n",
    "print(\"constraint violation perc: \" + str(cvs/MC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0299,  0.5195, -0.7092, -0.7588],\n",
       "        [ 0.3733,  1.8390, -0.5340, -0.5034],\n",
       "        [-2.2650,  1.0337, -0.9802, -0.2564],\n",
       "        [ 1.8102,  0.1315, -0.6048,  0.0329]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.l1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.9488,  1.1332, -2.3304, -0.1350],\n",
       "        [13.9292,  0.3053, -6.9772, -0.0780],\n",
       "        [-2.1691, -0.2620, -0.7883, -0.1084],\n",
       "        [-5.3243,  1.2536, -1.2958,  0.1740]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net3.l1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0003,  0.5817, -1.4422,  1.0523]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.l2.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6252, -1.0269, -0.9762, -0.8576]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net3.l2.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need to normalize rounds_left_to_go input data\n",
    "#parameter tuning, training is sensitive\n",
    "#call this function a policy since its a mapping from state to action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "x_1 = 0.7\n",
    "ramp_const = 0.3\n",
    "g[(g > x_1 - ramp_const) & (g < x_1 + ramp_const)]\n",
    "x_2_i = np.random.choice(g[(g > x_1 - ramp_const) & (g < x_1 + ramp_const)])\n",
    "print(x_2_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.5911742684245525, 0.2178092012053261)\n",
      "1.6635532333438685\n",
      "1.7545854381679407\n"
     ]
    }
   ],
   "source": [
    "#forward simulate, take the best result\n",
    "#how does the approx value function perform against naive?\n",
    "T = 3\n",
    "x_bar = 1\n",
    "pi_cp_perc = 0.6\n",
    "pi_cp = 0.6*T*log_utility(x_bar)\n",
    "\n",
    "naive_reward = T*log_utility(x_bar) - pi_cp*x_bar\n",
    "\n",
    "naive_opts = log_utility_opt_x(T, pi_cp)\n",
    "print(naive_opts)\n",
    "\n",
    "nopt_reward = T*log_utility(naive_opts[1]) - pi_cp *naive_opts[1]\n",
    "\n",
    "print(naive_reward)\n",
    "print(nopt_reward)\n",
    "\n",
    "#sample some paths even split between x_1 and x_2\n",
    "\n",
    "g = np.arange(0,1.1,0.1)\n",
    "s_vals = np.arange(-2.0,2.0,0.1)\n",
    "ramp_const = 0.3\n",
    "\n",
    "N = 5000\n",
    "half = int(N/2) #divide up by number of possible initial states, x_1, x_2 for T = 3\n",
    "\n",
    "best_responses = []\n",
    "\n",
    "X = np.zeros((4, N))\n",
    "\n",
    "Y = np.zeros((N,))\n",
    "\n",
    "ind = np.arange(0,N,1)\n",
    "\n",
    "sim_MC = 100\n",
    "\n",
    "for i in list(ind[0:half]):\n",
    "    x_1_i = np.random.randint(0,11)\n",
    "    x_1 = g[x_1_i]\n",
    "    X[0,i] = x_1  #previous state\n",
    "\n",
    "    s_1_i = np.random.randint(s_vals.shape[0])\n",
    "    s_1 = s_vals[s_1_i]\n",
    "    X[1,i] = s_1 #maximum noise seen so far\n",
    "\n",
    "    X[2,i] = 2 #rounds left to go\n",
    "    \n",
    "    X[3,i] = np.random.uniform(0,1) #linear bias term\n",
    "\n",
    "    #forward simulate x_2, x_3\n",
    "    best_x_2_i = 0\n",
    "    best_reward = 0.0\n",
    "    for x_2_i in range(g.shape[0]):\n",
    "        x_2 = g[x_2_i]\n",
    "        if np.abs(x_2 - x_1) > ramp_const:\n",
    "            pass\n",
    "        else:\n",
    "            for s in range(sim_MC):\n",
    "                g_const = g[(g > x_2 - ramp_const) & (g < x_2 + ramp_const)]\n",
    "                x_3 = np.random.choice(g_const)\n",
    "                s_2 = np.random.choice(s_vals)\n",
    "                s_3 = np.random.choice(s_vals)\n",
    "                plays = np.array([x_1, x_2, x_3])\n",
    "                noises = np.array([s_1, s_2, s_3])\n",
    "\n",
    "                sim_reward = log_utility(plays) - pi_cp*plays[np.argmax(noises)] \n",
    "                if sim_reward > best_reward:\n",
    "                    best_reward = sim_reward\n",
    "                    best_x_2 = x_2\n",
    "                \n",
    "    Y[i] = best_x_2\n",
    "    \n",
    "for i in list(ind[half:]):\n",
    "    x_2_i = np.random.randint(0,11)\n",
    "    x_2 = g[x_2_i]\n",
    "    x_1 = np.random.choice(g[(g > x_2 - ramp_const) & (g < x_2 + ramp_const)])\n",
    "    X[0,i] = x_2  #previous state\n",
    "\n",
    "    s_1_i = np.random.randint(s_vals.shape[0])\n",
    "    s_1 = s_vals[s_1_i]\n",
    "    s_2_i = np.random.randint(s_vals.shape[0])\n",
    "    s_2 = s_vals[s_2_i]\n",
    "    X[1,i] = np.max([s_1, s_2]) #maximum noise seen so far, want to reflect true s_m distribution shape so include iid samples from previous rounds\n",
    "\n",
    "    X[2,i] = 1 #rounds left to go\n",
    "    \n",
    "    X[3,i] = np.random.uniform(0,1) #linear bias term\n",
    "    \n",
    "    g_const = g[(g > x_2 - ramp_const) & (g < x_2 + ramp_const)]\n",
    "    best_x_3 = 0\n",
    "    best_reward = 0.0\n",
    "    for s in range(sim_MC):\n",
    "        x_3 = np.random.choice(g_const)\n",
    "        s_3 = np.random.choice(s_vals)\n",
    "        \n",
    "        noises = np.array([s_1, s_2, s_3])\n",
    "        plays = np.array([x_1, x_2, x_3])\n",
    "    \n",
    "        sim_reward = log_utility(plays) - pi_cp*plays[np.argmax(noises)]\n",
    "        if sim_reward > best_reward:\n",
    "            best_reward = sim_reward\n",
    "            best_x_3 = x_3\n",
    "    Y[i] = best_x_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = X[:,0:int(0.8*X.shape[1])]\n",
    "train_Y = Y[0:int(0.8*Y.shape[0])]\n",
    "val_X = X[:,int(0.8*X.shape[1]):]\n",
    "val_Y = Y[int(0.8*Y.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'FEATURE_DIM': 4, 'OUTPUT_DIM': 1, 'HIDDEN_1': 4}\n",
    "\n",
    "net3 = nnet(params)#.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net3.parameters(),lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train(net3, criterion, optimizer, train_X, train_Y, val_X, val_Y, batchsize=100, epochs=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win/loss perc: 0.84\n",
      "expected utility gap: 0.17209520052769747\n",
      "constraint violation perc: 0.0\n"
     ]
    }
   ],
   "source": [
    "#how does the approx value function perform against naive?\n",
    "MC = 100\n",
    "\n",
    "winloss = []\n",
    "rewards = []\n",
    "best_responses = []\n",
    "gaps = []\n",
    "cvs = 0\n",
    "\n",
    "\n",
    "######need to recurse on net()######################################\n",
    "x_1 = 0.7\n",
    "for j in range(MC):\n",
    "    noises = []\n",
    "    plays = []\n",
    "    x_curr = x_1\n",
    "    for i in range(T):\n",
    "\n",
    "        t = i+1 #we've already set x_1\n",
    "        \n",
    "        si = np.random.randint(0,s_vals.shape[0])\n",
    "        s = s_vals[si]\n",
    "        noises.append(s)\n",
    "        \n",
    "        sm = np.max(noises) #max so far\n",
    "    \n",
    "        inputvec = torch.Tensor(np.array([x_curr, sm, T-t, 1.0])).unsqueeze(1).transpose(1,0)\n",
    "    \n",
    "        br = net3(inputvec)\n",
    "        best_response = float(br.data[0,0])\n",
    "        \n",
    "        if best_response > x_bar:\n",
    "            best_response = x_bar\n",
    "        if best_response < 0.0:\n",
    "            best_response = 0.0\n",
    "            \n",
    "        #snap to nearest constraint\n",
    "        if np.abs(x_curr - best_response) > ramp_const:\n",
    "            if best_response > x_curr:\n",
    "                best_response = x_curr + ramp_const\n",
    "            elif best_response < x_curr:\n",
    "                best_response = x_curr - ramp_const\n",
    "        \n",
    "        #if np.abs(x_1 - best_response) > ramp_const:\n",
    "        #    cvs += 1\n",
    "\n",
    "        #attach previous play\n",
    "        plays.append(x_curr)\n",
    "        x_curr = best_response #update play\n",
    "    \n",
    "    reward = log_utility(np.array(plays)) - pi_cp*plays[np.argmax(noises)]\n",
    "    rewards.append(reward)\n",
    "    if reward > nopt_reward:\n",
    "        winloss.append(1)\n",
    "    else:\n",
    "        winloss.append(0)\n",
    "    gaps.append(reward - nopt_reward)\n",
    "        \n",
    "print(\"win/loss perc: \" + str(np.mean(winloss)))\n",
    "print(\"expected utility gap: \" + str(np.mean(gaps)))\n",
    "print(\"constraint violation perc: \" + str(cvs/MC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#can it outperform a concentration-inequality based method on E[s_max]?\n",
    "\n",
    "#to do\n",
    "#refactor above simulation code for arbitrary T, divide samples amongst x_t evenly\n",
    "#determine expected utility for T = 2, 10 (50?) \n",
    "#compare to naive opt for T = 2, 10 (50?)\n",
    "#compare to grid search for T = 2, 3, 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
