{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#im skipping leap days currently in multiple places\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ERCOT data\n",
    "subregions = ['COAST', 'EAST', 'FAR_WEST', 'NORTH', 'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST', 'ERCOT']\n",
    "other_subregions = ['COAST', 'EAST', 'FWEST', 'NORTH', 'NCENT', 'SOUTH', 'SCENT', 'WEST', 'ERCOT']\n",
    "\n",
    "\n",
    "lds = datetime.datetime(year=2016, month=2, day=29, hour=0, minute=0) #lines 1415 to 1439\n",
    "lde = datetime.datetime(year=2016, month=2, day=29, hour=23, minute=0)\n",
    "\n",
    "path = \"/home/chase/projects/peakload/data/ercot/\"\n",
    "ercot2017 = pd.read_excel(path + \"native_Load_2017.xlsx\") #different subregions Index(['Hour Ending', 'COAST', 'EAST', 'FWEST', 'NORTH', 'NCENT', 'SOUTH', 'SCENT', 'WEST', 'ERCOT'],dtype='object')\n",
    "ercot2016 = pd.read_excel(path + \"native_Load_2016.xlsx\") #leap year\n",
    "#ercot2016.drop(ercot2016.index[1415:1439], inplace=True)\n",
    "\n",
    "ercot2015 = pd.read_excel(path + \"native_Load_2015.xls\")\n",
    "ercot2014 = pd.read_excel(path + \"2014_ERCOT_Hourly_Load_Data.xls\")\n",
    "ercot2013 = pd.read_excel(path + \"2013_ERCOT_Hourly_Load_Data.xls\")\n",
    "ercot2012 = pd.read_excel(path + \"2012_ERCOT_Hourly_Load_Data.xls\") #leap year\n",
    "#ercot2012.drop(ercot2012.index[1415:1439], inplace=True)\n",
    "\n",
    "ercot2011 = pd.read_excel(path + \"2011_ERCOT_Hourly_Load_Data.xls\")\n",
    "ercot2010 = pd.read_excel(path + \"2010_ERCOT_Hourly_Load_Data.xls\")\n",
    "\n",
    "#ercotdata = list(ercot2010['ERCOT']) + list(ercot2011['ERCOT']) + list(ercot2012['ERCOT']) + list(ercot2013['ERCOT']) + list(ercot2014['ERCOT']) + list(ercot2015['ERCOT']) + list(ercot2016['ERCOT']) + list(ercot2017['ERCOT'])\n",
    "yearly_data = {}\n",
    "yearly_data['ERCOT'] = [list(ercot2010['ERCOT']), list(ercot2011['ERCOT']), list(ercot2012['ERCOT']), list(ercot2013['ERCOT']), list(ercot2014['ERCOT']), list(ercot2015['ERCOT']), list(ercot2016['ERCOT']), list(ercot2017['ERCOT'])]\n",
    "\n",
    "for region in subregions:\n",
    "    yearly_data[region] = [list(ercot2010[region]), list(ercot2011[region]), list(ercot2012[region]), list(ercot2013[region]), list(ercot2014[region]), list(ercot2015[region]), list(ercot2016[region])]\n",
    "\n",
    "for region in other_subregions:\n",
    "    r = subregions[other_subregions.index(region)]\n",
    "    yearly_data[r].append(list(ercot2017[region]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70128\n",
      "61368\n",
      "8760\n"
     ]
    }
   ],
   "source": [
    "startdate = datetime.datetime(year=2010, month=1, day=1, hour=0, minute=0, second=0)\n",
    "curr = startdate\n",
    "enddate = datetime.datetime(year=2018, month=1, day=1, hour=0, minute=0, second=0)\n",
    "\n",
    "allhours = []\n",
    "\n",
    "while curr < enddate:\n",
    "    allhours.append(curr)\n",
    "    curr += datetime.timedelta(hours=1)\n",
    "    \n",
    "trainhours = allhours[0:-8760] #all but last year, not a leap year\n",
    "testhours = allhours[-8760:]\n",
    "print(len(allhours))\n",
    "print(len(trainhours))\n",
    "print(len(testhours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with eliminated leap day, for calculating average year model\n",
    "#yearly_loads = np.zeros((8,8760))\n",
    "#\n",
    "#for i in range(len(yearly_data)):\n",
    "#    yearly_loads[i,:] = np.array(yearly_data[i]['ERCOT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testhours' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-700133d8aa37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtesthours\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'testhours' is not defined"
     ]
    }
   ],
   "source": [
    "testhours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for subregions\n",
    "vectorized_subregions = {}\n",
    "for region in subregions:\n",
    "    vectorized_subregions[region] = {}\n",
    "    train_data_vec = np.array(yearly_data[region][0]) - np.mean(yearly_data[region][0])\n",
    "    means = [np.mean(yearly_data[region][0])]\n",
    "    for i in range(1, len(yearly_data[region])-1):\n",
    "        yr = yearly_data[region][i]\n",
    "        m = np.nanmean(yr)\n",
    "        means.append(m)\n",
    "        train_data_vec = np.concatenate((train_data_vec, np.array(yr)-m), axis=0)\n",
    "        \n",
    "    vectorized_subregions[region]['train_data'] = train_data_vec\n",
    "\n",
    "    train_data_labels = np.zeros((len(yearly_data[region][0]),))\n",
    "    for i in range(len(yearly_data[region][0])):\n",
    "        greater = np.sum([1 for k in yearly_data[region][0] if k >= yearly_data[region][0][i]])/float(len(yearly_data[region][0]))\n",
    "        train_data_labels[i] = 1.0 - greater\n",
    "    \n",
    "    for i in range(1, len(yearly_data[region])-1):\n",
    "        year_data = yearly_data[region][i]\n",
    "        train_data_l = np.zeros((len(year_data),))\n",
    "        for j in range(len(year_data)):\n",
    "            greater = np.sum([1 for k in year_data if k <= year_data[j]])/float(len(year_data))\n",
    "            train_data_l[j] = greater\n",
    "        train_data_labels = np.concatenate((train_data_labels, train_data_l), axis=0)\n",
    "            \n",
    "    vectorized_subregions[region]['train_labels'] = train_data_labels\n",
    "    \n",
    "    vectorized_subregions[region]['means'] = means\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61368"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_subregions['ERCOT']['train_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202.21956698167793 10330.675211917747\n",
      "1.1445290359099922 1389.0497308542554\n",
      "157.5580701856505 1235.861525615093\n",
      "-6.286314908476045 862.2773959771353\n",
      "55.294436102682454 12689.296038363827\n",
      "74.74464079097235 2906.584283652104\n",
      "69.80779480723267 6095.32943668585\n",
      "14.267152541100186 1046.8526042098674\n",
      "568.7498755367494 36555.92622727587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chase/applications/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:4: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    }
   ],
   "source": [
    "#use system-wide loadgrowth for all regions\n",
    "for region in vectorized_subregions:\n",
    "    A = np.vstack((np.array([0, 1, 2, 3, 4, 5, 6]), np.array([1, 1, 1, 1, 1, 1, 1]))).T\n",
    "    a, b = np.linalg.lstsq(A, np.array(vectorized_subregions[region]['means']))[0]\n",
    "    print(a, b)\n",
    "    projected_mean = a*7.0 + b\n",
    "    vectorized_subregions[region]['projected_mean'] = projected_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for region in vectorized_subregions:\n",
    "    test_data_vec = np.array(yearly_data[region][-1]) - vectorized_subregions[region]['projected_mean']\n",
    "\n",
    "    test_data_labels = np.zeros((test_data_vec.shape[0],))        \n",
    "    for i in range(test_data_vec.shape[0]):\n",
    "        greater = np.sum([1 for k in test_data_vec if k >= test_data_vec[i]])/float(test_data_vec.shape[0])\n",
    "        test_data_labels[i] = 1.0 - greater\n",
    "        \n",
    "    vectorized_subregions[region]['test_data'] = test_data_vec\n",
    "    vectorized_subregions[region]['test_labels'] = test_data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61368,) (8760,) (61368,) (8760,)\n",
      "(61368,) (8760,) (61368,) (8760,)\n",
      "(61368,) (8760,) (61368,) (8760,)\n",
      "(61368,) (8760,) (61368,) (8760,)\n",
      "(61368,) (8760,) (61368,) (8760,)\n",
      "(61368,) (8760,) (61368,) (8760,)\n",
      "(61368,) (8760,) (61368,) (8760,)\n",
      "(61368,) (8760,) (61368,) (8760,)\n",
      "(61368,) (8760,) (61368,) (8760,)\n"
     ]
    }
   ],
   "source": [
    "for region in vectorized_subregions:\n",
    "    print(vectorized_subregions[region]['train_data'].shape, vectorized_subregions[region]['test_data'].shape, vectorized_subregions[region]['train_labels'].shape, vectorized_subregions[region]['test_labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def itermean(oldmean, newval, num):\n",
    "    if np.isnan(newval):\n",
    "        out = oldmean\n",
    "    else:\n",
    "        out = oldmean + ((newval - oldmean)/float(num))\n",
    "    return(out)\n",
    "\n",
    "def itervar(oldvar, oldmean, newval, num):\n",
    "    if np.isnan(newval):\n",
    "        out = oldvar\n",
    "    else:\n",
    "        n = float(num)\n",
    "        out = ((n - 1)*oldvar + (newval - oldmean)*(newval - oldmean))/n\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70128,)\n",
      "70128\n",
      "8760\n",
      "61368\n"
     ]
    }
   ],
   "source": [
    "all_loads = np.concatenate((vectorized_subregions['ERCOT']['train_data'], vectorized_subregions['ERCOT']['test_data']))\n",
    "print(all_loads.shape)\n",
    "print(len(allhours))\n",
    "print(len(testhours))\n",
    "print(len(trainhours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chase/applications/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:13: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/home/chase/applications/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:5: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/home/chase/applications/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/chase/applications/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/chase/applications/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:780: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= first_edge)\n",
      "/home/chase/applications/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:781: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= last_edge)\n"
     ]
    }
   ],
   "source": [
    "#do this for 'ERCOT' only\n",
    "\n",
    "\n",
    "load_features = {}\n",
    "seasons = {1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 2, 8: 2, 9: 2, 10: 3, 11: 3, 12: 3}\n",
    "\n",
    "maxsofar = 0.0\n",
    "meansofar = 0.0\n",
    "varsofar = 0.0\n",
    "year = 2010\n",
    "for i in range(len(allhours)):\n",
    "    load_features[allhours[i]] = {}\n",
    "    loads_dayhour = []\n",
    "    loads_hour = []\n",
    "    \n",
    "    currtime = allhours[i]\n",
    "    curr_year = currtime.year\n",
    "    if curr_year > year:\n",
    "        year = curr_year\n",
    "        maxsofar = 0.0\n",
    "        meansofar = 0.0\n",
    "        varsofar = 0.0\n",
    "    if all_loads[i] > maxsofar:\n",
    "        maxsofar = all_loads[i]\n",
    "    varsofar = itervar(varsofar, meansofar, all_loads[i], i)\n",
    "    meansofar = itermean(meansofar, all_loads[i], i)\n",
    "    \n",
    "    dt_back = currtime\n",
    "    stop_1 = currtime - datetime.timedelta(days=30*4)\n",
    "    stop_2 = currtime - datetime.timedelta(days=30*2)\n",
    "    \n",
    "    while dt_back > stop_1:\n",
    "        if dt_back < allhours[0]:\n",
    "            break\n",
    "        if dt_back.month == 2 and dt_back.day == 29:\n",
    "            pass\n",
    "        else:\n",
    "            #get loads over the last 2 months by matching day/hour\n",
    "            if dt_back.hour == currtime.hour and dt_back.weekday() == currtime.weekday():\n",
    "                loads_dayhour.append(all_loads[allhours.index(dt_back)])\n",
    "            #get loads over the last 2 months by matching hour\n",
    "            if dt_back > stop_2:\n",
    "                if dt_back.hour == currtime.hour:\n",
    "                    loads_hour.append(all_loads[allhours.index(dt_back)])\n",
    "        dt_back -= datetime.timedelta(hours=24)\n",
    "    \n",
    "    dhhist = np.histogram(loads_dayhour, bins=7, range=(np.nanmin(loads_dayhour), np.nanmax(loads_dayhour)))[0]\n",
    "    dayhourmax = np.nanmax(loads_dayhour)\n",
    "    dayhourmin = np.nanmin(loads_dayhour)\n",
    "    dayhourhist = (1.0/sum(dhhist)) * dhhist\n",
    "    dayhourmean = np.nanmean(loads_dayhour)\n",
    "    dayhourvar = np.nanvar(loads_dayhour)\n",
    "    hourmax = np.nanmax(loads_hour)\n",
    "    hourmin = np.nanmin(loads_hour)\n",
    "    hhist = np.histogram(loads_hour, bins=7, range=(np.nanmin(loads_hour), np.nanmax(loads_hour)))[0]\n",
    "    hourhist = (1.0/sum(hhist)) * hhist\n",
    "    hourmean = np.nanmean(loads_hour)\n",
    "    hourvar = np.nanvar(loads_hour)\n",
    "    \n",
    "    load_features[currtime][\"dayhourmax\"] = dayhourmax #1\n",
    "    load_features[currtime][\"dayhourmin\"] = dayhourmin #1\n",
    "    load_features[currtime][\"dayhourhist\"] = dayhourhist #7\n",
    "    load_features[currtime][\"dayhourmean\"] = dayhourmean #1\n",
    "    load_features[currtime][\"dayhourvar\"] = dayhourvar #1\n",
    "    load_features[currtime][\"hourmax\"] = hourmax #1\n",
    "    load_features[currtime][\"hourmin\"] = hourmin #1\n",
    "    load_features[currtime][\"hourhist\"] = hourhist #7\n",
    "    load_features[currtime][\"hourmean\"] = hourmean #1\n",
    "    load_features[currtime][\"hourvar\"] = hourvar #1\n",
    "    load_features[currtime][\"maxloadsofar\"] = maxsofar #1\n",
    "    load_features[currtime][\"meanloadsofar\"] = meansofar #1\n",
    "    load_features[currtime][\"varsofar\"] = varsofar #1\n",
    "    load_features[currtime][\"month\"] = currtime.month #1\n",
    "    load_features[currtime][\"week\"] = currtime.isocalendar()[1] #1\n",
    "    load_features[currtime][\"weekday\"] = currtime.weekday() #1\n",
    "    load_features[currtime][\"season\"] = seasons[currtime.month] #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_features_copy = copy.copy(load_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_features = {}\n",
    "for key in load_features_copy:\n",
    "    t = str(int(time.mktime(key.timetuple())))\n",
    "    out_features[t] = {}\n",
    "    for kk in load_features_copy[key]:\n",
    "        item = load_features_copy[key][kk]\n",
    "        if type(item) == np.ndarray:\n",
    "            out_features[t][kk] = item.tolist()\n",
    "        else:\n",
    "            out_features[t][kk] = item\n",
    "        \n",
    "\n",
    "with open(\"/home/chase/projects/peakload/data/ercot/load_features.json\", 'w') as d:\n",
    "    out = json.dumps(out_features)\n",
    "    d.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reload feature data\n",
    "with open(\"/home/chase/projects/peakload/data/ercot/load_features.json\", 'r') as d:\n",
    "    load_features = json.load(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature construction\n",
    "lookback = 24*7\n",
    "numfeatures = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = list(load_features.keys())\n",
    "test_times = times[-8760:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training data input arrays, output_arrays 24 hours ahead\n",
    "training_data_pairs = []\n",
    "\n",
    "for i in range(len(trainhours)-lookback-24): #must offset for last 24 hours in dataset\n",
    "    dt = trainhours[i]\n",
    "    currtime = str(int(time.mktime(trainhours[i].timetuple())))\n",
    "    data_point = np.zeros((lookback, numfeatures)) #current number of features\n",
    "    label = np.zeros((24,1))\n",
    "    for j in range(lookback):\n",
    "        features = load_features[currtime]\n",
    "        data_point[j,0] = vectorized_subregions['ERCOT']['train_data'][i+j]\n",
    "        #data_point[j,1:8] = features[\"hourhist\"]\n",
    "        #data_point[j,8:15] = features[\"dayhourhist\"]vectorized_subregions['ERCOT']['train_data'][i+j]\n",
    "        data_point[j,1] = features[\"dayhourmax\"]\n",
    "        data_point[j,2] = features[\"dayhourmin\"]\n",
    "        data_point[j,3] = features[\"dayhourmean\"]\n",
    "        data_point[j,4] = features[\"dayhourvar\"]\n",
    "        data_point[j,5] = features[\"hourmax\"]\n",
    "        data_point[j,6] = features[\"hourmin\"]\n",
    "        data_point[j,7] = features[\"hourmean\"]\n",
    "        data_point[j,8] = features[\"hourvar\"]\n",
    "        data_point[j,9] = features[\"month\"]\n",
    "        data_point[j,10] = features[\"week\"]\n",
    "        data_point[j,11] = features[\"weekday\"]\n",
    "        data_point[j,12] = features[\"season\"]\n",
    "        data_point[j,13] = features[\"maxloadsofar\"]\n",
    "        data_point[j,14] = features[\"meanloadsofar\"]\n",
    "        data_point[j,15] = features[\"varsofar\"]\n",
    "        data_point[j,16] = float(dt.hour)/24.0\n",
    "    for j in range(24):\n",
    "        label[j,0] = train_data_labels[(i+24)+j]\n",
    "    training_data_pairs.append((data_point, label))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_pairs = []\n",
    "for i in range(len(testhours)-lookback-24):\n",
    "    dt = testhours[i]\n",
    "    currtime = str(int(time.mktime(testhours[i].timetuple())))\n",
    "    data_point = np.zeros((lookback,numfeatures))\n",
    "    label = np.zeros((24,1))\n",
    "    for j in range(lookback):\n",
    "        features = load_features[currtime]\n",
    "        data_point[j,0] = vectorized_subregions['ERCOT']['test_data'][i+j]\n",
    "        #data_point[j,1:8] = features[\"hourhist\"]\n",
    "        #data_point[j,8:15] = features[\"dayhourhist\"]\n",
    "        data_point[j,1] = features[\"dayhourmax\"]\n",
    "        data_point[j,2] = features[\"dayhourmin\"]\n",
    "        data_point[j,3] = features[\"dayhourmean\"]\n",
    "        data_point[j,4] = features[\"dayhourvar\"]\n",
    "        data_point[j,5] = features[\"hourmax\"]\n",
    "        data_point[j,6] = features[\"hourmin\"]\n",
    "        data_point[j,7] = features[\"hourmean\"]\n",
    "        data_point[j,8] = features[\"hourvar\"]\n",
    "        data_point[j,9] = features[\"month\"]\n",
    "        data_point[j,10] = features[\"week\"]\n",
    "        data_point[j,11] = features[\"weekday\"]\n",
    "        data_point[j,12] = features[\"season\"]\n",
    "        data_point[j,13] = features[\"maxloadsofar\"]\n",
    "        data_point[j,14] = features[\"meanloadsofar\"]\n",
    "        data_point[j,15] = features[\"varsofar\"]\n",
    "        data_point[j,16] = float(dt.hour)/24.0\n",
    "    for j in range(24):\n",
    "        label[j,0] = test_data_labels[(i+24)+j]\n",
    "    test_data_pairs.append((data_point, label))\n",
    "    \n",
    "all_data_pairs = training_data_pairs + test_data_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/chase/projects/peakload/data/ercot/formatted/load_features_training_data_pairs.pck\", 'wb') as d:\n",
    "    pickle.dump(training_data_pairs, d)\n",
    "    \n",
    "with open(\"/home/chase/projects/peakload/data/ercot/formatted/load_features_test_data_pairs.pck\", 'wb') as d:\n",
    "    pickle.dump(test_data_pairs, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/chase/projects/peakload/data/ercot/formatted/load_features_test_data_pairs.pck\", 'rb') as d:\n",
    "    test_data_pairs = pickle.load(d)\n",
    "    \n",
    "with open(\"/home/chase/projects/peakload/data/ercot/formatted/load_features_training_data_pairs.pck\", 'rb') as d:\n",
    "    training_data_pairs = pickle.load(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normcolumns = [i for i in range(17)]\n",
    "\n",
    "#only get normalization values from training data\n",
    "colmaxs = np.zeros((1,numfeatures))\n",
    "colmins = np.zeros((1,numfeatures))\n",
    "for pair in training_data_pairs:\n",
    "    currmax = np.max(pair[0],axis=0)\n",
    "    currmax = currmax[np.newaxis,:]\n",
    "    currmin = np.min(pair[0],axis=0)\n",
    "    currmin = currmin[np.newaxis,:]\n",
    "    \n",
    "    cmax = np.concatenate((colmaxs, currmax), axis=0)\n",
    "    cmin = np.concatenate((colmins, currmin), axis=0)\n",
    "    \n",
    "    colmaxs = np.max(cmax, axis=0)[np.newaxis,:]\n",
    "    colmins = np.min(cmin, axis=0)[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normed_training_data_pairs = []\n",
    "normed_test_data_pairs = []\n",
    "\n",
    "for pair in training_data_pairs:\n",
    "    normedarr = copy.copy(pair[0])\n",
    "    for column in normcolumns:\n",
    "        coldata = normedarr\n",
    "        coldata = (1.0/(colmaxs[0,column] - colmins[0,column])) * (pair[0][:,column] - colmins[0,column])\n",
    "        normedarr[:,column] = coldata.T\n",
    "    \n",
    "    normed_training_data_pairs.append((normedarr, pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/chase/projects/peakload/data/ercot/formatted/load_features_normed_training_data_pairs.pck\", 'wb') as d:\n",
    "    pickle.dump(normed_training_data_pairs, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pair in test_data_pairs:\n",
    "    normedarr = copy.copy(pair[0])\n",
    "    for column in normcolumns:\n",
    "        coldata = normedarr\n",
    "        coldata = (1.0/(colmaxs[0,column] - colmins[0,column])) * (pair[0][:,column] - colmins[0,column])\n",
    "        normedarr[:,column] = coldata.T\n",
    "    \n",
    "    normed_test_data_pairs.append((normedarr, pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/chase/projects/peakload/data/ercot/formatted/load_features_normed_test_data_pairs.pck\", 'wb') as d:\n",
    "    pickle.dump(normed_test_data_pairs, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/chase/projects/peakload/data/ercot/formatted/load_features_normed_training_data_pairs.pck\", 'rb') as d:\n",
    "    normed_training_data_pairs = pickle.load(d)\n",
    "    \n",
    "with open(\"/home/chase/projects/peakload/data/ercot/formatted/load_features_normed_test_data_pairs.pck\", 'rb') as d:\n",
    "    normed_test_data_pairs = pickle.load(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 17)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_training_data_pairs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8568"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normed_test_data_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load normalized weather data\n",
    "weatherdatapath = \"/home/chase/projects/peakload/data/weather/ercot/major_cities/array_formatted/\"\n",
    "\n",
    "with open(weatherdatapath + \"hourly_data_feature_vector.pck\", 'rb') as d:\n",
    "    hourly_weather = pickle.load(d)\n",
    "    \n",
    "with open(weatherdatapath + \"daily_data_feature_vector.pck\", 'rb') as d:\n",
    "    daily_weather = pickle.load(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70128\n",
      "70128\n"
     ]
    }
   ],
   "source": [
    "allweatherhours = list(hourly_weather.keys())\n",
    "print(len(allhours))\n",
    "\n",
    "train_weatherhours = allweatherhours[0:-8760]\n",
    "test_weatherhours = allweatherhours[-8760:]\n",
    "print(len(allweatherhours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training data pairs\n",
    "weather_array_path = \"/home/chase/projects/peakload/data/weather/ercot/major_cities/array_formatted/yearly_normed_arrays/\"\n",
    "\n",
    "weather_normed_training_data_pairs = []\n",
    "hourly_keys = list(hourly_weather.keys())\n",
    "\n",
    "for i in range(len(normed_training_data_pairs)):\n",
    "    pair = normed_training_data_pairs[i]\n",
    "    p1 = np.nan_to_num(pair[0]).flatten()\n",
    "    dtplus = trainhours[i+1]\n",
    "    day = datetime.datetime(year=dtplus.year, month=dtplus.month, day=dtplus.day, hour=0, minute=0, second=0)\n",
    "    day_weather_vec = daily_weather[day] #get the forecast\n",
    "    hourly_weather_vecs = []\n",
    "    for j in range(24):\n",
    "        hourly_weather_vecs.append(hourly_weather[hourly_keys[i+j+1]])\n",
    "        \n",
    "    wforecast = np.concatenate((np.asarray(hourly_weather_vecs).flatten(), day_weather_vec.flatten()), axis=0)\n",
    "    wf = np.nan_to_num(wforecast)\n",
    "    \n",
    "    weather_normed_training_data_pairs.append( (np.concatenate((p1, wf), axis=0), pair[1].flatten()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write the normalized weather data pairs\n",
    "i = 0\n",
    "for pair in weather_normed_training_data_pairs:\n",
    "    with open(\"/home/chase/projects/peakload/data/ercot/weather_pairs/training/pair_\" + str(i) + \".pck\", 'wb') as d:\n",
    "        pickle.dump(pair, d)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test data pairs\n",
    "weather_normed_test_data_pairs = []\n",
    "\n",
    "for i in range(len(normed_test_data_pairs)):\n",
    "    pair = normed_test_data_pairs[i]\n",
    "    p1 = np.nan_to_num(pair[0]).flatten()\n",
    "    dtplus = testhours[i+1]\n",
    "    day = datetime.datetime(year=dtplus.year, month=dtplus.month, day=dtplus.day, hour=0, minute=0, second=0)\n",
    "    day_weather_vec = daily_weather[day] #get the forecast\n",
    "    hourly_weather_vecs = []\n",
    "    for j in range(24):\n",
    "        hourly_weather_vecs.append(hourly_weather[test_weatherhours[i+j+1]])\n",
    "        \n",
    "    \n",
    "    wforecast = np.concatenate((np.asarray(hourly_weather_vecs).flatten(), day_weather_vec.flatten()), axis=0)\n",
    "    wf = np.nan_to_num(wforecast)\n",
    "    \n",
    "    weather_normed_test_data_pairs.append( (np.concatenate((p1, wf), axis=0), pair[1].flatten()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write the normalized weather data pairs\n",
    "i = 0\n",
    "for pair in weather_normed_test_data_pairs:\n",
    "    with open(\"/home/chase/projects/peakload/data/ercot/weather_pairs/test/pair_\" + str(i) + \".pck\", 'wb') as d:\n",
    "        pickle.dump(pair, d)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17030"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir = \"/home/chase/projects/peakload/data/ercot/weather_pairs/\"\n",
    "trainfiles = os.listdir(datadir + \"training/\")\n",
    "\n",
    "weather_normed_training_data_pairs = []\n",
    "for f in trainfiles:\n",
    "    with open(datadir + \"training/\" + f, 'rb') as d:\n",
    "        weather_normed_training_data_pairs.append(pickle.load(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61177\n"
     ]
    }
   ],
   "source": [
    "print(len(weather_normed_training_data_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(weather_normed_training_data_pairs)):\n",
    "    pair = weather_normed_training_data_pairs[i]\n",
    "    if np.isnan(pair[0]).any():\n",
    "        print(\"fail\")\n",
    "        for j in range(len(pair[0])):\n",
    "            if np.isnan(pair[0][j]):\n",
    "                pair[0][j] = 0.0\n",
    "        weather_normed_training_data_pairs[i] = (pair[0], pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/chase/projects/peakload/data/ercot/formatted/load_features_normed_test_data_pairs.pck\", 'rb') as d:\n",
    "    normed_test_data_pairs = pickle.load(d)\n",
    "    \n",
    "with open(\"/home/chase/projects/peakload/data/ercot/formatted/load_features_normed_training_data_pairs.pck\", 'rb') as d:\n",
    "    normed_training_data_pairs = pickle.load(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8760"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normed_test_data_pairs) + 168 + 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffle training/test data pairs, split out validation\n",
    "\n",
    "random.shuffle(weather_normed_training_data_pairs)\n",
    "training_pairs = weather_normed_training_data_pairs[0:int(0.9*float(len(weather_normed_training_data_pairs)))]\n",
    "val_pairs = weather_normed_training_data_pairs[int(0.9*float(len(weather_normed_training_data_pairs))):]\n",
    "\n",
    "random.shuffle(weather_normed_test_data_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17030,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_normed_training_data_pairs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class linear_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(linear_net, self).__init__()\n",
    "        self.D_in = training_pairs[0][0].shape[0]\n",
    "        self.H1 = 10000 #increased hidden size\n",
    "        #can try additional hidden layer\n",
    "        self.H2 = 5000\n",
    "        self.H3 = 500\n",
    "        self.D_out = 24\n",
    "        self.l1 = nn.Linear(self.D_in, self.H1)\n",
    "        self.l2 = nn.Linear(self.H1, self.H2)\n",
    "        self.l3 = nn.Linear(self.H2, self.H3)\n",
    "        self.l4 = nn.Linear(self.H3, self.D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.l1(x))\n",
    "        x = F.tanh(self.l2(x))\n",
    "        x = F.sigmoid(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write parameter objects to pck files\n",
    "\n",
    "call = \"python /home/chase/projects/peakload/src/python/train.py\"\n",
    "net =  \"/home/chase/projects/peakload/src/python/nets/single_hidden/single_hidden.py\"\n",
    "#paramfile = \"/home/chase/projects/peakload/src/python/nets/single_hidden/EWloss_h1_250_params.pck\"\n",
    "train_data_path = \"/home/chase/projects/peakload/data/ercot/formatted/load_features_normed_training_data_pairs.pck\"\n",
    "test_data_path = \"/home/chase/projects/peakload/data/ercot/formatted/load_features_normed_test_data_pairs.pck\"\n",
    "model_name = \"single_sigmoid\"\n",
    "\n",
    "epochs = 100\n",
    "lookback = 168 #one week in hours\n",
    "losses = [\"L1\", \"EW\"]\n",
    "hidden_1 = [250, 500, 1000]\n",
    "\n",
    "train_test_file = open(\"/home/chase/projects/peakload/src/python/nets/single_hidden/train_and_test.sh\", 'w')\n",
    "\n",
    "for loss in losses:\n",
    "    for h in hidden_1:\n",
    "        params = {'LOOKBACK': lookback, 'HIDDEN_1': h, 'LOSS': loss, 'EPOCHS': epochs, 'DATA': 'file'}\n",
    "        parampath = \"/home/chase/projects/peakload/src/python/nets/single_hidden/\" + loss + \"loss_h1_\" + str(h) + \"_params.pck\"\n",
    "        with open(parampath, 'wb') as p:\n",
    "            pickle.dump(params, p)\n",
    "        shcmd = \" \".join([call, net, parampath, train_data_path, test_data_path, model_name])\n",
    "        train_test_file.write(shcmd + \";\\n\")\n",
    "        \n",
    "train_test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write parameter objects to pck files\n",
    "\n",
    "call = \"python /home/chase/projects/peakload/src/python/train.py\"\n",
    "net =  \"/home/chase/projects/peakload/src/python/nets/tanh_hidden/tanh_hidden.py\"\n",
    "#paramfile = \"/home/chase/projects/peakload/src/python/nets/single_hidden/EWloss_h1_250_params.pck\"\n",
    "train_data_path = \"/home/chase/projects/peakload/data/ercot/formatted/load_features_normed_training_data_pairs.pck\"\n",
    "test_data_path = \"/home/chase/projects/peakload/data/ercot/formatted/load_features_normed_test_data_pairs.pck\"\n",
    "model_name = \"tanh_sigmoid\"\n",
    "\n",
    "epochs = 100\n",
    "lookback = 168 #one week in hours\n",
    "losses = [\"EW\"]\n",
    "hidden_1 = [250, 500, 1000, 1500]\n",
    "hidden_2 = [250, 500, 1000, 1500]\n",
    "hidden_3 = [250, 500, 1000, 1500]\n",
    "\n",
    "train_test_file = open(\"/home/chase/projects/peakload/src/python/nets/tanh_hidden/train_and_test.sh\", 'w')\n",
    "\n",
    "for loss in losses:\n",
    "    for i in range(len(hidden_2)):\n",
    "        for j in range(i, len(hidden_1)):\n",
    "            h1 = hidden_1[j]\n",
    "            h2 = hidden_2[i]\n",
    "            h3 = hidden_3[i]\n",
    "            params = {'LOOKBACK': lookback, 'HIDDEN_1': h1, 'HIDDEN_2': h2, 'HIDDEN_3': h3, 'LOSS': loss, 'EPOCHS': epochs, 'DATA': 'file'}\n",
    "            parampath = \"/home/chase/projects/peakload/src/python/nets/tanh_hidden/\" + loss + \"loss_h1_\" + str(h1) + \"_h2_\" + str(h2) + \"_h3_\" + str(h3) + \"_params.pck\"\n",
    "            with open(parampath, 'wb') as p:\n",
    "                pickle.dump(params, p)\n",
    "            shcmd = \" \".join([call, net, parampath, train_data_path, test_data_path, model_name])\n",
    "            train_test_file.write(shcmd + \";\\n\")\n",
    "        \n",
    "train_test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write parameter objects to pck files\n",
    "\n",
    "call = \"python /home/chase/projects/peakload/src/python/train.py\"\n",
    "net =  \"/home/chase/projects/peakload/src/python/nets/tanh_hidden/tanh_hidden.py\"\n",
    "#paramfile = \"/home/chase/projects/peakload/src/python/nets/single_hidden/EWloss_h1_250_params.pck\"\n",
    "train_data_path = \"/home/chase/projects/peakload/data/ercot/weather_pairs/training\"\n",
    "test_data_path = \"/home/chase/projects/peakload/data/ercot/weather_pairs/test\"\n",
    "model_name = \"tanh_sigmoid_weather\"\n",
    "\n",
    "epochs = 100\n",
    "lookback = 168 #one week in hours\n",
    "losses = [\"EW\", \"L1\"]\n",
    "hidden_1 = [250, 500, 1000, 1500, 2500, 5000, 7500, 10000, 20000]\n",
    "hidden_2 = [250, 500, 1000, 1500, 2500, 5000, 7500]\n",
    "hidden_3 = [50, 100, 250, 500, 1000, 1500, 2500, 5000]\n",
    "\n",
    "train_test_file = open(\"/home/chase/projects/peakload/src/python/nets/tanh_hidden/train_and_test.sh\", 'w')\n",
    "\n",
    "for loss in losses:\n",
    "    for i in range(len(hidden_2)):\n",
    "        for j in range(i, len(hidden_1)):\n",
    "            h1 = hidden_1[j]\n",
    "            h2 = hidden_2[i]\n",
    "            h3 = hidden_3[i]\n",
    "            params = {'LOOKBACK': lookback, 'HIDDEN_1': h1, 'HIDDEN_2': h2, 'HIDDEN_3': h3, 'LOSS': loss, 'EPOCHS': epochs, 'DATA': 'directory'}\n",
    "            parampath = \"/home/chase/projects/peakload/src/python/nets/tanh_hidden/\" + loss + \"loss_h1_\" + str(h1) + \"_h2_\" + str(h2) + \"_h3_\" + str(h3) + \"_params.pck\"\n",
    "            with open(parampath, 'wb') as p:\n",
    "                pickle.dump(params, p)\n",
    "            shcmd = \" \".join([call, net, parampath, train_data_path, test_data_path, model_name])\n",
    "            train_test_file.write(shcmd + \";\\n\")\n",
    "        \n",
    "train_test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write parameter objects to pck files\n",
    "\n",
    "call = \"python /home/chase/projects/peakload/src/python/train.py\"\n",
    "net =  \"/home/chase/projects/peakload/src/python/nets/tanh_hidden/tanh_hidden.py\"\n",
    "#paramfile = \"/home/chase/projects/peakload/src/python/nets/single_hidden/EWloss_h1_250_params.pck\"\n",
    "train_data_path = \"/home/chase/projects/peakload/data/ercot/weather_pairs/training\"\n",
    "test_data_path = \"/home/chase/projects/peakload/data/ercot/weather_pairs/test\"\n",
    "model_name = \"tanh_sigmoid_weather\"\n",
    "\n",
    "epochs = 100\n",
    "lookback = 168 #one week in hours\n",
    "losses = [\"EW\", \"L1\"]\n",
    "hidden_1 = [250, 500, 1000, 1500, 2500, 5000, 7500, 10000, 20000]\n",
    "hidden_2 = [250, 500, 1000, 1500, 2500, 5000, 7500]\n",
    "hidden_3 = [50, 100, 250, 500, 1000, 1500, 2500, 5000]\n",
    "\n",
    "train_test_file = open(\"/home/chase/projects/peakload/src/python/nets/tanh_hidden/train_and_test.sh\", 'w')\n",
    "\n",
    "for loss in losses:\n",
    "    for i in range(len(hidden_2)):\n",
    "        for j in range(i, len(hidden_1)):\n",
    "            h1 = hidden_1[j]\n",
    "            h2 = hidden_2[i]\n",
    "            h3 = hidden_3[i]\n",
    "            params = {'LOOKBACK': lookback, 'HIDDEN_1': h1, 'HIDDEN_2': h2, 'HIDDEN_3': h3, 'LOSS': loss, 'EPOCHS': epochs, 'DATA': 'directory'}\n",
    "            parampath = \"/home/chase/projects/peakload/src/python/nets/tanh_hidden/\" + loss + \"loss_h1_\" + str(h1) + \"_h2_\" + str(h2) + \"_h3_\" + str(h3) + \"_params.pck\"\n",
    "            with open(parampath, 'wb') as p:\n",
    "                pickle.dump(params, p)\n",
    "            shcmd = \" \".join([call, net, parampath, train_data_path, test_data_path, model_name])\n",
    "            train_test_file.write(shcmd + \";\\n\")\n",
    "        \n",
    "train_test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write parameter objects to pck files\n",
    "\n",
    "model_name = \"tanh_dropout\"\n",
    "call = \"python /home/chase/projects/peakload/src/python/train.py\"\n",
    "net =  \"/home/chase/projects/peakload/src/python/nets/\" + model_name + \"/\" + model_name + \".py\"\n",
    "#paramfile = \"/home/chase/projects/peakload/src/python/nets/single_hidden/EWloss_h1_250_params.pck\"\n",
    "train_data_path = \"/home/chase/projects/peakload/data/ercot/weather_pairs/training\"\n",
    "test_data_path = \"/home/chase/projects/peakload/data/ercot/weather_pairs/test\"\n",
    "\n",
    "epochs = 100\n",
    "lookback = 168 #one week in hours\n",
    "losses = [\"EW\", \"L1\"]\n",
    "hidden_1 = [1500, 2500, 5000, 7500, 10000, 20000]\n",
    "hidden_2 = [250, 1000, 2500, 5000]\n",
    "hidden_3 = [25, 100, 200, 250]\n",
    "\n",
    "train_test_file = open(\"/home/chase/projects/peakload/src/python/nets/\" + model_name + \"/train_and_test.sh\", 'w')\n",
    "\n",
    "for loss in losses:\n",
    "    for i in range(len(hidden_2)):\n",
    "        for j in range(i, len(hidden_1)):\n",
    "            h1 = hidden_1[j]\n",
    "            h2 = hidden_2[i]\n",
    "            h3 = hidden_3[i]\n",
    "            params = {'LOOKBACK': lookback, 'HIDDEN_1': h1, 'HIDDEN_2': h2, 'HIDDEN_3': h3, 'LOSS': loss, 'EPOCHS': epochs, 'DATA': 'directory'}\n",
    "            parampath = \"/home/chase/projects/peakload/src/python/nets/\" + model_name + \"/\" + loss + \"loss_h1_\" + str(h1) + \"_h2_\" + str(h2) + \"_h3_\" + str(h3) + \"_params.pck\"\n",
    "            with open(parampath, 'wb') as p:\n",
    "                pickle.dump(params, p)\n",
    "            shcmd = \" \".join([call, net, parampath, train_data_path, test_data_path, model_name])\n",
    "            train_test_file.write(shcmd + \";\\n\")\n",
    "        \n",
    "train_test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write parameter objects to pck files\n",
    "\n",
    "model_name = \"tanh_deep\"\n",
    "call = \"python /home/chase/projects/peakload/src/python/train.py\"\n",
    "net =  \"/home/chase/projects/peakload/src/python/nets/\" + model_name + \"/\" + model_name + \".py\"\n",
    "#paramfile = \"/home/chase/projects/peakload/src/python/nets/single_hidden/EWloss_h1_250_params.pck\"\n",
    "train_data_path = \"/home/chase/projects/peakload/data/ercot/weather_pairs/training\"\n",
    "test_data_path = \"/home/chase/projects/peakload/data/ercot/weather_pairs/test\"\n",
    "\n",
    "epochs = 100\n",
    "lookback = 168 #one week in hours\n",
    "losses = [\"EW\", \"L1\"]\n",
    "hidden_1 = [1500, 2500, 5000, 7500, 10000, 20000]\n",
    "hidden_2 = [1000, 1500, 2500, 5000, 7500]\n",
    "hidden_3 = [50, 500, 1000]\n",
    "hidden_4 = [50, 500, 1000]\n",
    "hidden_5 = [50, 500, 1000]\n",
    "hidden_6 = [50, 100]\n",
    "\n",
    "train_test_file = open(\"/home/chase/projects/peakload/src/python/nets/\" + model_name + \"/train_and_test.sh\", 'w')\n",
    "\n",
    "for loss in losses:\n",
    "    for i in range(len(hidden_2)):\n",
    "        for j in range(i, len(hidden_1)):\n",
    "            h1 = hidden_1[j]\n",
    "            h2 = hidden_2[i]\n",
    "            h3 = hidden_3[i]\n",
    "            h4 = hidden_4[i]\n",
    "            h5 = hidden_5[i]\n",
    "            h6 = hidden_6[i]\n",
    "            params = {'LOOKBACK': lookback, 'HIDDEN_1': h1, 'HIDDEN_2': h2, 'HIDDEN_3': h3, 'HIDDEN_4': h4, 'HIDDEN_5': h5, 'HIDDEN_6': h6, 'LOSS': loss, 'EPOCHS': epochs, 'DATA': 'directory'}\n",
    "            parampath = \"/home/chase/projects/peakload/src/python/nets/\" + model_name + \"/\" + loss + \"loss_h1_\" + str(h1) + \"_h2_\" + str(h2) + \"_h3_\" + str(h3) + \"_h4_\" + str(h4) + \"_h5_\" + str(h5) + \"_h6_\" + str(h6) + \"_params.pck\"\n",
    "            with open(parampath, 'wb') as p:\n",
    "                pickle.dump(params, p)\n",
    "            shcmd = \" \".join([call, net, parampath, train_data_path, test_data_path, model_name])\n",
    "            train_test_file.write(shcmd + \";\\n\")\n",
    "        \n",
    "train_test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write parameter objects to pck files\n",
    "\n",
    "model_name = \"tanh_deep_dropout\"\n",
    "call = \"python /home/chase/projects/peakload/src/python/train.py\"\n",
    "net =  \"/home/chase/projects/peakload/src/python/nets/\" + model_name + \"/\" + model_name + \".py\"\n",
    "#paramfile = \"/home/chase/projects/peakload/src/python/nets/single_hidden/EWloss_h1_250_params.pck\"\n",
    "train_data_path = \"/home/chase/projects/peakload/data/ercot/weather_pairs/training\"\n",
    "test_data_path = \"/home/chase/projects/peakload/data/ercot/weather_pairs/test\"\n",
    "\n",
    "epochs = 100\n",
    "lookback = 168 #one week in hours\n",
    "losses = [\"EW\", \"L1\"]\n",
    "hidden_1 = [1500, 2500, 5000, 7500, 10000, 20000]\n",
    "hidden_2 = [1000, 1500, 2500, 5000, 7500]\n",
    "hidden_3 = [50, 500, 1000]\n",
    "hidden_4 = [50, 500, 1000]\n",
    "hidden_5 = [50, 500, 1000]\n",
    "hidden_6 = [50, 100]\n",
    "\n",
    "train_test_file = open(\"/home/chase/projects/peakload/src/python/nets/\" + model_name + \"/train_and_test.sh\", 'w')\n",
    "\n",
    "for loss in losses:\n",
    "    for i in range(len(hidden_2)):\n",
    "        for j in range(i, len(hidden_1)):\n",
    "            h1 = hidden_1[j]\n",
    "            h2 = hidden_2[i]\n",
    "            h3 = hidden_3[i]\n",
    "            h4 = hidden_4[i]\n",
    "            h5 = hidden_5[i]\n",
    "            h6 = hidden_6[i]\n",
    "            params = {'LOOKBACK': lookback, 'HIDDEN_1': h1, 'HIDDEN_2': h2, 'HIDDEN_3': h3, 'HIDDEN_4': h4, 'HIDDEN_5': h5, 'HIDDEN_6': h6, 'LOSS': loss, 'EPOCHS': epochs, 'DATA': 'directory'}\n",
    "            parampath = \"/home/chase/projects/peakload/src/python/nets/\" + model_name + \"/\" + loss + \"loss_h1_\" + str(h1) + \"_h2_\" + str(h2) + \"_h3_\" + str(h3) + \"_h4_\" + str(h4) + \"_h5_\" + str(h5) + \"_h6_\" + str(h6) + \"_params.pck\"\n",
    "            with open(parampath, 'wb') as p:\n",
    "                pickle.dump(params, p)\n",
    "            shcmd = \" \".join([call, net, parampath, train_data_path, test_data_path, model_name])\n",
    "            train_test_file.write(shcmd + \";\\n\")\n",
    "        \n",
    "train_test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write a bash file to train all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class l_linear_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(linear_net, self).__init__()\n",
    "        self.D_in = lookback * 28\n",
    "        self.H1 = 250\n",
    "        self.H2 = 150\n",
    "        self.H3 = 50\n",
    "        self.D_out = 24\n",
    "        self.l1 = nn.Linear(self.D_in, self.H1)\n",
    "        self.l2 = nn.Linear(self.H1, self.H2)\n",
    "        self.l3 = nn.Linear(self.H2, self.H3)\n",
    "        self.l4 = nn.Linear(self.H3, self.D_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.l1(F.dropout(x)))\n",
    "        x = F.sigmoid(self.l2(x))\n",
    "        x = F.softmax(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_sample_list(datalist, batchsize):\n",
    "    remainder = len(datalist) % batchsize\n",
    "    diff = batchsize - remainder\n",
    "    tail = datalist[-diff:] + datalist[0:remainder]\n",
    "    out = [ datalist[i*batchsize:(i+1)*batchsize] for i in range(int(float(len(datalist))/float(batchsize)))]\n",
    "    out = out + [tail]\n",
    "    return(out)\n",
    "\n",
    "def torch_reshape_data(databatch):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for sample in databatch:\n",
    "        inputs.append(sample[0].flatten())\n",
    "        labels.append(sample[1].flatten())\n",
    "    return(torch.Tensor(np.asarray(inputs)), torch.Tensor(np.asarray(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(np.array([1,2,3]) > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batched_model_score(net_obj, datalist, thresh=0.85, batchsize=100):\n",
    "    recs = []\n",
    "    precs = []\n",
    "    thresh_losses_low = []\n",
    "    thresh_losses_high = []\n",
    "    set_losses = []\n",
    "    \n",
    "    #batchdata\n",
    "    batches = batch_sample_list(datalist, batchsize)\n",
    "    \n",
    "    #do prediction on datalist\n",
    "    for i, data, in enumerate(batches):\n",
    "        inputs, labels = torch_reshape_data(data)\n",
    "        labels = Variable(labels.cuda())\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        outputs = net_obj(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        set_losses.append(loss.data[0])\n",
    "        \n",
    "        labels = labels.data.cpu().numpy()\n",
    "        outputs = outputs.data.cpu().numpy()\n",
    "        \n",
    "        for i in range(outputs.shape[0]):\n",
    "            l = labels[i,:]\n",
    "            o = outputs[i,:]\n",
    "            \n",
    "            if any(l > thresh):\n",
    "                rec = binary_rec(l, o, thresh)\n",
    "                recs.append(rec)\n",
    "            if any(o > thresh):\n",
    "                prec = binary_prec(l, o, thresh)\n",
    "                precs.append(prec)\n",
    "                \n",
    "            loss_low = thresh_loss(l, o, thresh)\n",
    "            loss_high = thresh_loss(l, o, 0.95)\n",
    "            \n",
    "            thresh_losses_low.append(np.mean(loss_low))\n",
    "            thresh_losses_high.append(np.mean(loss_high))\n",
    "    \n",
    "    if len(recs) < 1:\n",
    "        r = np.nan\n",
    "    else:\n",
    "        r = np.mean(recs)\n",
    "    if len(precs) < 1:\n",
    "        p = np.nan\n",
    "    else:\n",
    "        p = np.mean(precs)\n",
    "    if np.isnan(r) or np.isnan(p):\n",
    "        batch_f1 = np.nan\n",
    "    else:\n",
    "        batch_f1 = f1_score(r, p)\n",
    "    \n",
    "    low_thresh_mean_loss = np.nanmean(thresh_losses_low)\n",
    "    high_thresh_mean_loss = np.nanmean(thresh_losses_high)\n",
    "    mean_set_loss = np.mean(set_losses)\n",
    "    \n",
    "    ret = {\"batch_recall\": r, \"batch_precision\": p, \"batch_f1\": batch_f1, \"low_thresh_mean_loss\": low_thresh_mean_loss, \"high_thresh_mean_loss\": high_thresh_mean_loss, \"mean_set_loss\": mean_set_loss}          \n",
    "    return(ret)\n",
    "\n",
    "def print_model_scores(scores, name):\n",
    "    print(\"====\" + name + \"====\")\n",
    "    for k in scores:\n",
    "        print(k + \": \" + str(scores[k]))\n",
    "    print(\"=================\\n\")\n",
    "    \n",
    "    \n",
    "def binary_rec(true, pred, thresh=0.85):\n",
    "    true_pos = 0\n",
    "    pred_pos = 0\n",
    "    for i in range(len(true)):\n",
    "        val = true[i]\n",
    "        if val >= thresh:\n",
    "            true_pos += 1\n",
    "            pred_val = pred[i]\n",
    "            if pred_val >= thresh:\n",
    "                pred_pos += 1\n",
    "    if true_pos == 0:\n",
    "        return(0)\n",
    "    else:\n",
    "        return(float(pred_pos)/float(true_pos))\n",
    "    \n",
    "def binary_prec(true, pred, thresh=0.85):\n",
    "    pos = 0\n",
    "    true_pos = 0\n",
    "    for i in range(len(pred)):\n",
    "        val = pred[i]\n",
    "        if val >= thresh:\n",
    "            pos += 1\n",
    "            comp_val = true[i]\n",
    "            if comp_val >= thresh:\n",
    "                true_pos += 1\n",
    "    if pos == 0:\n",
    "        return(0)\n",
    "    else:\n",
    "        return(float(true_pos)/float(pos))\n",
    "\n",
    "def thresh_loss(true, pred, thresh=0.85):\n",
    "    out = []\n",
    "    for i in range(len(true)):\n",
    "        val = true[i]\n",
    "        if val >= thresh:\n",
    "            loss = np.abs(val - pred[i])\n",
    "            out.append(loss)\n",
    "    return(out)\n",
    "\n",
    "def f1_score(rec, prec):\n",
    "    return(2.0/((1/rec) + (1/prec)))\n",
    "\n",
    "def set_loss(net_obj, data):\n",
    "    inputs, labels = torch_reshape_data(data)\n",
    "    labels = Variable(labels.cuda())\n",
    "    outputs = net_obj(Variable(inputs.cuda()))\n",
    "    loss = criterion(outputs, labels)\n",
    "    return(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_score(net_obj, datalist, threshes=[0.85, 0.95, 0.99, 0.999], batchsize=100):\n",
    "    #model scoring is on L1 loss\n",
    "    recs = {}\n",
    "    precs = {}\n",
    "    thresh_losses = {}\n",
    "    for t in threshes:\n",
    "        thresh_losses[t] = []\n",
    "        recs[t] = []\n",
    "        precs[t] = []\n",
    "    set_losses = []\n",
    "    \n",
    "    #for also recording weighted loss function later\n",
    "    criterion_l1 = nn.L1Loss()\n",
    "    \n",
    "    #do prediction on datalist\n",
    "    inputs, labels = torch_reshape_data(datalist)\n",
    "    labels = Variable(labels.cuda())\n",
    "    inputs = Variable(inputs.cuda())\n",
    "    outputs = net_obj(inputs)\n",
    "    loss = criterion_l1(outputs, labels)\n",
    "    set_losses.append(loss.data[0])\n",
    "    \n",
    "    labels = labels.data.cpu().numpy()\n",
    "    outputs = outputs.data.cpu().numpy()\n",
    "    \n",
    "    for i in range(outputs.shape[0]):\n",
    "        l = labels[i,:]\n",
    "        o = outputs[i,:]\n",
    "        \n",
    "        for t in threshes:\n",
    "            if any(l > t):\n",
    "                rec = binary_rec(l, o, t)\n",
    "                recs[t].append(rec)\n",
    "            if any(o > t):\n",
    "                prec = binary_prec(l, o, t)\n",
    "                precs[t].append(prec)\n",
    "  \n",
    "            tl = thresh_loss(l, o, t)\n",
    "            thresh_losses[t].append(np.nanmean(tl))\n",
    "    \n",
    "    for t in thresh_losses:\n",
    "        thresh_losses[t] = np.nanmean(thresh_losses[t])\n",
    "        if len(recs[t]) < 1:\n",
    "            recs[t] = np.nan\n",
    "        else:\n",
    "            recs[t] = np.nanmean(recs[t])\n",
    "        if len(precs[t]) < 1:\n",
    "            precs[t] = np.nan\n",
    "        else:\n",
    "            precs[t] = np.nanmean(precs[t])\n",
    "\n",
    "    mean_set_loss = np.nanmean(set_losses)\n",
    "    \n",
    "    ret = {\"batch_recall\": recs, \"batch_precision\": precs, \"thresh_losses\": thresh_losses, \"mean_set_loss\": mean_set_loss}          \n",
    "    return(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 4\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "torch.exp(torch.Tensor([0, math.log(2)])) * torch.exp(torch.Tensor([0, math.log(2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expw_mae_loss(input, target, base=10.0):\n",
    "    base = Variable(torch.Tensor([base])).type_as(target)\n",
    "    return(( (input - target).abs() * torch.pow(base, target) ).sum() / input.data.nelement() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net_obj, loss_fnc, opt_fnc, traindata, valdata, batchsize=100, epochs=50):  \n",
    "    train_batches = batch_sample_list(traindata, batchsize)\n",
    "    num_batches = len(train_batches)\n",
    "    tenth = int(num_batches/10)\n",
    "    \n",
    "    training_epoch_loss = []\n",
    "    train_epoch_set_loss = []\n",
    "    val_epoch_set_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_batches):\n",
    "            inputs, labels = torch_reshape_data(data)\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda()) #they need to be .cuda() with each training epoch\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net_obj(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "\n",
    "            #training loss\n",
    "            #print(\"epoch \" + str(epoch) + \" ,iter \" + str(i) + \": \" + str(running_loss))\n",
    "            #training_epoch_loss.append(running_loss)\n",
    "        \n",
    "        print(\"==epoch \" + str(epoch) + \"==\")\n",
    "        train_samp = random.randint(0,num_batches-1)\n",
    "        train_set_loss = set_loss(net_obj, train_batches[train_samp])\n",
    "        train_epoch_set_loss.append(train_set_loss)\n",
    "        print(\"training set sampled loss: \" + str(train_set_loss))\n",
    "        val_set_loss = set_loss(net_obj, valdata)\n",
    "        val_epoch_set_loss.append(val_set_loss)\n",
    "        print(\"validation set loss: \" + str(val_set_loss))\n",
    "       \n",
    "    #sampled epoch set loss\n",
    "    print(\"\\n\")\n",
    "    print(\"Model results\")\n",
    "    \n",
    "    train_model_scores = batched_model_score(net_obj, traindata)\n",
    "    print_model_scores(train_model_scores, \"training data scores\")\n",
    "    \n",
    "    val_model_scores = batched_model_score(net_obj, valdata)\n",
    "    print_model_scores(val_model_scores, \"validation data scores\")\n",
    "                \n",
    "    return(training_epoch_loss, train_epoch_set_loss, val_epoch_set_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#switch nans to -1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100 #batch size\n",
    "\n",
    "net = linear_net().cuda()\n",
    "\n",
    "criterion = expw_mae_loss #nn.L1Loss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==epoch 0==\n",
      "training set sampled loss: 0.8030710816383362\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, test_accs = train(net, criterion, optimizer, training_pairs, val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chase/applications/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:39: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====test data scores====\n",
      "batch_recall: {0.85: 0.8184080665931602, 0.95: 0.9420365780671124, 0.99: 0.9969432314410481, 0.999: 1.0}\n",
      "batch_precision: {0.85: 0.8928108353067051, 0.95: 0.6117134316069153, 0.99: 0.15997180162558833, 0.999: 0.007426426426426425}\n",
      "thresh_losses: {0.85: 0.042240818868496385, 0.95: 0.05805267103670435, 0.99: 0.08449693017515857, 0.999: 0.11889468113037005}\n",
      "mean_set_loss: 0.1052764281630516\n",
      "=================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model_scores = model_score(net, weather_normed_test_data_pairs)\n",
    "print_model_scores(test_model_scores, \"test data scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l1_loss(input, target, size_average=True):\n",
    "    return _functions.thnn.L1Loss.apply(input, target, size_average)ls\n",
    "\n",
    "\n",
    "class L1Loss(_Loss):\n",
    "    r\"\"\"Creates a criterion that measures the mean absolute value of the\n",
    "    element-wise difference between input `x` and target `y`:\n",
    "\n",
    "    :math:`{loss}(x, y)  = 1/n \\sum |x_i - y_i|`\n",
    "\n",
    "    `x` and `y` arbitrary shapes with a total of `n` elements each.\n",
    "\n",
    "    The sum operation still operates over all the elements, and divides by `n`.\n",
    "\n",
    "    The division by `n` can be avoided if one sets the constructor argument\n",
    "    `size_average=False`.\n",
    "\n",
    "    Args:\n",
    "        size_average (bool, optional): By default, the losses are averaged\n",
    "           over observations for each minibatch. However, if the field\n",
    "           size_average is set to False, the losses are instead summed for\n",
    "           each minibatch. Default: True\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *)` where `*` means, any number of additional\n",
    "          dimensions\n",
    "        - Target: :math:`(N, *)`, same shape as the input\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> loss = nn.L1Loss()\n",
    "        >>> input = autograd.Variable(torch.randn(3, 5), requires_grad=True)\n",
    "        >>> target = autograd.Variable(torch.randn(3, 5))\n",
    "        >>> output = loss(input, target)\n",
    "        >>> output.backward()\n",
    "    \"\"\"\n",
    "    def forward(self, input, target):\n",
    "        _assert_no_grad(target)\n",
    "        return F.l1_loss(input, target, size_average=self.size_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22591324,  0.28630137,  0.3872146 ,  0.45502284,  0.48664382,\n",
       "        0.47203195,  0.43481734,  0.38972601,  0.36004567,  0.34406394,\n",
       "        0.33755708,  0.36883563,  0.43584475,  0.45285389,  0.40958905,\n",
       "        0.3303653 ,  0.23504566,  0.16575342,  0.11312786,  0.08070777,\n",
       "        0.0609589 ,  0.0553653 ,  0.05947489,  0.08413242], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.27877405,  0.29841518,  0.31782278,  0.3170574 ,  0.30463257,\n",
       "        0.30868047,  0.3124792 ,  0.3071259 ,  0.31804854,  0.31793466,\n",
       "        0.31255594,  0.30287418,  0.30706862,  0.27466702,  0.26765308,\n",
       "        0.23752765,  0.21457636,  0.19224998,  0.19455633,  0.19908968,\n",
       "        0.2263395 ,  0.23829129,  0.25208017,  0.26730546], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs torch.Size([3, 5, 7])\n",
      "hidden torch.Size([2, 5, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.c1 = nn.Conv1d(input_size, hidden_size, 2)\n",
    "        self.p1 = nn.AvgPool1d(2)\n",
    "        self.c2 = nn.Conv1d(hidden_size, hidden_size, 1)\n",
    "        self.p2 = nn.AvgPool1d(2)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=0.01)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        batch_size = inputs.size(1)\n",
    "        \n",
    "        # Turn (seq_len x batch_size x input_size) into (batch_size x input_size x seq_len) for CNN\n",
    "        inputs = inputs.transpose(0, 1).transpose(1, 2)\n",
    "\n",
    "        # Run through Conv1d and Pool1d layers\n",
    "        c = self.c1(inputs)\n",
    "        p = self.p1(c)\n",
    "        c = self.c2(p)\n",
    "        p = self.p2(c)\n",
    "\n",
    "        # Turn (batch_size x hidden_size x seq_len) back into (seq_len x batch_size x hidden_size) for RNN\n",
    "        p = p.transpose(1, 2).transpose(0, 1)\n",
    "        \n",
    "        p = F.tanh(p)\n",
    "        output, hidden = self.gru(p, hidden)\n",
    "        conv_seq_len = output.size(0)\n",
    "        output = output.view(conv_seq_len * batch_size, self.hidden_size) # Treating (conv_seq_len x batch_size) as batch_size for linear layer\n",
    "        output = F.tanh(self.out(output))\n",
    "        output = output.view(conv_seq_len, -1, self.output_size)\n",
    "        return output, hidden\n",
    "\n",
    "input_size = 20\n",
    "hidden_size = 50\n",
    "output_size = 7\n",
    "batch_size = 5\n",
    "n_layers = 2\n",
    "seq_len = 15\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, output_size, n_layers=n_layers)\n",
    "\n",
    "inputs = Variable(torch.rand(seq_len, batch_size, input_size)) # seq_len x batch_size x \n",
    "outputs, hidden = rnn(inputs, None)\n",
    "print('outputs', outputs.size()) # conv_seq_len x batch_size x output_size\n",
    "print('hidden', hidden.size()) # n_layers x batch_size x hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 5, 20])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
