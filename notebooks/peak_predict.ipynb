{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use the Q2 2016 data first\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.inp = nn.Linear(5, hidden_size)\n",
    "        #self.inp = nn.Linear(1, hidden_size)\n",
    "        #self.rnn = nn.RNN(hidden_size, hidden_size, nonlinearity='tanh')\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def step(self, input, hidden=None):\n",
    "        input = self.inp(input.view(1, 5)).unsqueeze(1) #-1 means unsure of number of rows\n",
    "        #input = self.inp(input.view(1,-1)).unsqueeze(1)\n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        output = self.out(output.squeeze(1))\n",
    "        return output, hidden\n",
    "    \n",
    "    def forward(self, inputs, hidden=None, force=True, steps=0):\n",
    "        steps = len(inputs)\n",
    "        outputs = Variable(torch.zeros(steps, 1, 1)) #.cuda()\n",
    "        for i in range(steps):\n",
    "            input = inputs[i,:]\n",
    "            #input = inputs[i]\n",
    "            output, hidden = self.step(input, hidden)\n",
    "            outputs[i] = output\n",
    "        return outputs, hidden\n",
    "\n",
    "    \"\"\"\n",
    "    def forward(self, inputs, hidden=None, force=True, steps=0):\n",
    "        if force or steps == 0: steps = len(inputs)\n",
    "        outputs = Variable(torch.zeros(steps, 1, 1))\n",
    "        for i in range(steps):\n",
    "            if force or i == 0:\n",
    "                input = inputs[i,:]\n",
    "            else:\n",
    "                input = output\n",
    "            output, hidden = self.step(input, hidden)\n",
    "            outputs[i] = output\n",
    "        return outputs, hidden\n",
    "    \"\"\"\n",
    "\n",
    "n_epochs = 500\n",
    "n_iters = 50\n",
    "hidden_size = 30*24\n",
    "\n",
    "model = SimpleRNN(hidden_size) #.cuda()\n",
    "#criterion = nn.SmoothL1Loss()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "losses = np.zeros(n_epochs) # For plotting\n",
    "val_errs = np.zeros(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for iter in range(n_iters):\n",
    "        #_inputs = sample(50)\n",
    "        inputs = torch.from_numpy(ex_train[:-1,:]).float()\n",
    "        inputs = Variable(inputs) #cuda()\n",
    "        targets = torch.from_numpy(ex_train[1:,0]).float()\n",
    "        targets = Variable(targets) #.cuda()\n",
    "\n",
    "        # Use teacher forcing 50% of the time\n",
    "        force = np.random.random() < 0.1\n",
    "        outputs, hidden = model(inputs, None, force)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses[epoch] += loss.data[0]\n",
    "        \n",
    "        #validation error\n",
    "        input_val = ex_val[:-1,:]\n",
    "        target_val = ex_val[1:,0]\n",
    "        val_inputs = Variable(torch.from_numpy(input_val)).float() #.cuda()\n",
    "        val_targets = Variable(torch.from_numpy(target_val)).float() #.cuda()\n",
    "        val_outputs, hidden = model(val_inputs, hidden)\n",
    "        val_loss = criterion(val_outputs, val_targets)\n",
    "        \n",
    "        val_errs[epoch] += val_loss.data[0]\n",
    "\n",
    "    if epoch > 0:\n",
    "        print(epoch, \"Training loss: \" + str(loss.data[0]))\n",
    "        \n",
    "        #train error\n",
    "        print(epoch, \"Val loss: \" + str(val_loss.data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
