{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import urllib3\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate lattice of points to sample within the state\n",
    "border = []\n",
    "max_lat = -90.0 #set to entire globe\n",
    "min_lat = 90.0\n",
    "max_lon = -180.0\n",
    "min_lon = 180.0\n",
    "with open(\"/home/chase/projects/peakload/data/stateborders/texasborder.txt\", 'r') as d:\n",
    "    lines = d.readlines()\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split(\",\")\n",
    "        lon = float(tokens[0])\n",
    "        lat = float(tokens[1])\n",
    "        border.append((lat, lon))\n",
    "        if lat > max_lat:\n",
    "            max_lat = lat\n",
    "        if lat < min_lat:\n",
    "            min_lat = lat\n",
    "        if lon > max_lon:\n",
    "            max_lon = lon\n",
    "        if lon < min_lon:\n",
    "            min_lon = lon\n",
    "            \n",
    "bounding_box = [max_lat, min_lat, max_lon, min_lon]\n",
    "\n",
    "row_len = int(np.sqrt(grid_size))\n",
    "lambdas = np.arange(0,1+(1/float(row_len)),1/float(row_len))\n",
    "grid = np.zeros((row_len+1,row_len+1,2))\n",
    "row_starts = [ (1 - i)*min_lon + i*max_lon for i in lambdas ]\n",
    "col_starts = [ (1 - i)*min_lat + i*max_lat for i in lambdas ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(grid.shape[0]):\n",
    "    for j in range(grid.shape[1]):\n",
    "        grid[i,j,0] = row_starts[i]\n",
    "        grid[i,j,1] = col_starts[j] #reversing to read from north to south"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmap = gmplot.GoogleMapPlotter(31.9686, -99.9018, 3)\n",
    "\n",
    "lats = []\n",
    "lons = []\n",
    "for i in range(grid.shape[0]):\n",
    "    for j in range(grid.shape[1]):\n",
    "        lats.append(grid[i,j,1])\n",
    "        lons.append(grid[i,j,0])\n",
    "\n",
    "#gmap.scatter(lats, lons, '#3B0B39', size=40, marker=True)\n",
    "for i in range(len(lats)):\n",
    "    gmap.marker(lats[i], lons[i])\n",
    "\n",
    "gmap.draw(\"/home/chase/projects/peakload/figs/texas_grid25.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "startdate = datetime.datetime(year=2010, month=1, day=1, hour=0, minute=0, second=0)\n",
    "curr = startdate\n",
    "#enddate = datetime.datetime(year=2011, month=1, day=1, hour=0, minute=0, second=0)\n",
    "enddate = datetime.datetime(year=2018, month=3, day=31, hour=23, minute=0, second=0)\n",
    "\n",
    "alldays = []\n",
    "\n",
    "while curr < enddate:\n",
    "    cunix = int(time.mktime(curr.timetuple()))\n",
    "    alldays.append(cunix)\n",
    "    curr += datetime.timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13140"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36 * 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data request files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#want to make files with batch requests, per day with all grid point\n",
    "basepath = \"/home/chase/projects/peakload/data/weather/ercot/\"\n",
    "reqpath = basepath + \"requests/\"\n",
    "with open(\"/home/chase/api_keys/darksky_key.txt\", 'r') as d:\n",
    "    pkey = d.readline().strip()\n",
    "baseurl = \"https://api.darksky.net/forecast/\" + \"/\"\n",
    "http = urllib3.PoolManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_reqs = []\n",
    "for day in alldays:\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            lat = grid[i,j,0]\n",
    "            lon = grid[i,j,1]\n",
    "            requrl = baseurl + str(lon) + \",\" + str(lat) +  \",\" + str(day) + \"?exclude=flags\"\n",
    "            all_reqs.append(requrl)\n",
    "            \n",
    "with open(reqpath + \"/grid\" + str(grid_size) + \"_all.txt\", 'w') as d:\n",
    "    for r in all_reqs:\n",
    "        d.write(r + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-6ac30a9ff011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcureq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mlat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mlon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mposix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#need to do this on the desktop\n",
    "\n",
    "with open(reqpath + \"grid25_all.txt\", 'r') as d:\n",
    "    lines = d.readlines()\n",
    "    for line in lines:\n",
    "        cureq = d.readline().strip()\n",
    "        tokens = cureq.split(\"/\")\n",
    "        tokens = tokens[-1].split(\",\")\n",
    "        lat = tokens[1]\n",
    "        lon = tokens[0]\n",
    "        posix = int(tokens[2].split(\"?\")[0])\n",
    "        year = str(datetime.datetime.fromtimestamp(posix).year)\n",
    "        outpath = basepath + \"/grid\" + str(grid_size) + \"/\" + year + \"/\" + str(posix)\n",
    "        if not os.path.exists(outpath):\n",
    "            os.mkdir(outpath)\n",
    "        response = http.request('GET', cureq)\n",
    "        with open(outpath + \"/grid_\" + str(lat) + \",\" + str(lon) + \".json\", 'w') as f:\n",
    "            payload = json.loads(response.data)\n",
    "            out = json.dumps(payload)\n",
    "            f.write(out)\n",
    "        time.sleep(1)\n",
    "        print(posix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#request weather by cities\n",
    "cities = {\"Witchita Falls\": (33.9137, -98.4934), \n",
    "          \"Mineral Wells\": (32.8085, -98.1128), \n",
    "          \"Paris\": (33.6609, -95.5555), \n",
    "          \"Dallas\": (32.7767, -96.7970), \n",
    "          \"Fort Worth\": (32.7555, -97.3308), \n",
    "          \"Tyler\": (32.3513, -95.3011),  \n",
    "          \"Waco\": (31.5493, -97.1467), \n",
    "          \"San Angelo\": (31.4638, -100.4370), \n",
    "          \"Abilene\": (32.4487, -99.7331), \n",
    "          \"Midland\": (31.9973, -102.0779), \n",
    "          \"Wink\": (31.7512, -103.1599), \n",
    "          \"Junction\": (30.4894, -99.7720), \n",
    "          \"Austin\": (30.2672, -97.7431), \n",
    "          \"Houston\": (29.7604, -95.3698), \n",
    "          \"Galveston\": (29.3013, -94.7977), \n",
    "          \"Victoria\": (28.8053, -97.0036), \n",
    "          \"Corpus Christi\": (27.8006, -97.39640), \n",
    "          \"Laredo\": (27.5306, -99.4803),\n",
    "          \"San Antonio\": (29.4241, -98.4936)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_reqs = []\n",
    "for day in alldays:\n",
    "    for city in cities.keys():\n",
    "        lat = cities[city][0]\n",
    "        lon = cities[city][1]\n",
    "        requrl = baseurl + str(lat) + \",\" + str(lon) +  \",\" + str(day) + \"?exclude=flags\"\n",
    "        all_reqs.append(requrl)\n",
    "            \n",
    "with open(reqpath + \"/major_cities.txt\", 'w') as d:\n",
    "    for r in all_reqs:\n",
    "        d.write(r + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-f0f3613e3eb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcureq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mlat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mlon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mposix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "with open(reqpath + \"major_cities.txt\", 'r') as d:\n",
    "    lines = d.readlines()\n",
    "    for line in lines:\n",
    "        cureq = d.readline().strip()\n",
    "        tokens = cureq.split(\"/\")\n",
    "        tokens = tokens[-1].split(\",\")\n",
    "        lat = tokens[1]\n",
    "        lon = tokens[0]\n",
    "        posix = int(tokens[2].split(\"?\")[0])\n",
    "        year = str(datetime.datetime.fromtimestamp(posix).year)\n",
    "        outpath = basepath + \"/cities/\" + year + \"/\" + str(posix)\n",
    "        if not os.path.exists(outpath):\n",
    "            os.mkdir(outpath)\n",
    "        response = http.request('GET', cureq)\n",
    "        with open(outpath + \"/grid_\" + str(lat) + \",\" + str(lon) + \".json\", 'w') as f:\n",
    "            payload = json.loads(response.data)\n",
    "            out = json.dumps(payload)\n",
    "            f.write(out)\n",
    "        time.sleep(1)\n",
    "        print(posix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#process weather features\n",
    "#need a vector for each hour of 2011-2017 across all latitude-longitudes, one big array for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "startdate = datetime.datetime(year=2010, month=1, day=1, hour=0, minute=0, second=0)\n",
    "curr = startdate\n",
    "enddate = datetime.datetime(year=2018, month=1, day=1, hour=0, minute=0, second=0)\n",
    "\n",
    "alldays = []\n",
    "\n",
    "while curr < enddate:\n",
    "    cunix = int(time.mktime(curr.timetuple()))\n",
    "    alldays.append(cunix)\n",
    "    curr += datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "startdate = datetime.datetime(year=2010, month=1, day=1, hour=0, minute=0, second=0)\n",
    "curr = startdate\n",
    "enddate = datetime.datetime(year=2018, month=1, day=1, hour=0, minute=0, second=0)\n",
    "\n",
    "allhours = []\n",
    "\n",
    "while curr < enddate:\n",
    "    cunix = int(time.mktime(curr.timetuple()))\n",
    "    allhours.append(cunix)\n",
    "    curr += datetime.timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#what features are available to me?\n",
    "#weatherdir = \"/home/chase/projects/peakload/data/weather/ercot/grid25/\"\n",
    "weatherdir = \"/home/chase/projects/peakload/data/weather/ercot/major_cities/\"\n",
    "\n",
    "outdir = weatherdir + \"array_formatted/\"\n",
    "years = range(2010,2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tkeys = ['currently', 'daily', 'hourly', 'latitude', 'longitude']\n",
    "\n",
    "#files names are grid_longitude,latitude.json\n",
    "def parse_filename(fnamestr):\n",
    "    token = fnamestr.split(\"_\")[1]\n",
    "    token_ = token[:-5] #break off .json extension\n",
    "    tokens = token_.split(\",\")\n",
    "    #return (lat, long)\n",
    "    return((float(tokens[1]), float(tokens[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'daily'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-cfc097362fe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'daily'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'hourly'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                     \u001b[0mcurr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#list of dicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'daily'"
     ]
    }
   ],
   "source": [
    "#want to scan ahead to get all keys and use map for missing data points\n",
    "#dont' need k + i + lk, just want lk's for each hour\n",
    "lkeys = {'daily': set(), 'hourly': set()}\n",
    "\n",
    "grid_locs = set()\n",
    "\n",
    "for year in years:\n",
    "    days = os.listdir(weatherdir + str(year))\n",
    "    for day in days:\n",
    "        wms = os.listdir(weatherdir + str(year) + \"/\" + day)\n",
    "        for fname in wms:\n",
    "            loc = parse_filename(fname)\n",
    "            sloc = str(loc[0]) + \",\" + str(loc[1])\n",
    "            grid_locs.add(sloc)\n",
    "            with open(weatherdir + str(year) + \"/\" + day + \"/\" + fname, 'r') as d:\n",
    "                data = json.load(d)\n",
    "            for k in tkeys:\n",
    "                if k == 'daily' or k == 'hourly':\n",
    "                    curr = data[k]['data'] #list of dicts\n",
    "                    for di in range(len(curr)):\n",
    "                        d = curr[di]\n",
    "                        for lk in d:\n",
    "                            lkeys[k].add(lk)\n",
    "                #elif k == 'currently':\n",
    "                #    curr = data[k]\n",
    "                #    for lk in curr.keys():\n",
    "                #        lkeys[k].add(lk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 400, 'error': 'The given location is invalid.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_locs = sorted(list(grid_locs))\n",
    "with open(outdir + \"grid_locs.pck\", 'wb') as f:\n",
    "    pickle.dump(grid_locs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(outdir + \"grid_locs.pck\", 'rb') as f:\n",
    "    grid_locs = pickle.load(f)\n",
    "grid_locs = sorted(list(grid_locs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'daily': {'moonPhase': 0, 'apparentTemperatureLowTime': 1, 'apparentTemperatureMinTime': 2, 'pressure': 3, 'summary': 4, 'sunsetTime': 5, 'precipIntensity': 6, 'apparentTemperatureHighTime': 7, 'temperatureLowTime': 8, 'time': 9, 'temperatureHigh': 10, 'apparentTemperatureMax': 11, 'windBearing': 12, 'temperatureHighTime': 13, 'temperatureMax': 14, 'dewPoint': 15, 'precipAccumulation': 16, 'precipProbability': 17, 'windSpeed': 18, 'precipIntensityMaxTime': 19, 'precipIntensityMax': 20, 'icon': 21, 'cloudCover': 22, 'temperatureMin': 23, 'apparentTemperatureHigh': 24, 'apparentTemperatureLow': 25, 'temperatureMinTime': 26, 'temperatureMaxTime': 27, 'precipType': 28, 'sunriseTime': 29, 'visibility': 30, 'apparentTemperatureMaxTime': 31, 'apparentTemperatureMin': 32, 'temperatureLow': 33, 'humidity': 34}, 'hourly': {'temperature': 35, 'windSpeed': 36, 'windBearing': 37, 'pressure': 38, 'summary': 39, 'precipType': 40, 'dewPoint': 41, 'icon': 42, 'precipIntensity': 43, 'visibility': 44, 'precipProbability': 45, 'precipAccumulation': 46, 'apparentTemperature': 47, 'humidity': 48, 'cloudCover': 49, 'time': 50}}\n"
     ]
    }
   ],
   "source": [
    "#for each hour, create a vector with an element for each key in lkeys\n",
    "#assign an index for each element in lkeys\n",
    "\n",
    "lkeymap = {}\n",
    "i = 0\n",
    "for tk in lkeys:\n",
    "    lkeymap[tk] = {}\n",
    "    for lk in lkeys[tk]:\n",
    "        lkeymap[tk][lk] = i\n",
    "        i += 1\n",
    "\n",
    "lkeymax = i - 1\n",
    "\n",
    "print(lkeymap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lkeymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save lkeymap to \n",
    "with open(outdir + \"data_keys.pck\", 'wb') as d:\n",
    "    pickle.dump(lkeymap, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load lkeymap\n",
    "with open(outdir + \"data_keys.pck\", 'rb') as d:\n",
    "    lkeymap = pickle.load(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daily': {'apparentTemperatureHigh': 24,\n",
       "  'apparentTemperatureHighTime': 7,\n",
       "  'apparentTemperatureLow': 25,\n",
       "  'apparentTemperatureLowTime': 1,\n",
       "  'apparentTemperatureMax': 11,\n",
       "  'apparentTemperatureMaxTime': 31,\n",
       "  'apparentTemperatureMin': 32,\n",
       "  'apparentTemperatureMinTime': 2,\n",
       "  'cloudCover': 22,\n",
       "  'dewPoint': 15,\n",
       "  'humidity': 34,\n",
       "  'icon': 21,\n",
       "  'moonPhase': 0,\n",
       "  'precipAccumulation': 16,\n",
       "  'precipIntensity': 6,\n",
       "  'precipIntensityMax': 20,\n",
       "  'precipIntensityMaxTime': 19,\n",
       "  'precipProbability': 17,\n",
       "  'precipType': 28,\n",
       "  'pressure': 3,\n",
       "  'summary': 4,\n",
       "  'sunriseTime': 29,\n",
       "  'sunsetTime': 5,\n",
       "  'temperatureHigh': 10,\n",
       "  'temperatureHighTime': 13,\n",
       "  'temperatureLow': 33,\n",
       "  'temperatureLowTime': 8,\n",
       "  'temperatureMax': 14,\n",
       "  'temperatureMaxTime': 27,\n",
       "  'temperatureMin': 23,\n",
       "  'temperatureMinTime': 26,\n",
       "  'time': 9,\n",
       "  'visibility': 30,\n",
       "  'windBearing': 12,\n",
       "  'windSpeed': 18},\n",
       " 'hourly': {'apparentTemperature': 47,\n",
       "  'cloudCover': 49,\n",
       "  'dewPoint': 41,\n",
       "  'humidity': 48,\n",
       "  'icon': 42,\n",
       "  'precipAccumulation': 46,\n",
       "  'precipIntensity': 43,\n",
       "  'precipProbability': 45,\n",
       "  'precipType': 40,\n",
       "  'pressure': 38,\n",
       "  'summary': 39,\n",
       "  'temperature': 35,\n",
       "  'time': 50,\n",
       "  'visibility': 44,\n",
       "  'windBearing': 37,\n",
       "  'windSpeed': 36}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lkeymap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daily_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ad65b3f8ce00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mtime_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mtime_int\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdaily_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'daily_data' is not defined"
     ]
    }
   ],
   "source": [
    "#all_days_hourly_arrays = {}\n",
    "\n",
    "#the data is being delivered to me in PST\n",
    "\n",
    "for year in years:\n",
    "    days = os.listdir(weatherdir + str(year))\n",
    "    for day in days:\n",
    "        wms = os.listdir(weatherdir + str(year) + \"/\" + day)\n",
    "        for fname in wms:\n",
    "            loc = parse_filename(fname)\n",
    "            sloc = str(loc[0]) + \",\" + str(loc[1])\n",
    "            loc_i = grid_locs.index(sloc)\n",
    "            with open(weatherdir + str(year) + \"/\" + day + \"/\" + fname, 'r') as d:\n",
    "                data = json.load(d)\n",
    "\n",
    "            for k in tkeys:\n",
    "                if k == 'hourly':\n",
    "                    curr = data[k]['data'] #list of dicts\n",
    "                    \n",
    "                    #need to just pull out time stamps from data and start/stop accordingly\n",
    "                    for i in range(len(curr)):\n",
    "                        time_int = curr[i]['time']\n",
    "                        for key in curr[i]:\n",
    "                            if time_int not in hourly_data.keys():\n",
    "                                print(time_int)\n",
    "                                pass\n",
    "                            else:\n",
    "                                hourly_data[time_int][sloc][key] = curr[i][key]\n",
    "                                \n",
    "                elif k == 'daily':\n",
    "                    d = data[k]['data'][0]\n",
    "                    time_int = int(day)\n",
    "                    for key in d:\n",
    "                        if time_int not in daily_data.keys():\n",
    "                            print(time_int)\n",
    "                            pass\n",
    "                        else:\n",
    "                            daily_data[time_int][sloc][key] = d[key]\n",
    "        \n",
    "#all_days_hourly_arrays[day] = allhours_outvector_gridlocs\n",
    "with open(outdir + \"daily_weather_dict.pck\", 'wb') as f:\n",
    "    pickle.dump(daily_data, f)\n",
    "        \n",
    "#all_days_hourly_arrays[day] = allhours_outvector_gridlocs\n",
    "with open(outdir + \"hourly_weather_dict.pck\", 'wb') as f:\n",
    "    pickle.dump(hourly_data, f)\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pick up where left off\n",
    "#all_days_hourly_arrays[day] = allhours_outvector_gridlocs\n",
    "with open(outdir + \"daily_weather_dict.pck\", 'rb') as f:\n",
    "    daily_data = pickle.load(f)\n",
    "        \n",
    "#all_days_hourly_arrays[day] = allhours_outvector_gridlocs\n",
    "with open(outdir + \"hourly_weather_dict.pck\", 'rb') as f:\n",
    "    hourly_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(outdir + \"data_keys.pck\", 'rb') as f:\n",
    "    lkeymap = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lkeymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numdatapoints = lkeymax + 1\n",
    "\n",
    "data_key_values = {}\n",
    "\n",
    "for hour in hourly_data.keys():\n",
    "    data_arrays = {}\n",
    "    t = datetime.datetime.fromtimestamp(hour)\n",
    "    year = str(t.year)\n",
    "    write_path = outdir + \"yearly/\" + year\n",
    "    if not os.path.exists(write_path):\n",
    "        os.mkdir(write_path)\n",
    "    dayt = datetime.datetime(year=t.year, month=t.month, day=t.day, hour=0, minute=0, second=0)\n",
    "    dayint = int(time.mktime(dayt.timetuple()))\n",
    "    #get the day and corresponding day data\n",
    "    for l in grid_locs:\n",
    "        tokens = l.split(\",\")\n",
    "        lat = float(tokens[0])\n",
    "        lon = float(tokens[1])\n",
    "        data_arrays[l] = [ np.nan for i in range(numdatapoints) ]\n",
    "        for k in hourly_data[hour][l].keys():\n",
    "            data_arrays[l][lkeymap['hourly'][k]] = hourly_data[hour][l][k]\n",
    "            if year < \"2017\": #getting min/max normalization values for training data only\n",
    "                if k not in data_key_values.keys():\n",
    "                    data_key_values[k] = set( [hourly_data[hour][l][k]] )\n",
    "                else:\n",
    "                    data_key_values[k].add(hourly_data[hour][l][k]) \n",
    "        for k in daily_data[dayint][l].keys():\n",
    "            data_arrays[l][lkeymap['daily'][k]] = daily_data[dayint][l][k]\n",
    "            if year < \"2017\":\n",
    "                if k not in data_key_values.keys():\n",
    "                    data_key_values[k] = set( [daily_data[dayint][l][k]] )\n",
    "                else:\n",
    "                    data_key_values[k].add(daily_data[dayint][l][k])\n",
    "\n",
    "    \n",
    "    with open(write_path + \"/\" + str(hour) + \"_arrays.pck\", 'wb') as d:\n",
    "        pickle.dump(data_arrays, d)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in all written data arrays, find out what keys have nan values\n",
    "\n",
    "all_data_arrays = {}\n",
    "\n",
    "for year in years:\n",
    "    hourfs = os.listdir(outdir + \"yearly/\" + str(year))\n",
    "    for fname in hourfs:\n",
    "        pth = outdir + \"yearly/\" + str(year) + \"/\" + fname\n",
    "        with open(pth, 'rb') as d:\n",
    "            data = pickle.load(d)\n",
    "            all_data_arrays[fname] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nankeys = {}\n",
    "nankeys['daily'] = {}\n",
    "nankeys['hourly'] = {}\n",
    "for f in all_data_arrays.keys():\n",
    "    for l in grid_locs:\n",
    "        for i in range(len(all_data_arrays[f][l])):\n",
    "            val = all_data_arrays[f][l][i]\n",
    "            if type(val) == str:\n",
    "                pass\n",
    "            elif np.isnan(val):\n",
    "                for k in lkeymap.keys():\n",
    "                    if k == 'daily' or k == 'hourly':\n",
    "                        for lk in lkeymap[k].keys():\n",
    "                            if lkeymap[k][lk] == i:\n",
    "                                if lk not in nankeys[k].keys():\n",
    "                                    nankeys[k][lk] = 1\n",
    "                                else:\n",
    "                                    nankeys[k][lk] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apparentTemperature': 46,\n",
       " 'cloudCover': 50,\n",
       " 'dewPoint': 47,\n",
       " 'humidity': 42,\n",
       " 'icon': 48,\n",
       " 'precipAccumulation': 44,\n",
       " 'precipIntensity': 43,\n",
       " 'precipProbability': 37,\n",
       " 'precipType': 35,\n",
       " 'pressure': 36,\n",
       " 'summary': 39,\n",
       " 'temperature': 45,\n",
       " 'time': 49,\n",
       " 'visibility': 40,\n",
       " 'windBearing': 38,\n",
       " 'windSpeed': 41}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lkeymap['hourly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daily': {}, 'hourly': {'humidity': 1}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nankeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pressure 0.616576579023613\n",
      "temperatureHighTime 0.04945132551064754\n",
      "humidity 0.981412067217152\n",
      "precipIntensity 0.04945132551064754\n",
      "icon 0.04945132551064754\n",
      "apparentTemperatureMax 0.6048090504128639\n",
      "precipIntensityMax 0.011189428509343763\n",
      "temperatureHigh 0.011189428509343763\n",
      "apparentTemperatureMaxTime 0.02670668549905838\n",
      "precipProbability 0.026897725626539186\n",
      "visibility 0.011949967405475881\n",
      "windBearing 0.011949967405475881\n",
      "apparentTemperatureHighTime 0.012688776618861364\n",
      "temperatureMaxTime 2.17296827466319e-05\n",
      "apparentTemperatureMinTime 4.34593654932638e-05\n",
      "dewPoint 2.17296827466319e-05\n",
      "cloudCover 4.34593654932638e-05\n",
      "temperatureMinTime 2.17296827466319e-05\n",
      "temperatureMin 2.17296827466319e-05\n",
      "summary 2.17296827466319e-05\n",
      "sunriseTime 0.0001955671447196871\n",
      "time 2.17296827466319e-05\n",
      "apparentTemperatureMin 0.0001955671447196871\n",
      "precipIntensityMaxTime 2.17296827466319e-05\n",
      "apparentTemperatureHigh 2.17296827466319e-05\n",
      "apparentTemperatureLowTime 0.0001955671447196871\n",
      "temperatureLow 4.34593654932638e-05\n",
      "sunsetTime 4.34593654932638e-05\n",
      "apparentTemperatureLow 0.0001955671447196871\n",
      "precipType 0.9935820476604375\n",
      "precipAccumulation 0.05520290091264667\n",
      "dewPoint 0.05520290091264667\n",
      "humidity 0.8361744893524554\n",
      "windBearing 0.0829562327973345\n",
      "visibility 0.009787411270462117\n",
      "windSpeed 0.011236509488628133\n",
      "precipIntensity 0.08175747863247863\n",
      "temperature 0.009787411270462117\n",
      "time 0.02863519484282196\n",
      "cloudCover 0.030440116615964075\n",
      "apparentTemperature 0.0001507496740547588\n",
      "icon 0.0001507496740547588\n",
      "pressure 3.395262929161234e-05\n",
      "precipProbability 3.395262929161234e-05\n",
      "summary 3.395262929161234e-05\n"
     ]
    }
   ],
   "source": [
    "for k in nankeys:\n",
    "    if k != 'latitude' or k != 'longitudue':\n",
    "        for lk in nankeys[k]:\n",
    "            print(lk, nankeys[k][lk]/(len(all_data_arrays) * 36))\n",
    "            \n",
    "#if the value is <1%, set to random unif(0,1)\n",
    "    #if > 1, \n",
    "    #if time, set to -1\n",
    "#if the value \n",
    "#humidity: set to 0 #value that would otherwise be 0\n",
    "#visibility set to 1 #value that would otherwise be 1\n",
    "#windbearing: set to random unif(0,1)\n",
    "#summary: throw it out\n",
    "#if string, set to -1\n",
    "#precipAccumuluation: set to 0 #value that would otherwise be 0\n",
    "#dewPoint: set to 0\n",
    "#windSpeed: set to 0\n",
    "#precipIntensity: set to 0\n",
    "#cloudCover: set to 0\n",
    "#pressure: throw it out\n",
    "\n",
    "\n",
    "\n",
    "#pressure: throw it out\n",
    "#temperatureHighTime: set to -1\n",
    "#humidity: set to 0 or throw it out\n",
    "#precipIntensity: set to -1\n",
    "#icon set to -1\n",
    "#apparentTemperatureMax: throw it out\n",
    "#precipIntensityMax: -1\n",
    "#temperatureHigh: \n",
    "#apparentTemperatureMaxTime: -1 or throw out\n",
    "#precipProbability:  set to 0\n",
    "#visilibity: set to 1\n",
    "#windBearing: set to -1\n",
    "#apparentTemperatureHighTime: set to -1\n",
    "#temperatureMaxTime: random value in 0,1 or set to -1\n",
    "#apparent\n",
    "\n",
    "#need to interpolate values instead\n",
    "\n",
    "#throw out\n",
    "#pressure\n",
    "#humidity\n",
    "#precipType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary\n",
      "icon\n",
      "precipType\n"
     ]
    }
   ],
   "source": [
    "for k in data_key_values:\n",
    "    vals = list(data_key_values[k])\n",
    "    if type(vals[0]) == str:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clear-day',\n",
       " 'clear-night',\n",
       " 'cloudy',\n",
       " 'fog',\n",
       " 'partly-cloudy-day',\n",
       " 'partly-cloudy-night',\n",
       " 'rain',\n",
       " 'snow',\n",
       " 'wind'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key_values['icon'] #normalize to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rain', 'snow'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key_values['precipType'] #normalize to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2207520"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36 * 8760 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['25.8468651,-101.4818115',\n",
       " '25.8468651,-104.0603027',\n",
       " '25.8468651,-106.6387939',\n",
       " '25.8468651,-93.7463379',\n",
       " '25.8468651,-96.3248291',\n",
       " '25.8468651,-98.9033203',\n",
       " '27.97941932,-101.4818115',\n",
       " '27.97941932,-104.0603027',\n",
       " '27.97941932,-106.6387939',\n",
       " '27.97941932,-93.7463379',\n",
       " '27.97941932,-96.3248291',\n",
       " '27.97941932,-98.9033203',\n",
       " '30.11197354,-101.4818115',\n",
       " '30.11197354,-104.0603027',\n",
       " '30.11197354,-106.6387939',\n",
       " '30.11197354,-93.7463379',\n",
       " '30.11197354,-96.3248291',\n",
       " '30.11197354,-98.9033203',\n",
       " '32.24452776,-101.4818115',\n",
       " '32.24452776,-104.0603027',\n",
       " '32.24452776,-106.6387939',\n",
       " '32.24452776,-93.7463379',\n",
       " '32.24452776,-96.3248291',\n",
       " '32.24452776,-98.9033203',\n",
       " '34.37708198,-101.4818115',\n",
       " '34.37708198,-104.0603027',\n",
       " '34.37708198,-106.6387939',\n",
       " '34.37708198,-93.7463379',\n",
       " '34.37708198,-96.3248291',\n",
       " '34.37708198,-98.9033203',\n",
       " '36.5096362,-101.4818115',\n",
       " '36.5096362,-104.0603027',\n",
       " '36.5096362,-106.6387939',\n",
       " '36.5096362,-93.7463379',\n",
       " '36.5096362,-96.3248291',\n",
       " '36.5096362,-98.9033203']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_locs\n",
    "minlat = 25.8468651\n",
    "maxlat = 36.5096362\n",
    "minlon = -101.4818115\n",
    "maxlon = -93.7463379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 1293861600 1483254000\n",
      "summary Breezy Windy until evening.\n",
      "icon clear-day wind\n",
      "precipType rain snow\n",
      "temperature -18.04 110.76\n",
      "apparentTemperature -43.45 137.14\n",
      "dewPoint -22.63 91\n",
      "humidity 0.02 1\n",
      "windSpeed 0 114\n",
      "windBearing 0 359\n",
      "cloudCover 0 1\n",
      "visibility 0 10\n",
      "sunriseTime 1293886982 1483193980\n",
      "sunsetTime 1293923475 1483230460\n",
      "moonPhase 0 0.99\n",
      "temperatureHigh 1.17 110.76\n",
      "temperatureHighTime 1293886800 1483225200\n",
      "temperatureLow -18.04 88\n",
      "temperatureLowTime 1293940800 1483282800\n",
      "apparentTemperatureHigh -8.53 137.14\n",
      "apparentTemperatureHighTime 1293886800 1483225200\n",
      "apparentTemperatureLow -43.45 100.22\n",
      "apparentTemperatureLowTime 1293940800 1483279200\n",
      "pressure 986.92 1051.23\n",
      "temperatureMin -18.04 86\n",
      "temperatureMinTime 1293879600 1483250400\n",
      "temperatureMax 4.95 110.76\n",
      "temperatureMaxTime 1293861600 1483225200\n",
      "apparentTemperatureMin -43.45 99.8\n",
      "apparentTemperatureMinTime 1293876000 1483250400\n",
      "apparentTemperatureMax -8.49 137.14\n",
      "apparentTemperatureMaxTime 1293861600 1483225200\n",
      "precipIntensity 0 3.2235\n",
      "precipProbability 0 1\n",
      "precipIntensityMax 0 3.2235\n",
      "precipIntensityMaxTime 1293861600 1483246800\n",
      "precipAccumulation 0 48.681\n"
     ]
    }
   ],
   "source": [
    "for k in data_key_values:\n",
    "    vals = list(data_key_values[k])\n",
    "    print(k, min(vals), max(vals)) #moon phase is already normalized, how nice\n",
    "    \n",
    "for l in grid_locs:\n",
    "    tokens = grid_locs.split(\",\")\n",
    "    minlat, maxlat\n",
    "    minlon, maxlon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_data_keys = []\n",
    "for k in data_key_values:\n",
    "    if k != \"summary\" or k != \"time\":\n",
    "        valid_data_keys.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_to_time_of_day(unixtime):\n",
    "    if np.isnan(unixtime):\n",
    "        val = np.nan\n",
    "    else:\n",
    "        dt = datetime.datetime.fromtimestamp(unixtime)\n",
    "        h = dt.hour * 3600 + dt.minute * 60 + dt.second \n",
    "        val = h/86400\n",
    "    return(val)\n",
    "\n",
    "def norm_lat_long(val, latorlon):\n",
    "    minlat = 25.8468651\n",
    "    maxlat = 36.5096362\n",
    "    minlon = -101.4818115\n",
    "    maxlon = -93.7463379\n",
    "    if latorlon == \"latitude\":\n",
    "        out = (val - minlat)/(maxlat - minlat)\n",
    "    elif latorlon == \"longitude\":\n",
    "        out = (val - minlon)/(maxlon - minlon)\n",
    "    \n",
    "    return(out)\n",
    "\n",
    "def str_to_norm_func(strin, key):\n",
    "    if type(strin) == str:\n",
    "        vals = sorted(list(data_key_values[key]))\n",
    "        i = vals.index(strin)\n",
    "        denom = len(vals) - 1\n",
    "        out = i/denom\n",
    "    else:\n",
    "        out = np.nan\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if the value is <1%, set to random unif(0,1)\n",
    "    #if > 1, \n",
    "    #if time, set to -1\n",
    "#if the value \n",
    "#humidity: set to 0 #value that would otherwise be 0\n",
    "#visibility set to 1 #value that would otherwise be 1\n",
    "#windbearing: set to random unif(0,1)\n",
    "#summary: throw it out\n",
    "#if string, set to -1\n",
    "#precipAccumuluation: set to 0 #value that would otherwise be 0\n",
    "#dewPoint: set to 0\n",
    "#windSpeed: set to 0\n",
    "#precipIntensity: set to 0\n",
    "#cloudCover: set to 0\n",
    "#pressure: throw it out\n",
    "\n",
    "\n",
    "#set_nan_to_rand = all the rest\n",
    "#set_nan_to_minus_one = string, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalize sunriseTime and sunsetTime to time of day\n",
    "#normalize temperatureHighTime to time of day\n",
    "#normalize temperatureLowTime to time of day\n",
    "#temperatureMinTime\n",
    "#temperatureMaxTime\n",
    "#apparentTemperatureMinTime\n",
    "#apparentTemperatureMaxTime\n",
    "#precipIntensityMaxTime\n",
    "#normalize time to time of day\n",
    "\n",
    "time_keys_to_norm = ['sunriseTime', 'sunsetTime', 'temperatureHighTime', 'temperatureLowTime', 'temperatureMinTime', 'temperatureMaxTime', 'apparentTemperatureMinTime', 'apparentTemperatureMaxTime', 'precipIntensityMaxTime', 'time']\n",
    "values_to_norm = ['temperature', 'apparentTemperature', 'dewPoint', 'humidity', 'windSpeed', 'windBearing', 'visibility', 'temperatureHigh', 'temperatureLow', 'apparentTemperatureHigh', 'apparentTemperatureLow', 'pressure', 'temperatureMin', 'temperatureMax', 'apparentTemperatureMax', 'apparentTemperatureMin', 'precipIntensity', 'precipIntensityMax', 'precipAccumulation', 'latitude', 'longitude']\n",
    "str_to_norm = ['icon', 'precipType']\n",
    "values_no_norm = ['cloudCover', 'moonPhase', 'precipProbability']\n",
    "\n",
    "throw_out = ['pressure', 'humidity', 'precipType']\n",
    "\n",
    "minmaxs = {}\n",
    "for k in values_to_norm:\n",
    "    if k == \"latitude\" or k == \"longitude\":\n",
    "        pass\n",
    "    else:\n",
    "        minmaxs[k] = {}\n",
    "        vals = list(data_key_values[k])\n",
    "        minmaxs[k][\"max\"] = max(vals)\n",
    "        minmaxs[k][\"min\"] = min(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(outdir + \"training_minmaxs.pck\", 'wb') as f:\n",
    "    pickle.dump(minmaxs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(outdir + \"training_minmaxs.pck\", 'rb') as f:\n",
    "    minmaxs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    hourfs = os.listdir(outdir + \"yearly/\" + str(year))\n",
    "    for fname in hourfs:\n",
    "        pth = outdir + \"yearly/\" + str(year) + \"/\" + fname\n",
    "        vec = []\n",
    "        with open(pth, 'rb') as d:\n",
    "            data = pickle.load(d)\n",
    "            for l in grid_locs:\n",
    "                gi = grid_locs.index(l)\n",
    "                catvec = [ 0.0 for i in range(lkeymax + 1) ]\n",
    "                for k in lkeymap.keys():\n",
    "                    if k == \"daily\" or k == \"hourly\":\n",
    "                        for lk in lkeymap[k].keys():\n",
    "                            if lk in throw_out:\n",
    "                                pass\n",
    "                            else:\n",
    "                                ind = lkeymap[k][lk]\n",
    "                                dval = data[l][ind]\n",
    "                                if lk in values_to_norm:\n",
    "                                    catvec[ind] = (dval - minmaxs[lk][\"min\"])/(minmaxs[lk][\"max\"] - minmaxs[lk][\"min\"])\n",
    "                                elif lk in str_to_norm:\n",
    "                                    catvec[ind] = str_to_norm_func(dval, lk)\n",
    "                                elif lk in values_no_norm:\n",
    "                                    catvec[ind] = dval\n",
    "                                elif lk in time_keys_to_norm:\n",
    "                                    catvec[ind] = norm_to_time_of_day(dval)\n",
    "                    elif k == \"latitude\" or k == \"longitude\":\n",
    "                        pass\n",
    "                vec += catvec\n",
    "                                                                           \n",
    "        vec = np.asarray(vec, dtype=np.float32)\n",
    "        write_path = outdir + \"yearly_normed_arrays/\" + str(year)\n",
    "        if not os.path.exists(write_path):\n",
    "            os.mkdir(write_path)\n",
    "        np.savetxt(write_path + \"/\" + fname.split(\"_\")[0] + \"_normed_vec.txt\", vec, delimiter=\",\")\n",
    "                        \n",
    "                        \n",
    "                     \n",
    "                        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_normed_data_arrays = {}\n",
    "\n",
    "for year in years:\n",
    "    hourfs = os.listdir(outdir + \"yearly_normed_arrays/\" + str(year))\n",
    "    for fname in hourfs:\n",
    "        pth = outdir + \"yearly_normed_arrays/\" + str(year) + \"/\" + fname.split(\"_\")[0] + \"_normed_vec.txt\"\n",
    "        data = np.loadtxt(pth, delimiter=\",\")\n",
    "        all_normed_data_arrays[fname] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-d75178ec4168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mlocadd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlkeymax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mprv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_normed_data_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhourfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlocadd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_normed_data_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhourfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlocadd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_normed_data_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhourfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlocadd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mnxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_normed_data_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhourfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlocadd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_normed_data_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhourfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlocadd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_normed_data_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhourfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlocadd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnxt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mavals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "hourfiles = list(all_normed_data_arrays.keys())\n",
    "for i in range(len(hourfiles)):\n",
    "    hf = all_normed_data_arrays[hourfiles[i]]\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    for j in range(len(hf)):\n",
    "        val = hf[j] \n",
    "        if np.isnan(val):\n",
    "            #average across all geographic locations\n",
    "            avals = []\n",
    "            for k in range(len(grid_locs)):\n",
    "                locadd = int(-1.0*k*lkeymax)\n",
    "                prv = [all_normed_data_arrays[hourfiles[i-3]][j + locadd], all_normed_data_arrays[hourfiles[i-2]][j + locadd], all_normed_data_arrays[hourfiles[i-1]][j + locadd]]\n",
    "                nxt = [all_normed_data_arrays[hourfiles[i+1]][j + locadd], all_normed_data_arrays[hourfiles[i+2]][j + locadd], all_normed_data_arrays[hourfiles[i+3]][j + locadd]]\n",
    "                a = prv + nxt\n",
    "                avals.append(np.nanmean(np.array(a)))\n",
    "            interp = np.nanmean(np.array(avals))\n",
    "            if np.isnan(interp):\n",
    "                print(hour)\n",
    "                interp = -1.0\n",
    "            all_normed_data_arrays[hourfiles[i]][j] = interp\n",
    "    hour = int(hourfiles[i].split(\"_\")[0])\n",
    "    t = datetime.datetime.fromtimestamp(hour)\n",
    "    year = str(t.year)\n",
    "    if np.isnan(all_normed_data_arrays[hourfiles[i]]).any():\n",
    "        print(hour)\n",
    "    np.savetxt(outdir + \"yearly_normed_arrays_clean/\" + year + \"/\" + str(hour) + \"_no_nans_interp_normed_vec.txt\", all_normed_data_arrays[hourfiles[i]], delimiter=\",\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fixing lag for last three\n",
    "for i in range(len(hourfiles)-4,len(hourfiles)):\n",
    "    hf = all_normed_data_arrays[hourfiles[i]]\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "    for j in range(len(hf)):\n",
    "        val = hf[j] \n",
    "        if np.isnan(val):\n",
    "            #average across all geographic locations\n",
    "            avals = []\n",
    "            for k in range(len(grid_locs)):\n",
    "                locadd = int(-1.0*k*lkeymax)\n",
    "                prv = [all_normed_data_arrays[hourfiles[i-3]][j + locadd], all_normed_data_arrays[hourfiles[i-2]][j + locadd], all_normed_data_arrays[hourfiles[i-1]][j + locadd]]\n",
    "                #nxt = [all_normed_data_arrays[hourfiles[i+1]][j + locadd], all_normed_data_arrays[hourfiles[i+2]][j + locadd], all_normed_data_arrays[hourfiles[i+3]][j + locadd]]\n",
    "                a = prv #+ nxt\n",
    "                avals.append(np.nanmean(np.array(a)))\n",
    "            interp = np.nanmean(np.array(avals))\n",
    "            if np.isnan(interp):\n",
    "                print(hour)\n",
    "                interp = -1.0\n",
    "            all_normed_data_arrays[hourfiles[i]][j] = interp\n",
    "    hour = int(hourfiles[i].split(\"_\")[0])\n",
    "    t = datetime.datetime.fromtimestamp(hour)\n",
    "    year = str(t.year)\n",
    "    if np.isnan(all_normed_data_arrays[hourfiles[i]]).any():\n",
    "        print(hour)\n",
    "    np.savetxt(outdir + \"yearly_normed_arrays_clean/\" + year + \"/\" + str(hour) + \"_no_nans_interp_normed_vec.txt\", all_normed_data_arrays[hourfiles[i]], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'dict' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0e55779a67a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlkeymax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlkeymap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlkeymap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlkeymax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mlkeymax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlkeymap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'dict' and 'int'"
     ]
    }
   ],
   "source": [
    "lkeymax = 0\n",
    "for k in lkeymap:\n",
    "    if lkeymap[k] > lkeymax:\n",
    "        lkeymax = lkeymap[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now I want to do some feature engineering with the weather data\n",
    "#reset the notebook and load arrays\n",
    "all_normed_data_arrays = {}\n",
    "\n",
    "max_over_bill = sorted([14, 35, 10, 24, 11, 47])\n",
    "min_over_bill = sorted([33, 23, 35, 25, 47])\n",
    "#total_over_bill = [16, 46]\n",
    "average_over_two_weeks = sorted([24, 25, 11, 32, 22, 34, 16, 6, 20, 17, 10, 33, 14, 23, 18, 47, 49, 48, 46, 43, 45, 35, 36])\n",
    "\n",
    "single_loc_keys = {}\n",
    "j = 0\n",
    "for i in max_over_bill + min_over_bill + average_over_two_weeks:\n",
    "    single_loc_keys[i] = j\n",
    "    j += 1\n",
    "single_loc = [ 0.0 for s in range(len(max_over_bill + min_over_bill + average_over_two_weeks))]\n",
    "\n",
    "for year in years:\n",
    "    hourfs = os.listdir(outdir + \"yearly_normed_arrays_clean/\" + str(year))\n",
    "    for fname in hourfs:\n",
    "        pth = outdir + \"yearly_normed_arrays_clean/\" + str(year) + \"/\" + fname.split(\"_\")[0] + \"_no_nans_interp_normed_vec.txt\"\n",
    "        data = np.loadtxt(pth, delimiter=\",\")\n",
    "        all_normed_data_arrays[fname] = data\n",
    "\n",
    "hourfiles = sorted(list(all_normed_data_arrays.keys()))\n",
    "all_arrays = []\n",
    "\n",
    "for i in range(len(hourfiles)):\n",
    "    all_arrays.append(all_normed_data_arrays[hourfiles[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-8-90a20e974fc7>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-90a20e974fc7>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    for k in range(len(grid_locs)):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "curr_year = 2011\n",
    "for i in range(len(all_arrays)):\n",
    "    ut = hourfiles[i].split(\"_\")[0]\n",
    "    year = datetime.datetime.fromtimestamp(ut).year\n",
    "    maxob = [0.0 for i in range(len(max_over_bill))]\n",
    "    minob = [100.0 for i in range(len(min_over_bill))]\n",
    "    if year > curr_year:\n",
    "        maxob = [0.0 for i in range(len(max_over_bill))]\n",
    "        minob = [100.0 for i in range(len(min_over_bill))]\n",
    "    for k in range(len(grid_locs)):\n",
    "        catvec = copy.copy(single_loc)\n",
    "        for j in range(lkeymax): #all features\n",
    "            ind = k*lkeymax + j\n",
    "            if j in max_over_bill: #going to need to catch leap years\n",
    "                catind = single_loc_keys[j]\n",
    "                val = all_arrays[i][ind]\n",
    "                running_val = maxob[max_over_bill.index(j)]\n",
    "                if val > running_val:\n",
    "                    maxob[maxob.index(j)] = val\n",
    "                    running_val = val\n",
    "                catvec[catind] = running_val\n",
    "            if j in min_over_bill:\n",
    "                catind = single_loc_keys[j]\n",
    "                val = all_arrays[i][ind]\n",
    "                running_val = maxob[maxob.index(j)]\n",
    "                if val < running_val:\n",
    "                    maxob[min_over_bill.index(j)] = val\n",
    "                    running_val = val\n",
    "                catvec[catit] = running_val\n",
    "            if j in average_over_two_weeks:\n",
    "                catind = single_loc_keys[j]\n",
    "                vals = []\n",
    "                #look at the last 168 values at the same index\n",
    "                for t in range(168):\n",
    "                    vals.append(all_arrays[i - t][ind])\n",
    "                val = np.nanmean(vals)\n",
    "                catvec[catind] = val\n",
    "    out = np.concatenate((all_arrays[i], catvec), axis=0)\n",
    "    hour = hourfiles[i].split(\"_\")[0]\n",
    "    np.savetxt(outdir + \"yearly_normed_arrays_clean/\" + year + \"/\" + hour + \"_clean_normed_feature_vec.txt\", out, delimiter=\",\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92000002, 0.        , 0.83333331, ..., 0.        , 0.13      ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_arrays[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(all_arrays)):\n",
    "    for k in range(len(grid_locs)):\n",
    "        for j in range(50):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lkeymax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-08b3f1985432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlkeymax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lkeymax' is not defined"
     ]
    }
   ],
   "source": [
    "lkeymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'daily': {'apparentTemperatureHigh': 24,\n",
    "  'apparentTemperatureHighTime': 7,\n",
    "  'apparentTemperatureLow': 25,\n",
    "  'apparentTemperatureLowTime': 1,\n",
    "  'apparentTemperatureMax': 11,\n",
    "  'apparentTemperatureMaxTime': 31,\n",
    "  'apparentTemperatureMin': 32,\n",
    "  'apparentTemperatureMinTime': 2,\n",
    "  'cloudCover': 22,\n",
    "  'dewPoint': 15,\n",
    "  'humidity': 34,\n",
    "  'icon': 21,\n",
    "  'moonPhase': 0,\n",
    "  'precipAccumulation': 16,\n",
    "  'precipIntensity': 6,\n",
    "  'precipIntensityMax': 20,\n",
    "  'precipIntensityMaxTime': 19,\n",
    "  'precipProbability': 17,\n",
    "  'precipType': 28,\n",
    "  'pressure': 3,\n",
    "  'summary': 4,\n",
    "  'sunriseTime': 29,\n",
    "  'sunsetTime': 5,\n",
    "  'temperatureHigh': 10,\n",
    "  'temperatureHighTime': 13,\n",
    "  'temperatureLow': 33,\n",
    "  'temperatureLowTime': 8,\n",
    "  'temperatureMax': 14,\n",
    "  'temperatureMaxTime': 27,\n",
    "  'temperatureMin': 23,\n",
    "  'temperatureMinTime': 26,\n",
    "  'time': 9,\n",
    "  'visibility': 30,\n",
    "  'windBearing': 12,\n",
    "  'windSpeed': 18},\n",
    " 'hourly': {'apparentTemperature': 47,\n",
    "  'cloudCover': 49,\n",
    "  'dewPoint': 41,\n",
    "  'humidity': 48,\n",
    "  'icon': 42,\n",
    "  'precipAccumulation': 46,\n",
    "  'precipIntensity': 43,\n",
    "  'precipProbability': 45,\n",
    "  'precipType': 40,\n",
    "  'pressure': 38,\n",
    "  'summary': 39,\n",
    "  'temperature': 35,\n",
    "  'time': 50,\n",
    "  'visibility': 44,\n",
    "  'windBearing': 37,\n",
    "  'windSpeed': 36}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
